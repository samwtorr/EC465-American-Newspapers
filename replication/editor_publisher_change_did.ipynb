{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927ee80b",
   "metadata": {},
   "source": [
    "###  Estimating the effect of new editors/owners on Newspaper Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3154e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samwt\\AppData\\Local\\Temp\\ipykernel_14772\\70318997.py:11: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/master.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PUBLISHER & EDITOR CHANGE ANALYSIS RESULTS\n",
      "============================================================\n",
      "\n",
      "Total newspapers analyzed: 48137\n",
      "Newspapers with at least 4 years of data: 12597\n",
      "Newspapers with insufficient data: 35540\n",
      "\n",
      "----------------------------------------\n",
      "CATEGORY BREAKDOWN:\n",
      "----------------------------------------\n",
      "Editor & publisher changed (same year): 4686 (37.2%)\n",
      "Editor & publisher changed (diff years): 838 (6.7%)\n",
      "Publisher changed only: 816 (6.5%)\n",
      "Editor changed only: 741 (5.9%)\n",
      "No change detected: 5516 (43.8%)\n",
      "\n",
      "============================================================\n",
      "Updated master.csv with 'category', 'publisher_change_year', and 'editor_change_year' columns\n",
      "============================================================\n",
      "\n",
      "----------------------------------------\n",
      "SAMPLE: Newspapers with changes\n",
      "----------------------------------------\n",
      "state      town     newspaper_name                              category  publisher_change_year  editor_change_year\n",
      "  NaN     Afton            Tribane editor_and_publisher_change_same_year                 1876.0              1876.0\n",
      "  NaN     Albia Spirit of the West editor_and_publisher_change_same_year                 1872.0              1872.0\n",
      "  NaN    Albion            Pioneer                 publisher_change_only                 1872.0                 NaN\n",
      "  NaN     Aledo  Democratic Banner editor_and_publisher_change_same_year                 1872.0              1872.0\n",
      "  NaN Allentown Lehigh Valley News                    editor_change_only                    NaN              1871.0\n",
      "  NaN Allentown Stadtand Land-Bote                    editor_change_only                    NaN              1871.0\n",
      "  NaN  Alliance              Local editor_and_publisher_change_same_year                 1872.0              1872.0\n",
      "  NaN  Angelica         Republican editor_and_publisher_change_same_year                 1876.0              1876.0\n",
      "  NaN Ann Arbor Peninsular Courier                    editor_change_only                    NaN              1871.0\n",
      "  NaN     Anoka    Anoka Co. Press editor_and_publisher_change_same_year                 1873.0              1873.0\n",
      "\n",
      "============================================================\n",
      "DataFrames created:\n",
      "  - publisher_change_only_list\n",
      "  - editor_change_only_list\n",
      "  - editor_and_publisher_same_list\n",
      "  - editor_and_publisher_diff_list\n",
      "  - no_change_list\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# add publisher and editor change information to Master.csv\n",
    "# NOW SPLITS editor_and_publisher_change into:\n",
    "#   - editor_and_publisher_change_same_year\n",
    "#   - editor_and_publisher_change_diff_year\n",
    "# ALSO: ignores single-entry \"blips\" (likely data entry errors)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/master.csv')\n",
    "\n",
    "# Define the years we're tracking\n",
    "years = [1869, 1871, 1872, 1873, 1876, 1877, 1878, 1879, 1880, 1882, 1883, 1884, 1885, 1890]\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = prev_row[j + 1] + 1\n",
    "            deletions = curr_row[j] + 1\n",
    "            substitutions = prev_row[j] + (c1 != c2)\n",
    "            curr_row.append(min(insertions, deletions, substitutions))\n",
    "        prev_row = curr_row\n",
    "    return prev_row[-1]\n",
    "\n",
    "def strings_match(s1, s2, max_distance=1):\n",
    "    \"\"\"Check if two strings match within max_distance edits.\"\"\"\n",
    "    s1_clean = s1.strip().lower()\n",
    "    s2_clean = s2.strip().lower()\n",
    "    if s1_clean == s2_clean:\n",
    "        return True\n",
    "    return levenshtein_distance(s1_clean, s2_clean) <= max_distance\n",
    "\n",
    "def tokenize_publisher(publisher_str):\n",
    "    if not publisher_str:\n",
    "        return []\n",
    "    cleaned = re.sub(r'[;,]', ' ', publisher_str)\n",
    "    tokens = cleaned.split()\n",
    "    return [t.strip() for t in tokens if len(t.strip()) >= 4]\n",
    "\n",
    "def publishers_match_tokenized(pub1, pub2):\n",
    "    tokens1 = tokenize_publisher(pub1)\n",
    "    tokens2 = tokenize_publisher(pub2)\n",
    "    if not tokens1 or not tokens2:\n",
    "        return False\n",
    "    for t1 in tokens1:\n",
    "        for t2 in tokens2:\n",
    "            if strings_match(t1, t2, max_distance=1):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def normalize_publisher(pub):\n",
    "    if not pub:\n",
    "        return \"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', pub.lower())\n",
    "\n",
    "def publishers_match_normalized(pub1, pub2):\n",
    "    n1 = normalize_publisher(pub1)\n",
    "    n2 = normalize_publisher(pub2)\n",
    "    if not n1 or not n2:\n",
    "        return False\n",
    "    return n1 == n2 or n1 in n2 or n2 in n1\n",
    "\n",
    "def names_match(name1, name2):\n",
    "    return publishers_match_tokenized(name1, name2) or publishers_match_normalized(name1, name2)\n",
    "\n",
    "def clean_field(value):\n",
    "    if pd.isna(value):\n",
    "        return ''\n",
    "    s = str(value).strip()\n",
    "    if s.lower() == 'nan':\n",
    "        return ''\n",
    "    return s\n",
    "\n",
    "def remove_blips(data_points):\n",
    "    \"\"\"\n",
    "    Remove single-entry 'blips' that are likely data entry errors.\n",
    "    A blip is where value changes at index i but reverts back at index i+1\n",
    "    (i.e., data_points[i-1] and data_points[i+1] match, but data_points[i] doesn't\n",
    "    match either of them).\n",
    "    \n",
    "    For example: A, A, B, A, A -> the lone B is a blip and gets removed.\n",
    "    \"\"\"\n",
    "    if len(data_points) <= 2:\n",
    "        return data_points\n",
    "\n",
    "    filtered = []\n",
    "    i = 0\n",
    "    while i < len(data_points):\n",
    "        if 0 < i < len(data_points) - 1:\n",
    "            prev_year, prev_val = data_points[i - 1]\n",
    "            curr_year, curr_val = data_points[i]\n",
    "            next_year, next_val = data_points[i + 1]\n",
    "\n",
    "            # Check if this is a blip: current doesn't match prev,\n",
    "            # but prev and next DO match each other\n",
    "            is_blip = (\n",
    "                not names_match(prev_val, curr_val)\n",
    "                and names_match(prev_val, next_val)\n",
    "            )\n",
    "            if is_blip:\n",
    "                # Skip this data point entirely\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        filtered.append(data_points[i])\n",
    "        i += 1\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def detect_first_change(data_points):\n",
    "    # First remove blips, then detect changes on the cleaned sequence\n",
    "    cleaned = remove_blips(data_points)\n",
    "    for i in range(1, len(cleaned)):\n",
    "        prev_year, prev_val = cleaned[i - 1]\n",
    "        curr_year, curr_val = cleaned[i]\n",
    "        if not names_match(prev_val, curr_val):\n",
    "            return curr_year\n",
    "    return None\n",
    "\n",
    "def analyze_changes(row):\n",
    "    \"\"\"\n",
    "    Analyze a newspaper row for publisher and editor changes.\n",
    "    Returns: (category, publisher_change_year, editor_change_year)\n",
    "    Categories:\n",
    "      - 'insufficient_data'\n",
    "      - 'no_change'\n",
    "      - 'publisher_change_only'\n",
    "      - 'editor_change_only'\n",
    "      - 'editor_and_publisher_change_same_year'\n",
    "      - 'editor_and_publisher_change_diff_year'\n",
    "    \"\"\"\n",
    "    pub_data = []\n",
    "    ed_data = []\n",
    "    for year in years:\n",
    "        publisher = clean_field(row.get(f'{year} publisher', ''))\n",
    "        editor = clean_field(row.get(f'{year} editor', ''))\n",
    "        if publisher:\n",
    "            pub_data.append((year, publisher))\n",
    "        if editor:\n",
    "            ed_data.append((year, editor))\n",
    "\n",
    "    has_enough_pub = len(pub_data) >= 3\n",
    "    has_enough_ed = len(ed_data) >= 3\n",
    "\n",
    "    if not has_enough_pub and not has_enough_ed:\n",
    "        return ('insufficient_data', None, None)\n",
    "\n",
    "    pub_change_year = detect_first_change(pub_data) if has_enough_pub else None\n",
    "    ed_change_year = detect_first_change(ed_data) if has_enough_ed else None\n",
    "\n",
    "    has_pub_change = pub_change_year is not None\n",
    "    has_ed_change = ed_change_year is not None\n",
    "\n",
    "    if has_pub_change and has_ed_change:\n",
    "        if pub_change_year == ed_change_year:\n",
    "            category = 'editor_and_publisher_change_same_year'\n",
    "        else:\n",
    "            category = 'editor_and_publisher_change_diff_year'\n",
    "    elif has_pub_change:\n",
    "        category = 'publisher_change_only'\n",
    "    elif has_ed_change:\n",
    "        category = 'editor_change_only'\n",
    "    else:\n",
    "        category = 'no_change'\n",
    "\n",
    "    return (category, pub_change_year, ed_change_year)\n",
    "\n",
    "# Apply analysis to each row\n",
    "results = df.apply(analyze_changes, axis=1)\n",
    "df['category'] = results.apply(lambda x: x[0])\n",
    "df['publisher_change_year'] = results.apply(lambda x: x[1])\n",
    "df['editor_change_year'] = results.apply(lambda x: x[2])\n",
    "\n",
    "# Filter out insufficient data\n",
    "valid_df = df[df['category'] != 'insufficient_data'].copy()\n",
    "\n",
    "# Count categories\n",
    "category_counts = valid_df['category'].value_counts()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PUBLISHER & EDITOR CHANGE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal newspapers analyzed: {len(df)}\")\n",
    "print(f\"Newspapers with at least 4 years of data: {len(valid_df)}\")\n",
    "print(f\"Newspapers with insufficient data: {len(df) - len(valid_df)}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"CATEGORY BREAKDOWN:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "category_labels = {\n",
    "    'editor_and_publisher_change_same_year': 'Editor & publisher changed (same year)',\n",
    "    'editor_and_publisher_change_diff_year': 'Editor & publisher changed (diff years)',\n",
    "    'publisher_change_only': 'Publisher changed only',\n",
    "    'editor_change_only': 'Editor changed only',\n",
    "    'no_change': 'No change detected',\n",
    "}\n",
    "\n",
    "for cat, label in category_labels.items():\n",
    "    count = category_counts.get(cat, 0)\n",
    "    pct = (count / len(valid_df) * 100) if len(valid_df) > 0 else 0\n",
    "    print(f\"{label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv('data/master.csv', index=False)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Updated master.csv with 'category', 'publisher_change_year', and 'editor_change_year' columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show sample of newspapers with changes\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"SAMPLE: Newspapers with changes\")\n",
    "print(\"-\" * 40)\n",
    "change_categories = list(category_labels.keys())\n",
    "change_categories.remove('no_change')\n",
    "changes_df = valid_df[valid_df['category'].isin(change_categories)]\n",
    "if len(changes_df) > 0:\n",
    "    sample_cols = ['state', 'town', 'newspaper_name', 'category', 'publisher_change_year', 'editor_change_year']\n",
    "    print(changes_df[sample_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"No changes found.\")\n",
    "\n",
    "# Create lists for each category\n",
    "publisher_change_only_list = valid_df[valid_df['category'] == 'publisher_change_only'][\n",
    "    ['state', 'town', 'newspaper_name', 'publisher_change_year']\n",
    "]\n",
    "editor_change_only_list = valid_df[valid_df['category'] == 'editor_change_only'][\n",
    "    ['state', 'town', 'newspaper_name', 'editor_change_year']\n",
    "]\n",
    "editor_and_publisher_same_list = valid_df[valid_df['category'] == 'editor_and_publisher_change_same_year'][\n",
    "    ['state', 'town', 'newspaper_name', 'publisher_change_year', 'editor_change_year']\n",
    "]\n",
    "editor_and_publisher_diff_list = valid_df[valid_df['category'] == 'editor_and_publisher_change_diff_year'][\n",
    "    ['state', 'town', 'newspaper_name', 'publisher_change_year', 'editor_change_year']\n",
    "]\n",
    "no_change_list = valid_df[valid_df['category'] == 'no_change'][\n",
    "    ['state', 'town', 'newspaper_name']\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DataFrames created:\")\n",
    "print(\"  - publisher_change_only_list\")\n",
    "print(\"  - editor_change_only_list\")\n",
    "print(\"  - editor_and_publisher_same_list\")\n",
    "print(\"  - editor_and_publisher_diff_list\")\n",
    "print(\"  - no_change_list\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaef444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samwt\\AppData\\Local\\Temp\\ipykernel_14772\\1763648295.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  master = pd.read_csv(\"data/master.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with master_id: 566\n",
      "Rows with publisher_change_year: 285\n",
      "49\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# filter down to newspapers that we can match \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "master = pd.read_csv(\"data/master.csv\")\n",
    "matches = pd.read_csv(\"data/matches.csv\")\n",
    "\n",
    "matches[\"publisher_change_year\"] = matches[\"master_id\"].dropna().astype(int).map(master[\"publisher_change_year\"])\n",
    "matches[\"editor_change_year\"] = matches[\"master_id\"].dropna().astype(int).map(master[\"editor_change_year\"])\n",
    "matches[\"category\"] = matches[\"master_id\"].dropna().astype(int).map(master[\"category\"])\n",
    "matches = matches[matches.master_id.notna()]\n",
    "matches.to_csv(\"data/final_list.csv\", index=False)\n",
    "\n",
    "print(f\"Rows with master_id: {matches['master_id'].notna().sum()}\")\n",
    "print(f\"Rows with publisher_change_year: {matches['publisher_change_year'].notna().sum()}\")\n",
    "\n",
    "print(len(matches[matches.category.str.contains('editor_change_only')]))\n",
    "print(len(matches[matches.category.str.contains('publisher_change_only')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6d4f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 71 diff-year papers from panel\n",
      "Median treatment year among treated papers: 1877.0\n",
      "Papers with ≥3 pre-treatment years: 313\n",
      "\n",
      "Sample of final panel:\n",
      "\n",
      " Newspaper_ID  Year  category  Post_it       Y_it  Y_lifecycle     Y_vol  Rel_Year\n",
      "          4.0  1877 no_change        0  42.951005    28.844740       NaN        -6\n",
      "          4.0  1878 no_change        0  11.705858    16.640013 43.564128        -5\n",
      "          4.0  1879 no_change        0  17.509141    17.888121 19.019220        -4\n",
      "          4.0  1880 no_change        0  19.699659    32.985300 27.094376        -3\n",
      "          4.0  1881 no_change        0  26.164697    42.832762 21.520588        -2\n",
      "          4.0  1882 no_change        0  20.727700    34.030956 22.411459        -1\n",
      "          4.0  1883 no_change        1  30.203680    43.337589 22.440928         0\n",
      "          4.0  1884 no_change        1  45.615109    46.038748 51.874028         1\n",
      "          4.0  1885 no_change        1  73.492058    66.489024 60.423300         2\n",
      "          4.0  1886 no_change        1  73.771808    66.951530 38.215715         3\n",
      "          4.0  1887 no_change        1  53.730919    60.490933 59.588225         4\n",
      "          4.0  1888 no_change        1  69.718602    77.984981 35.447737         5\n",
      "          4.0  1889 no_change        1 100.624361   108.285903 51.809623         6\n",
      "          4.0  1890 no_change        1  58.227718    71.106985 92.126290         7\n",
      "       3330.0  1869 no_change        0  28.901848    25.814964       NaN       -10\n",
      "       3330.0  1870 no_change        0  31.289667    31.593222 53.019081        -9\n",
      "       3330.0  1871 no_change        0  24.693600    22.761044 48.641353        -8\n",
      "       3330.0  1872 no_change        0  30.924282    29.985556 13.949653        -7\n",
      "       3330.0  1873 no_change        0  17.883405    15.870669 18.813283        -6\n",
      "       3330.0  1874 no_change        0  16.311888    18.658845 27.801500        -5\n",
      "\n",
      "--- Panel Summary ---\n",
      "Total observations: 2976\n",
      "Unique newspapers: 313\n",
      "Control papers (no_change): 213\n",
      "Treated papers (publisher_change_only): 31\n",
      "Treated papers (editor_change_only): 15\n",
      "Treated papers (editor_and_publisher_change_same_year): 54\n",
      "Pre-treatment obs: 1614\n",
      "Post-treatment obs: 1362\n",
      "\n",
      "--- Outcome Variable Summary ---\n",
      "Y_it: mean=27.354, sd=17.320, median=23.761, n=2976\n",
      "Y_lifecycle: mean=30.562, sd=19.247, median=26.793, n=2976\n",
      "Y_vol: mean=28.937, sd=18.029, median=25.374, n=2663\n",
      "\n",
      "Saved to 'panel_structural_drift2.csv'\n"
     ]
    }
   ],
   "source": [
    "# structural drift panel creation — multi-treatment DID\n",
    "# Updated: uses editor_and_publisher_change_same_year only\n",
    "#          (drops editor_and_publisher_change_diff_year observations)\n",
    "# Updated: topic_counts keyed by issn\n",
    "# Updated: adds year-over-year volatility (Y_vol) and lifecycle-anchored\n",
    "#          drift (Y_lifecycle) as alternative outcome measures to address\n",
    "#          the anchoring/variance-cone concern.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load data sources\n",
    "newspapers = pd.read_csv('data/final_list.csv')\n",
    "with open('data/topic_counts.json', 'r') as f:\n",
    "    topic_data = json.load(f)\n",
    "\n",
    "TOPICS = [\n",
    "    'labor_workers', 'politics_elections', 'congress_government',\n",
    "    'business_commerce', 'railroads_transportation', 'agriculture_farming',\n",
    "    'courts_law', 'finance_money', 'immigration_foreign', 'crime_police'\n",
    "]\n",
    "\n",
    "# Only these three categories are treated; diff-year papers are excluded entirely\n",
    "TREATMENT_CATEGORIES = [\n",
    "    'publisher_change_only',\n",
    "    'editor_change_only',\n",
    "    'editor_and_publisher_change_same_year',\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Build raw panel with topic rates per 1,000 headlines\n",
    "# =============================================================================\n",
    "records = []\n",
    "for year, papers in topic_data.items():\n",
    "    for issn, data in papers.items():\n",
    "        if 'topic_counts' in data and 'total_headlines' in data:\n",
    "            total = data['total_headlines']\n",
    "            if total >= 75:\n",
    "                record = {'year': int(year), 'issn': issn}\n",
    "                for topic in TOPICS:\n",
    "                    count = data['topic_counts'].get(topic, 0)\n",
    "                    record[topic] = (count / total) * 1000\n",
    "                records.append(record)\n",
    "\n",
    "panel = pd.DataFrame(records)\n",
    "\n",
    "# Merge with metadata\n",
    "panel = panel.merge(\n",
    "    newspapers[[\n",
    "        'issn', 'master_id', 'master_name',\n",
    "        'category', 'publisher_change_year', 'editor_change_year'\n",
    "    ]],\n",
    "    on='issn', how='left'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1.5: Drop diff-year papers entirely so they don't pollute control group\n",
    "# =============================================================================\n",
    "n_before = panel['master_id'].nunique()\n",
    "panel = panel[panel['category'] != 'editor_and_publisher_change_diff_year'].copy()\n",
    "n_after = panel['master_id'].nunique()\n",
    "print(f\"Dropped {n_before - n_after} diff-year papers from panel\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Determine treatment status and treatment year per paper\n",
    "# =============================================================================\n",
    "panel['is_treated'] = panel['category'].isin(TREATMENT_CATEGORIES)\n",
    "\n",
    "def get_treatment_year(row):\n",
    "    \"\"\"Return the earliest change year for a treated paper, or NaN for control.\"\"\"\n",
    "    if row['category'] not in TREATMENT_CATEGORIES:\n",
    "        return np.nan\n",
    "    years = []\n",
    "    if pd.notna(row['publisher_change_year']):\n",
    "        years.append(row['publisher_change_year'])\n",
    "    if pd.notna(row['editor_change_year']):\n",
    "        years.append(row['editor_change_year'])\n",
    "    return min(years) if years else np.nan\n",
    "\n",
    "panel['treatment_year'] = panel.apply(get_treatment_year, axis=1)\n",
    "\n",
    "# Adjust: subtract 1 so the treatment year marks the last pre-treatment year\n",
    "panel['treatment_year'] = panel['treatment_year'] - 1\n",
    "\n",
    "print(f\"Median treatment year among treated papers: \"\n",
    "      f\"{panel.loc[panel['is_treated'], 'treatment_year'].median()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: Create treatment group dummies\n",
    "# =============================================================================\n",
    "for cat in TREATMENT_CATEGORIES:\n",
    "    panel[f'is_{cat}'] = (panel['category'] == cat).astype(int)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 4: Define anchor cutoff year for each paper\n",
    "#         Treated  → treatment_year (already set above)\n",
    "#         Control  → max(first_year + 3, individual median year)\n",
    "# =============================================================================\n",
    "panel = panel.dropna(subset=['master_id']).copy()\n",
    "panel = panel.sort_values(['master_id', 'year']).reset_index(drop=True)\n",
    "\n",
    "paper_stats = panel.groupby('master_id')['year'].agg(['min', 'median'])\n",
    "paper_stats.columns = ['first_year', 'median_year']\n",
    "panel = panel.merge(paper_stats, on='master_id', how='left')\n",
    "\n",
    "panel['anchor_cutoff'] = np.where(\n",
    "    panel['is_treated'],\n",
    "    panel['treatment_year'],\n",
    "    np.maximum(panel['first_year'] + 3, panel['median_year'])\n",
    ").astype(int)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 5: Filter papers with at least 3 pre-treatment years\n",
    "# =============================================================================\n",
    "pre_counts = panel[panel['year'] < panel['anchor_cutoff']].groupby('master_id').size()\n",
    "valid_papers = pre_counts[pre_counts >= 3].index\n",
    "panel = panel[panel['master_id'].isin(valid_papers)].copy()\n",
    "print(f\"Papers with ≥3 pre-treatment years: {len(valid_papers)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 6: Calculate anchor vectors for BOTH anchoring strategies\n",
    "# =============================================================================\n",
    "\n",
    "# --- 6a: Original anchor (pre-cutoff mean) ---\n",
    "pre_panel = panel[panel['year'] < panel['anchor_cutoff']]\n",
    "anchors_original = pre_panel.groupby('master_id')[TOPICS].mean()\n",
    "anchors_original.columns = [f'anchor_{t}' for t in TOPICS]\n",
    "panel = panel.merge(anchors_original, on='master_id', how='left')\n",
    "\n",
    "# --- 6b: Lifecycle anchor (first 3 observed years for every paper) ---\n",
    "# This puts treated and control papers on the same footing: everyone's\n",
    "# anchor is their earliest topic distribution, so time-from-anchor is\n",
    "# comparable across groups.\n",
    "first_3 = (\n",
    "    panel.sort_values(['master_id', 'year'])\n",
    "    .groupby('master_id')\n",
    "    .head(3)\n",
    ")\n",
    "anchors_lifecycle = first_3.groupby('master_id')[TOPICS].mean()\n",
    "anchors_lifecycle.columns = [f'lifecycle_anchor_{t}' for t in TOPICS]\n",
    "panel = panel.merge(anchors_lifecycle, on='master_id', how='left')\n",
    "\n",
    "# Also store each paper's lifecycle anchor start year (for rel_year_lifecycle)\n",
    "lifecycle_start = first_3.groupby('master_id')['year'].max()\n",
    "lifecycle_start.name = 'lifecycle_anchor_end'\n",
    "panel = panel.merge(lifecycle_start, on='master_id', how='left')\n",
    "\n",
    "# =============================================================================\n",
    "# Step 7: Calculate outcome variables\n",
    "# =============================================================================\n",
    "\n",
    "# --- 7a: Y_it = original drift from pre-cutoff anchor ---\n",
    "def calc_drift(row, anchor_prefix='anchor'):\n",
    "    sq_diffs = sum((row[t] - row[f'{anchor_prefix}_{t}'])**2 for t in TOPICS)\n",
    "    return np.sqrt(sq_diffs)\n",
    "\n",
    "panel['Y_it'] = panel.apply(lambda r: calc_drift(r, 'anchor'), axis=1)\n",
    "\n",
    "# --- 7b: Y_lifecycle = drift from lifecycle (first-3-years) anchor ---\n",
    "panel['Y_lifecycle'] = panel.apply(\n",
    "    lambda r: calc_drift(r, 'lifecycle_anchor'), axis=1\n",
    ")\n",
    "\n",
    "# --- 7c: Y_vol = year-over-year volatility (Euclidean distance from t-1) ---\n",
    "# This sidesteps anchoring entirely: measures turbulence/instability.\n",
    "panel = panel.sort_values(['master_id', 'year']).reset_index(drop=True)\n",
    "\n",
    "topic_arr = panel[TOPICS].values\n",
    "# Shift within each paper group\n",
    "shifted = panel.groupby('master_id')[TOPICS].shift(1)\n",
    "diff = topic_arr - shifted.values\n",
    "panel['Y_vol'] = np.sqrt(np.nansum(diff**2, axis=1))\n",
    "\n",
    "# First observation per paper has no lag → NaN\n",
    "first_obs_mask = panel.groupby('master_id').cumcount() == 0\n",
    "panel.loc[first_obs_mask, 'Y_vol'] = np.nan\n",
    "\n",
    "# =============================================================================\n",
    "# Step 8: Create Post_it dummy and interaction terms\n",
    "# =============================================================================\n",
    "panel['Post_it'] = (panel['year'] >= panel['anchor_cutoff']).astype(int)\n",
    "\n",
    "for cat in TREATMENT_CATEGORIES:\n",
    "    panel[f'Post_x_{cat}'] = panel['Post_it'] * panel[f'is_{cat}']\n",
    "\n",
    "# =============================================================================\n",
    "# Step 9: Event-study relative time variables\n",
    "# =============================================================================\n",
    "# Original: relative to anchor_cutoff\n",
    "panel['rel_year'] = panel['year'] - panel['anchor_cutoff']\n",
    "\n",
    "# Lifecycle: relative to end of lifecycle anchor window\n",
    "panel['rel_year_lifecycle'] = panel['year'] - panel['lifecycle_anchor_end']\n",
    "\n",
    "# =============================================================================\n",
    "# Step 10: Build final output table\n",
    "# =============================================================================\n",
    "output_cols = [\n",
    "    'master_id', 'master_name', 'year', 'category',\n",
    "    'is_treated', 'anchor_cutoff', 'Post_it',\n",
    "    'Y_it', 'Y_lifecycle', 'Y_vol',\n",
    "    'rel_year', 'rel_year_lifecycle',\n",
    "] + [f'is_{cat}' for cat in TREATMENT_CATEGORIES] \\\n",
    "  + [f'Post_x_{cat}' for cat in TREATMENT_CATEGORIES]\n",
    "\n",
    "output = panel[output_cols].rename(columns={\n",
    "    'master_id': 'Newspaper_ID',\n",
    "    'master_name': 'Newspaper_Name',\n",
    "    'year': 'Year',\n",
    "    'anchor_cutoff': 'Anchor_Cutoff_Year',\n",
    "    'rel_year': 'Rel_Year',\n",
    "    'rel_year_lifecycle': 'Rel_Year_Lifecycle',\n",
    "}).sort_values(['Newspaper_ID', 'Year']).reset_index(drop=True)\n",
    "\n",
    "# Clean up intermediate anchor columns before saving\n",
    "panel.drop(\n",
    "    columns=[f'anchor_{t}' for t in TOPICS]\n",
    "    + [f'lifecycle_anchor_{t}' for t in TOPICS]\n",
    "    + ['first_year', 'median_year', 'lifecycle_anchor_end'],\n",
    "    inplace=True, errors='ignore'\n",
    ")\n",
    "\n",
    "# Display diagnostics\n",
    "print(\"\\nSample of final panel:\\n\")\n",
    "print(output[['Newspaper_ID', 'Year', 'category', 'Post_it',\n",
    "              'Y_it', 'Y_lifecycle', 'Y_vol', 'Rel_Year']].head(20).to_string(index=False))\n",
    "\n",
    "print(f\"\\n--- Panel Summary ---\")\n",
    "print(f\"Total observations: {len(output)}\")\n",
    "print(f\"Unique newspapers: {output['Newspaper_ID'].nunique()}\")\n",
    "\n",
    "treated_counts = panel[panel['is_treated']].groupby('category')['master_id'].nunique()\n",
    "control_count = panel[~panel['is_treated']]['master_id'].nunique()\n",
    "print(f\"Control papers (no_change): {control_count}\")\n",
    "for cat in TREATMENT_CATEGORIES:\n",
    "    count = treated_counts.get(cat, 0)\n",
    "    print(f\"Treated papers ({cat}): {count}\")\n",
    "\n",
    "print(f\"Pre-treatment obs: {(output['Post_it'] == 0).sum()}\")\n",
    "print(f\"Post-treatment obs: {(output['Post_it'] == 1).sum()}\")\n",
    "\n",
    "# Outcome variable summary\n",
    "print(f\"\\n--- Outcome Variable Summary ---\")\n",
    "for y_var in ['Y_it', 'Y_lifecycle', 'Y_vol']:\n",
    "    valid = output[y_var].dropna()\n",
    "    print(f\"{y_var}: mean={valid.mean():.3f}, sd={valid.std():.3f}, \"\n",
    "          f\"median={valid.median():.3f}, n={len(valid)}\")\n",
    "\n",
    "# Save\n",
    "output.to_csv('data/panel_structural_drift2.csv', index=False)\n",
    "print(\"\\nSaved to 'panel_structural_drift2.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca590c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
