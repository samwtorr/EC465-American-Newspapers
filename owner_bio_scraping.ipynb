{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ee1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samwt\\AppData\\Local\\Temp\\ipykernel_18716\\453501286.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  master = pd.read_csv('data/master.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2088 rows to data/owners_and_editors.csv\n",
      "  Publishers: 1075\n",
      "  Editors:    1013\n",
      "  Newspapers: 545\n",
      "\n",
      "Sample rows:\n",
      "newspaper_name      role                                                                                        name                                                                        years\n",
      "       Tribune publisher                                                                          TribuneAssociation                   1871; 1873; 1876; 1877; 1879; 1880; 1882; 1883; 1884; 1885\n",
      "       Tribune publisher                                                       Tribune circulation-daily Association                                                                         1878\n",
      "       Tribune publisher                                                                               Whitelaw Reid                                                                         1890\n",
      "       Tribune    editor Charles A. Dana,George Ripley,Bayard Taylor,Henry J. Raymond,Horace Greeley,Margaret Fuller                                                                   1869; 1871\n",
      "       Tribune    editor                                                                               Whitelaw Reid                   1873; 1876; 1877; 1878; 1879; 1880; 1882; 1883; 1884; 1885\n",
      "  Evening Star publisher                                                                                Crosby Noyes                                                 1869; 1871; 1872; 1873; 1890\n",
      "  Evening Star publisher                                                                      DISTRICT OF COLUMBIA .                                                                         1876\n",
      "  Evening Star publisher                                                              Evening Star Newspaper Company                                           1877; 1878; 1879; 1880; 1882; 1883\n",
      "  Evening Star    editor                                                                                Crosby Noyes 1869; 1871; 1872; 1873; 1877; 1878; 1879; 1880; 1882; 1883; 1884; 1885; 1890\n",
      "       Gazette publisher                                                                               Edgar Snowden 1869; 1871; 1872; 1873; 1876; 1877; 1878; 1879; 1880; 1882; 1883; 1884; 1885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "master = pd.read_csv('data/master.csv')\n",
    "matches = pd.read_csv('data/matches.csv')\n",
    "\n",
    "years = [1869, 1871, 1872, 1873, 1876, 1877, 1878, 1879, 1880, 1882, 1883, 1884, 1885, 1890]\n",
    "\n",
    "# --- Matching logic ---\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = prev_row[j + 1] + 1\n",
    "            deletions = curr_row[j] + 1\n",
    "            substitutions = prev_row[j] + (c1 != c2)\n",
    "            curr_row.append(min(insertions, deletions, substitutions))\n",
    "        prev_row = curr_row\n",
    "    return prev_row[-1]\n",
    "\n",
    "def strings_match(s1, s2, max_distance=1):\n",
    "    s1_clean = s1.strip().lower()\n",
    "    s2_clean = s2.strip().lower()\n",
    "    if s1_clean == s2_clean:\n",
    "        return True\n",
    "    return levenshtein_distance(s1_clean, s2_clean) <= max_distance\n",
    "\n",
    "def tokenize_publisher(publisher_str):\n",
    "    if not publisher_str:\n",
    "        return []\n",
    "    cleaned = re.sub(r'[;,]', ' ', publisher_str)\n",
    "    tokens = cleaned.split()\n",
    "    return [t.strip() for t in tokens if len(t.strip()) >= 4]\n",
    "\n",
    "def publishers_match_tokenized(pub1, pub2):\n",
    "    tokens1 = tokenize_publisher(pub1)\n",
    "    tokens2 = tokenize_publisher(pub2)\n",
    "    if not tokens1 or not tokens2:\n",
    "        return False\n",
    "    for t1 in tokens1:\n",
    "        for t2 in tokens2:\n",
    "            if strings_match(t1, t2, max_distance=1):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def normalize_publisher(pub):\n",
    "    if not pub:\n",
    "        return \"\"\n",
    "    return re.sub(r'[^a-z0-9]', '', pub.lower())\n",
    "\n",
    "def publishers_match_normalized(pub1, pub2):\n",
    "    n1 = normalize_publisher(pub1)\n",
    "    n2 = normalize_publisher(pub2)\n",
    "    if not n1 or not n2:\n",
    "        return False\n",
    "    return n1 == n2 or n1 in n2 or n2 in n1\n",
    "\n",
    "def names_match(name1, name2):\n",
    "    return publishers_match_tokenized(name1, name2) or publishers_match_normalized(name1, name2)\n",
    "\n",
    "def clean_field(value):\n",
    "    if pd.isna(value):\n",
    "        return ''\n",
    "    s = str(value).strip()\n",
    "    if s.lower() == 'nan':\n",
    "        return ''\n",
    "    return s\n",
    "\n",
    "def remove_blips(data_points):\n",
    "    \"\"\"\n",
    "    Remove single-entry 'blips' that are likely data entry errors.\n",
    "    A blip is where value changes at index i but reverts back at index i+1\n",
    "    (i.e., prev and next match each other, but current doesn't match prev).\n",
    "    \"\"\"\n",
    "    if len(data_points) <= 2:\n",
    "        return data_points\n",
    "\n",
    "    filtered = []\n",
    "    i = 0\n",
    "    while i < len(data_points):\n",
    "        if 0 < i < len(data_points) - 1:\n",
    "            prev_year, prev_val = data_points[i - 1]\n",
    "            curr_year, curr_val = data_points[i]\n",
    "            next_year, next_val = data_points[i + 1]\n",
    "            if not names_match(prev_val, curr_val) and names_match(prev_val, next_val):\n",
    "                i += 1\n",
    "                continue\n",
    "        filtered.append(data_points[i])\n",
    "        i += 1\n",
    "    return filtered\n",
    "\n",
    "def get_distinct_names(values):\n",
    "    \"\"\"\n",
    "    Given a list of (year, name) tuples, return a list of distinct names\n",
    "    using names_match() to deduplicate. Keeps the first occurrence as the\n",
    "    representative string. Also returns the years each distinct name appears.\n",
    "    \"\"\"\n",
    "    distinct = []  # list of (representative_name, [years])\n",
    "    for year, name in values:\n",
    "        found = False\n",
    "        for i, (rep, yr_list) in enumerate(distinct):\n",
    "            if names_match(rep, name):\n",
    "                yr_list.append(year)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            distinct.append((name, [year]))\n",
    "    return distinct\n",
    "\n",
    "def extract_for_row(row):\n",
    "    pub_data = []\n",
    "    ed_data = []\n",
    "    for year in years:\n",
    "        publisher = clean_field(row.get(f'{year} publisher', ''))\n",
    "        editor = clean_field(row.get(f'{year} editor', ''))\n",
    "        if publisher:\n",
    "            pub_data.append((year, publisher))\n",
    "        if editor:\n",
    "            ed_data.append((year, editor))\n",
    "\n",
    "    # Remove blips before deduplicating\n",
    "    pub_data = remove_blips(pub_data)\n",
    "    ed_data = remove_blips(ed_data)\n",
    "\n",
    "    return get_distinct_names(pub_data), get_distinct_names(ed_data)\n",
    "\n",
    "# Build one row per distinct publisher or editor\n",
    "rows_out = []\n",
    "for _, match_row in matches.iterrows():\n",
    "    if pd.isna(match_row['master_id']):\n",
    "        continue\n",
    "    master_id = int(match_row['master_id'])\n",
    "    issn = match_row.get('issn', '')\n",
    "    newspapers_all_years_name = match_row.get('newspapers_all_years_name', '')\n",
    "    master_name = match_row.get('master_name', '')\n",
    "\n",
    "    if master_id < 0 or master_id >= len(master):\n",
    "        continue\n",
    "\n",
    "    m_row = master.iloc[master_id]\n",
    "    state = clean_field(m_row.get('state', ''))\n",
    "    town = clean_field(m_row.get('town', ''))\n",
    "    newspaper_name = clean_field(m_row.get('newspaper_name', ''))\n",
    "\n",
    "    distinct_pubs, distinct_eds = extract_for_row(m_row)\n",
    "\n",
    "    base = {\n",
    "        'master_id': master_id,\n",
    "        'issn': issn,\n",
    "        'newspapers_all_years_name': newspapers_all_years_name,\n",
    "        'master_name': master_name,\n",
    "        'state': state,\n",
    "        'town': town,\n",
    "        'newspaper_name': newspaper_name,\n",
    "    }\n",
    "\n",
    "    for name, yr_list in distinct_pubs:\n",
    "        rows_out.append({\n",
    "            **base,\n",
    "            'role': 'publisher',\n",
    "            'name': name,\n",
    "            'years': '; '.join(str(y) for y in yr_list),\n",
    "            'first_year': min(yr_list),\n",
    "            'last_year': max(yr_list),\n",
    "            'num_years': len(yr_list),\n",
    "        })\n",
    "\n",
    "    for name, yr_list in distinct_eds:\n",
    "        rows_out.append({\n",
    "            **base,\n",
    "            'role': 'editor',\n",
    "            'name': name,\n",
    "            'years': '; '.join(str(y) for y in yr_list),\n",
    "            'first_year': min(yr_list),\n",
    "            'last_year': max(yr_list),\n",
    "            'num_years': len(yr_list),\n",
    "        })\n",
    "\n",
    "out_df = pd.DataFrame(rows_out)\n",
    "out_df.insert(0, 'unique_id', range(1, len(out_df) + 1))\n",
    "out_df.to_csv('data/owners_and_editors.csv', index=False)\n",
    "\n",
    "print(f\"Wrote {len(out_df)} rows to data/owners_and_editors.csv\")\n",
    "print(f\"  Publishers: {len(out_df[out_df['role'] == 'publisher'])}\")\n",
    "print(f\"  Editors:    {len(out_df[out_df['role'] == 'editor'])}\")\n",
    "print(f\"  Newspapers: {out_df['master_id'].nunique()}\")\n",
    "print(\"\\nSample rows:\")\n",
    "print(out_df[['newspaper_name', 'role', 'name', 'years']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269c044f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input rows: 2088\n",
      "Distinct names: 1312\n",
      "\n",
      "Names appearing more than once:\n",
      " person_id                                       name  num_entries\n",
      "        67                               Register Co.            7\n",
      "       993                        Herman Stockenstrom            6\n",
      "       846                             Ben F. Stanton            6\n",
      "       844                              David W. Cobb            6\n",
      "       142                              David Higgins            5\n",
      "        83                                E.D. Kelley            5\n",
      "        68                 West Virginia Printing Co.            5\n",
      "        24                                Lewis Baker            5\n",
      "       613                             John H. Marion            5\n",
      "       980                    Edward Alexander Oldham            4\n",
      "       955                            Robert D. Blair            4\n",
      "       948                              John W. Kelly            4\n",
      "      1015                      Clement A. Lounsberry            4\n",
      "       979                           George M. Mathes            4\n",
      "      1168                  Winchell Mansfield French            4\n",
      "       147                           Thaddeus Hanford            4\n",
      "       918 James Loyal Sims, Stiles Rivers Mellichamp            4\n",
      "       883                          Alfred S. Horsley            4\n",
      "       935                     Tribune Publishing Co.            4\n",
      "        22                             Harlan P. Hall            4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('data/owners_and_editors.csv')\n",
    "\n",
    "# --- Same matching logic ---\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            curr_row.append(min(prev_row[j+1]+1, curr_row[j]+1, prev_row[j]+(c1!=c2)))\n",
    "        prev_row = curr_row\n",
    "    return prev_row[-1]\n",
    "\n",
    "def names_match(a, b):\n",
    "    \"\"\"Only match if the entire string is at most 1 edit away.\"\"\"\n",
    "    a_clean = a.strip().lower()\n",
    "    b_clean = b.strip().lower()\n",
    "    if not a_clean or not b_clean:\n",
    "        return False\n",
    "    return a_clean == b_clean or levenshtein_distance(a_clean, b_clean) <= 1\n",
    "\n",
    "# Deduplicate names across all rows using names_match()\n",
    "# Each distinct name group gets a person_id and keeps the first occurrence as representative\n",
    "distinct = []  # list of (person_id, representative_name, [unique_ids])\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    name = str(row['name']).strip()\n",
    "    uid = row['unique_id']\n",
    "    if not name:\n",
    "        continue\n",
    "\n",
    "    found = False\n",
    "    for i, (pid, rep, uid_list) in enumerate(distinct):\n",
    "        if names_match(rep, name):\n",
    "            uid_list.append(uid)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        distinct.append((len(distinct) + 1, name, [uid]))\n",
    "\n",
    "# Build output: one row per distinct person, with all linked unique_ids\n",
    "rows_out = []\n",
    "for pid, rep_name, uid_list in distinct:\n",
    "    rows_out.append({\n",
    "        'person_id': pid,\n",
    "        'name': rep_name,\n",
    "        'unique_ids': '; '.join(str(u) for u in uid_list),\n",
    "        'num_entries': len(uid_list),\n",
    "    })\n",
    "\n",
    "out_df = pd.DataFrame(rows_out)\n",
    "out_df.to_csv('data/distinct_names.csv', index=False)\n",
    "\n",
    "print(f\"Input rows: {len(df)}\")\n",
    "print(f\"Distinct names: {len(out_df)}\")\n",
    "print(f\"\\nNames appearing more than once:\")\n",
    "multi = out_df[out_df['num_entries'] > 1].sort_values('num_entries', ascending=False)\n",
    "print(multi[['person_id', 'name', 'num_entries']].head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5035fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/6 (rows 1-250)...\n",
      "  -> 264 cleaned names\n",
      "Batch 2/6 (rows 251-500)...\n",
      "  -> 290 cleaned names\n",
      "Batch 3/6 (rows 501-750)...\n",
      "  -> 304 cleaned names\n",
      "Batch 4/6 (rows 751-1000)...\n",
      "  -> 294 cleaned names\n",
      "Batch 5/6 (rows 1001-1250)...\n",
      "  -> 298 cleaned names\n",
      "Batch 6/6 (rows 1251-1312)...\n",
      "  -> 70 cleaned names\n",
      "\n",
      "Done! 1312 input entries -> 1520 cleaned names\n",
      "Saved to data/cleaned_distinct_names.csv\n",
      "\n",
      "Sample:\n",
      " person_id             cleaned_name unique_ids\n",
      "         3            Whitelaw Reid       3; 5\n",
      "         4          Charles A. Dana          4\n",
      "         4            George Ripley          4\n",
      "         4            Bayard Taylor          4\n",
      "         4         Henry J. Raymond          4\n",
      "         4           Horace Greeley          4\n",
      "         4          Margaret Fuller          4\n",
      "         5             Crosby Noyes       6; 9\n",
      "         8            Edgar Snowden     10; 11\n",
      "         9                     Frew     12; 13\n",
      "         9                   Hagans     12; 13\n",
      "         9                     Hall     12; 13\n",
      "         9              Lewis Baker     12; 13\n",
      "        10                    Caine     14; 17\n",
      "        10                    Sloan     14; 17\n",
      "        10                   Dunbar     14; 17\n",
      "        12        William C. Dunbar         16\n",
      "        13     James Gordon Bennett         19\n",
      "        14           A. N. Merchant         20\n",
      "        15 James Wellington Latimer     21; 27\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash-lite\")\n",
    "\n",
    "df = pd.read_csv('data/distinct_names.csv')\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"You are cleaning a list of newspaper owner/editor names from the 1860s-1890s.\n",
    "For each numbered entry, do the following:\n",
    "1. If the entry is a real person's name (or multiple people's names), return the corrected name(s).\n",
    "2. Fix obvious typos and formatting issues (e.g. \"Jonh\" -> \"John\", missing spaces). But be very cautious here - don't change names on hunches.\n",
    "3. If the entry contains MULTIPLE people's names (separated by commas, semicolons, \"and\", \"&\", etc.), split them into separate names.\n",
    "4. If the entry is NOT a person's name (e.g. it's a company name like \"Tribune Association\", \"Printing Co.\", an organization, a place name, gibberish, or just a title/abbreviation with no real name), return an empty list.\n",
    "5. Keep honorifics like \"Dr.\" or \"Col.\" if they appear to be part of a real person's name.\n",
    "\n",
    "Return ONLY a JSON object mapping the entry number (as a string) to a list of cleaned names.\n",
    "Example input:\n",
    "1. Charles A. Dana,George Ripley,Bayard Taylor\n",
    "2. TribuneAssociation\n",
    "3. Jonh Smtih\n",
    "4. Caine, Sloan & Dun- bar\n",
    "\n",
    "Example output:\n",
    "{{\"1\": [\"Charles A. Dana\", \"George Ripley\", \"Bayard Taylor\"], \"2\": [], \"3\": [\"John Smith\"], \"4\": [\"Caine\", \"Sloan\", \"Dunbar\"]}}\n",
    "\n",
    "Here are the entries to clean:\n",
    "\"\"\"\n",
    "\n",
    "def clean_batch(batch_df):\n",
    "    \"\"\"Send a batch of names to Gemini for cleaning. Returns list of (person_id, cleaned_name, unique_ids).\"\"\"\n",
    "    lines = []\n",
    "    pid_map = {}  # entry_number -> (person_id, unique_ids)\n",
    "    for i, (_, row) in enumerate(batch_df.iterrows(), 1):\n",
    "        lines.append(f\"{i}. {row['name']}\")\n",
    "        pid_map[str(i)] = (row['person_id'], row['unique_ids'])\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE + \"\\n\".join(lines)\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = client.models.generate_content(model=MODEL, contents=prompt)\n",
    "            text = response.text.strip()\n",
    "            # Extract JSON from response (handle markdown code blocks)\n",
    "            if text.startswith(\"```\"):\n",
    "                text = text.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "            result = json.loads(text)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {attempt+1} failed: {e}\")\n",
    "            if attempt < 2:\n",
    "                time.sleep(5 * (attempt + 1))\n",
    "            else:\n",
    "                print(f\"  Skipping batch after 3 failures\")\n",
    "                return []\n",
    "\n",
    "    cleaned_rows = []\n",
    "    for entry_num, (pid, uids) in pid_map.items():\n",
    "        names = result.get(entry_num, result.get(int(entry_num), []))\n",
    "        if isinstance(names, str):\n",
    "            names = [names]\n",
    "        for name in names:\n",
    "            name = name.strip()\n",
    "            if name:\n",
    "                cleaned_rows.append({\n",
    "                    'person_id': pid,\n",
    "                    'cleaned_name': name,\n",
    "                    'unique_ids': uids,\n",
    "                })\n",
    "    return cleaned_rows\n",
    "\n",
    "# Process in batches\n",
    "all_cleaned = []\n",
    "num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start = i * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, len(df))\n",
    "    batch = df.iloc[start:end]\n",
    "    print(f\"Batch {i+1}/{num_batches} (rows {start+1}-{end})...\")\n",
    "    cleaned = clean_batch(batch)\n",
    "    all_cleaned.extend(cleaned)\n",
    "    print(f\"  -> {len(cleaned)} cleaned names\")\n",
    "    if i < num_batches - 1:\n",
    "        time.sleep(2)\n",
    "\n",
    "cleaned_df = pd.DataFrame(all_cleaned)\n",
    "cleaned_df.to_csv('data/cleaned_distinct_names.csv', index=False)\n",
    "\n",
    "print(f\"\\nDone! {len(df)} input entries -> {len(cleaned_df)} cleaned names\")\n",
    "print(f\"Saved to data/cleaned_distinct_names.csv\")\n",
    "print(f\"\\nSample:\")\n",
    "print(cleaned_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72b108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
