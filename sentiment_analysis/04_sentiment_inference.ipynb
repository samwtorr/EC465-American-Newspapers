{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Two-Pass Sentiment Inference\n",
    "\n",
    "Apply two fine-tuned DistilBERT models to all classified articles:\n",
    "- **Pass 1 (Labor)**: labor_only + both \u2192 labor_stance_model \u2192 labor_sentiment\n",
    "- **Pass 2 (Railroad)**: railroad_only + both \u2192 railroad_outlook_model \u2192 railroad_sentiment\n",
    "\n",
    "\"Both\" articles are scored on **both axes** (richest data points).\n",
    "\n",
    "Features: **ONNX Runtime**, **keyword-centered truncation**, **checkpointing** every 10K articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"data/inference_log.log\")\n",
    "    ]\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "LABOR_MODEL_PATH = Path('models/labor_stance_model')\n",
    "RAILROAD_MODEL_PATH = Path('models/railroad_outlook_model')\n",
    "CLASSIFIED_DIR = Path('data/classified_articles')\n",
    "OUTPUT_DIR = Path('data/sentiment_results')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = OUTPUT_DIR / 'all_sentiment_results.json'\n",
    "CHECKPOINT_INTERVAL = 10_000\n",
    "\n",
    "LABOR_LABEL_MAP = {0: 'pro_labor', 1: 'anti_labor', 2: 'neutral'}\n",
    "RAILROAD_LABEL_MAP = {0: 'optimistic', 1: 'pessimistic', 2: 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(filepath, data):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Keyword definitions for keyword-centered truncation ---\n",
    "LABOR_KEYWORDS = {\n",
    "    'labor union': 3, 'trade union': 3, 'labor strike': 3, 'labor riot': 3,\n",
    "    'collective bargaining': 3, 'labor movement': 3, 'strikebreaker': 3,\n",
    "    'scab labor': 3, 'working men': 3, 'workingmen': 3,\n",
    "    'knights of labor': 3, 'eight hour': 3,\n",
    "    'striker': 2, 'strikers': 2, 'picket': 2, 'lockout': 2,\n",
    "    'boycott': 2, 'walkout': 2, 'arbitration': 2, 'picketing': 2,\n",
    "    'strike': 1, 'strikes': 1, 'wage': 1, 'wages': 1,\n",
    "    'workers': 1, 'laborers': 1,\n",
    "}\n",
    "RAILROAD_KEYWORDS = {\n",
    "    'railroad company': 3, 'railroad strike': 3, 'railroad workers': 3,\n",
    "    'railway company': 3, 'union pacific': 3, 'central pacific': 3,\n",
    "    'northern pacific': 3, 'pennsylvania railroad': 3,\n",
    "    'baltimore and ohio': 3, 'railroad line': 3,\n",
    "    'locomotive': 2, 'locomotives': 2, 'brakeman': 2,\n",
    "    'freight car': 2, 'passenger car': 2, 'rail road': 2,\n",
    "    'railroad': 1, 'railway': 1, 'train': 1, 'trains': 1,\n",
    "}\n",
    "\n",
    "LABOR_PATTERNS = {kw: (re.compile(r'\\b' + re.escape(kw) + r'\\b', re.IGNORECASE), w)\n",
    "                  for kw, w in LABOR_KEYWORDS.items()}\n",
    "RAILROAD_PATTERNS = {kw: (re.compile(r'\\b' + re.escape(kw) + r'\\b', re.IGNORECASE), w)\n",
    "                     for kw, w in RAILROAD_KEYWORDS.items()}\n",
    "\n",
    "\n",
    "def best_keyword_position(text, axis):\n",
    "    \"\"\"Find char position of highest-weighted keyword for the given axis.\"\"\"\n",
    "    patterns = LABOR_PATTERNS if axis == 'labor' else RAILROAD_PATTERNS\n",
    "    best_pos, best_weight = 0, 0\n",
    "    for kw, (regex, weight) in patterns.items():\n",
    "        match = regex.search(text)\n",
    "        if match and weight > best_weight:\n",
    "            best_weight = weight\n",
    "            best_pos = match.start()\n",
    "    return best_pos\n",
    "\n",
    "\n",
    "print(\"Keyword patterns loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LABOR_MODEL_PATH)\n",
    "\n",
    "\n",
    "def smart_truncate(text: str, axis: str, max_length: int = 512) -> str:\n",
    "    \"\"\"Keyword-centered truncation: keep tokens around the best keyword match.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) <= max_length - 2:\n",
    "        return text\n",
    "\n",
    "    center_char = best_keyword_position(text, axis)\n",
    "    center_token = int(len(tokens) * center_char / len(text)) if len(text) > 0 else 0\n",
    "\n",
    "    window = max_length - 2\n",
    "    half = window // 2\n",
    "    start = max(0, center_token - half)\n",
    "    end = start + window\n",
    "    if end > len(tokens):\n",
    "        end = len(tokens)\n",
    "        start = max(0, end - window)\n",
    "\n",
    "    return tokenizer.convert_tokens_to_string(tokens[start:end])\n",
    "\n",
    "\n",
    "def load_model(model_path, model_name):\n",
    "    \"\"\"Load a model with ONNX if available, else PyTorch fallback.\"\"\"\n",
    "    use_onnx = False\n",
    "    try:\n",
    "        from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "        onnx_path = model_path / 'model.onnx'\n",
    "        if not onnx_path.exists():\n",
    "            log.info(f\"Converting {model_name} to ONNX format...\")\n",
    "            model = ORTModelForSequenceClassification.from_pretrained(\n",
    "                model_path, export=True\n",
    "            )\n",
    "            model.save_pretrained(model_path)\n",
    "            log.info(\"ONNX conversion complete.\")\n",
    "        else:\n",
    "            model = ORTModelForSequenceClassification.from_pretrained(model_path)\n",
    "        use_onnx = True\n",
    "        print(f\"{model_name}: Using ONNX Runtime\")\n",
    "    except ImportError:\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        model.eval()\n",
    "        print(f\"{model_name}: Using PyTorch (install optimum for 2-3x speedup)\")\n",
    "\n",
    "    metadata_path = model_path / 'training_metadata.json'\n",
    "    if metadata_path.exists():\n",
    "        metadata = load_json(metadata_path)\n",
    "        print(f\"  Val accuracy: {metadata.get('val_accuracy', 'N/A')}\")\n",
    "\n",
    "    return model, use_onnx\n",
    "\n",
    "\n",
    "labor_model, labor_onnx = load_model(LABOR_MODEL_PATH, 'Labor Stance')\n",
    "railroad_model, railroad_onnx = load_model(RAILROAD_MODEL_PATH, 'Railroad Outlook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(texts, model, axis, use_onnx, label_map, batch_size=32):\n",
    "    \"\"\"Predict sentiment for a batch of texts using keyword-centered truncation.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = [smart_truncate(t, axis) for t in texts[i:i+batch_size]]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        if use_onnx:\n",
    "            outputs = model(**inputs)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits_np = logits.numpy()\n",
    "        else:\n",
    "            logits_np = np.array(logits)\n",
    "\n",
    "        predictions = np.argmax(logits_np, axis=-1)\n",
    "        exp_logits = np.exp(logits_np - np.max(logits_np, axis=-1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    "        for j, pred in enumerate(predictions):\n",
    "            results.append({\n",
    "                'sentiment': label_map[int(pred)],\n",
    "                'confidence': round(float(probs[j][pred]), 3),\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Quick sanity test\n",
    "test_texts = [\n",
    "    \"The brave workers stood firm against the tyrannical factory owners in the labor strike.\",\n",
    "    \"The riotous mob of strikers destroyed property and threatened honest citizens.\",\n",
    "    \"Workers at the Pullman factory went on strike Tuesday morning.\",\n",
    "]\n",
    "print(\"Labor model test:\")\n",
    "for text, r in zip(test_texts, predict_batch(test_texts, labor_model, 'labor', labor_onnx, LABOR_LABEL_MAP)):\n",
    "    print(f\"  [{r['sentiment']:>12}] ({r['confidence']:.2f}) {text[:70]}\")\n",
    "\n",
    "rr_texts = [\n",
    "    \"The new transcontinental railroad line will bring prosperity to the western territories.\",\n",
    "    \"Another terrible railroad accident claimed twelve lives yesterday.\",\n",
    "    \"The Pennsylvania Railroad reported quarterly earnings on Monday.\",\n",
    "]\n",
    "print(\"\\nRailroad model test:\")\n",
    "for text, r in zip(rr_texts, predict_batch(rr_texts, railroad_model, 'railroad', railroad_onnx, RAILROAD_LABEL_MAP)):\n",
    "    print(f\"  [{r['sentiment']:>12}] ({r['confidence']:.2f}) {text[:70]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Classified Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labor_only = load_json(CLASSIFIED_DIR / 'labor_only.json')\n",
    "railroad_only = load_json(CLASSIFIED_DIR / 'railroad_only.json')\n",
    "both = load_json(CLASSIFIED_DIR / 'both.json')\n",
    "\n",
    "print(f\"Labor-only:    {len(labor_only):,}\")\n",
    "print(f\"Railroad-only: {len(railroad_only):,}\")\n",
    "print(f\"Both:          {len(both):,}\")\n",
    "\n",
    "labor_pass_articles = labor_only + both\n",
    "railroad_pass_articles = railroad_only + both\n",
    "\n",
    "print(f\"\\nLabor pass:    {len(labor_pass_articles):,} articles\")\n",
    "print(f\"Railroad pass: {len(railroad_pass_articles):,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Pass Inference\n",
    "\n",
    "Each pass has its own checkpoint file for independent resumability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_pass(articles, model, axis, use_onnx, label_map, checkpoint_path):\n",
    "    \"\"\"Run inference on articles with checkpointing. Returns dict: article_id -> {sentiment, confidence}.\"\"\"\n",
    "    results = {}\n",
    "    start_idx = 0\n",
    "\n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = load_json(checkpoint_path)\n",
    "        results = checkpoint['results']\n",
    "        start_idx = checkpoint['processed']\n",
    "        log.info(f\"Resuming {axis} pass: {start_idx:,} already done\")\n",
    "    else:\n",
    "        log.info(f\"Starting {axis} pass: {len(articles):,} articles\")\n",
    "\n",
    "    CHUNK_SIZE = 1000\n",
    "    BATCH_SIZE = 32\n",
    "    start_time = time.time()\n",
    "\n",
    "    for chunk_start in tqdm(range(start_idx, len(articles), CHUNK_SIZE),\n",
    "                            initial=start_idx // CHUNK_SIZE,\n",
    "                            total=(len(articles) + CHUNK_SIZE - 1) // CHUNK_SIZE,\n",
    "                            desc=f\"{axis} inference\"):\n",
    "        chunk = articles[chunk_start:chunk_start + CHUNK_SIZE]\n",
    "        texts = [a['text'] for a in chunk]\n",
    "\n",
    "        preds = predict_batch(texts, model, axis, use_onnx, label_map, batch_size=BATCH_SIZE)\n",
    "\n",
    "        for article, pred in zip(chunk, preds):\n",
    "            results[article['article_id']] = pred\n",
    "\n",
    "        processed = chunk_start + len(chunk)\n",
    "\n",
    "        if processed % CHECKPOINT_INTERVAL == 0:\n",
    "            save_json(checkpoint_path, {'processed': processed, 'results': results})\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (processed - start_idx) / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(articles) - processed) / rate if rate > 0 else 0\n",
    "            log.info(f\"Checkpoint ({axis}): {processed:,}/{len(articles):,} \"\n",
    "                     f\"({rate:.0f}/sec, ~{remaining/3600:.1f}h left)\")\n",
    "\n",
    "    if checkpoint_path.exists():\n",
    "        os.remove(checkpoint_path)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    log.info(f\"{axis} pass done: {len(results):,} articles in {elapsed/60:.1f}min\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 1: Labor model on labor_only + both\n",
    "print(\"=== PASS 1: Labor Stance ===\")\n",
    "labor_results = run_inference_pass(\n",
    "    articles=labor_pass_articles,\n",
    "    model=labor_model,\n",
    "    axis='labor',\n",
    "    use_onnx=labor_onnx,\n",
    "    label_map=LABOR_LABEL_MAP,\n",
    "    checkpoint_path=OUTPUT_DIR / 'checkpoint_labor.json',\n",
    ")\n",
    "print(f\"Labor results: {len(labor_results):,} articles scored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 2: Railroad model on railroad_only + both\n",
    "print(\"=== PASS 2: Railroad Outlook ===\")\n",
    "railroad_results = run_inference_pass(\n",
    "    articles=railroad_pass_articles,\n",
    "    model=railroad_model,\n",
    "    axis='railroad',\n",
    "    use_onnx=railroad_onnx,\n",
    "    label_map=RAILROAD_LABEL_MAP,\n",
    "    checkpoint_path=OUTPUT_DIR / 'checkpoint_railroad.json',\n",
    ")\n",
    "print(f\"Railroad results: {len(railroad_results):,} articles scored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Results\n",
    "\n",
    "Combine both passes into a single output. \"Both\" articles get both columns filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = labor_only + railroad_only + both\n",
    "article_info = {a['article_id']: a for a in all_articles}\n",
    "\n",
    "merged = []\n",
    "for article_id, info in article_info.items():\n",
    "    labor_pred = labor_results.get(article_id)\n",
    "    rr_pred = railroad_results.get(article_id)\n",
    "\n",
    "    entry = {\n",
    "        'article_id': article_id,\n",
    "        'lccn': info['lccn'],\n",
    "        'issn': info['issn'],\n",
    "        'year': info['year'],\n",
    "        'category': info['category'],\n",
    "        'labor_sentiment': labor_pred['sentiment'] if labor_pred else None,\n",
    "        'labor_confidence': labor_pred['confidence'] if labor_pred else None,\n",
    "        'railroad_sentiment': rr_pred['sentiment'] if rr_pred else None,\n",
    "        'railroad_confidence': rr_pred['confidence'] if rr_pred else None,\n",
    "    }\n",
    "    merged.append(entry)\n",
    "\n",
    "save_json(OUTPUT_FILE, merged)\n",
    "\n",
    "print(f\"Merged results: {len(merged):,} articles\")\n",
    "print(f\"Saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "has_labor = sum(1 for r in merged if r['labor_sentiment'] is not None)\n",
    "has_rr = sum(1 for r in merged if r['railroad_sentiment'] is not None)\n",
    "has_both = sum(1 for r in merged if r['labor_sentiment'] is not None and r['railroad_sentiment'] is not None)\n",
    "print(f\"\\nCoverage:\")\n",
    "print(f\"  With labor sentiment:    {has_labor:,}\")\n",
    "print(f\"  With railroad sentiment: {has_rr:,}\")\n",
    "print(f\"  With both sentiments:    {has_both:,} (should match 'both' article count)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(merged)\n",
    "\n",
    "print(\"=== Labor Sentiment Distribution ===\")\n",
    "print(df['labor_sentiment'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n=== Railroad Sentiment Distribution ===\")\n",
    "print(df['railroad_sentiment'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n=== By Category ===\")\n",
    "for cat in ['labor', 'railroad', 'both']:\n",
    "    subset = df[df['category'] == cat]\n",
    "    print(f\"\\n{cat.upper()} ({len(subset):,} articles):\")\n",
    "    if subset['labor_sentiment'].notna().any():\n",
    "        print(f\"  Labor stance:     {subset['labor_sentiment'].value_counts().to_dict()}\")\n",
    "    if subset['railroad_sentiment'].notna().any():\n",
    "        print(f\"  Railroad outlook: {subset['railroad_sentiment'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n=== Average Confidence ===\")\n",
    "print(f\"  Labor:    {df['labor_confidence'].mean():.3f}\")\n",
    "print(f\"  Railroad: {df['railroad_confidence'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n=== Historical Validation ===\")\n",
    "for year, event in [(1877, 'Great Railroad Strike'), (1886, 'Haymarket Affair')]:\n",
    "    subset = df[df['year'] == year]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"\\n{year} ({event}): {len(subset)} articles\")\n",
    "        if subset['labor_sentiment'].notna().any():\n",
    "            print(f\"  Labor:    {subset['labor_sentiment'].value_counts().to_dict()}\")\n",
    "        if subset['railroad_sentiment'].notna().any():\n",
    "            print(f\"  Railroad: {subset['railroad_sentiment'].value_counts().to_dict()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}