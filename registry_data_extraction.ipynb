{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fbcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 375\n",
      "  Page width: 374.4\n",
      "  Midpoint: 187.2\n",
      "  Left col max x1: 165.7\n",
      "  Right col min x0: 196.6\n",
      "  Computed boundary: 181.2\n",
      "  Right extent threshold: 262.1\n",
      "  Short line threshold: 84.9\n",
      "====================================================================================================\n",
      "     Y     X0     X1  WIDTH  %PAST    COL | Text (first 60 chars)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "      28.2   57.1   70.3   13.2   0.0%   LEFT | 368\n",
      "      28.5  146.9  241.5   94.6  63.8%   LEFT | GEO . P. ROWELL & CO'S\n",
      "      43.2  104.2  146.2   42.0   0.0%   LEFT | MINNESOTA.\n",
      "      45.0  244.1  285.7   41.6 251.3%  RIGHT | MINNESOTA.\n",
      "      55.9  197.0  333.4  136.4 111.6%  RIGHT | publishers ; circulation-daily H1 *, week-\n",
      "      56.7   61.9  192.6  130.7   8.7%   LEFT | water power, which is used in manufact-\n",
      "      61.7  197.3  333.4  136.2 111.8%  RIGHT | ly F 2 **; does not insert advertisements in\n",
      "      63.6   62.6  127.7   65.0   0.0%   LEFT | ures of various kinds.\n",
      "      67.1  196.5  244.1   47.6 132.2%  RIGHT | the Minnehaha.\n",
      "      68.1   63.1   93.1   30.0   0.0%   LEFT | STAR ;\n",
      "      70.6   99.1  133.0   33.9   0.0%   LEFT | Thursdays;\n",
      "      70.8  178.7  192.2   13.5  81.7% RIGHT* | four\n",
      "      71.0  138.2  173.3   35.1   0.0%   LEFT | republican ;\n",
      "      72.7  202.3  333.8  131.5 116.0%  RIGHT | DISPATCH; every evening except Sun-\n",
      "      75.4  196.6  330.0  133.4 111.6%  RIGHT | day, and WEEKLY, Thursdays ; republi\n",
      "      77.4   56.8  191.9  135.1   7.9%   LEFT | pages20x26; subscription $2; established 1873;\n",
      "      80.6   56.4  191.9  135.5   7.9%   LEFT | Geo. E.Kirkpatrick, editor and publisher; cir-\n",
      "      84.2   57.7   96.2   38.5   0.0%   LEFT | culation K 2.\n",
      "      84.4  196.6  332.7  136.1 111.4%  RIGHT | can; daily four pages 20x24, weekly eight\n",
      "      88.7  196.8  333.2  136.4 111.5%  RIGHT | pages; subscription-daily$4,weekly 75 cents;\n",
      "      92.0   57.4  192.1  134.7   8.1%   LEFT | ST. ANTHONY HILL, (ST. PAUL\n",
      "      95.9  197.0  332.5  135.4 111.7%  RIGHT | established 1868; George Thompson, editor;\n",
      "     100.6  195.6  332.4  136.8 110.5%  RIGHT | Dispatch PrintingCo., publishers; circulation\n",
      "     102.2  127.3  192.0   64.7  16.7%   LEFT | . A suburb and part\n",
      "     103.2  196.0  289.3   93.3 116.0%  RIGHT | -daily F2 **, weekly H 2 **.\n",
      "     103.7   61.7  119.4   57.7   0.0%   LEFT | P. O.) Ramsey Co. ,\n",
      "     109.7  201.6  332.2  130.6 115.6%  RIGHT | GLOBE ; every morning, and WEEK-\n",
      "     110.6   62.8   95.5   32.7   0.0%   LEFT | of St. Paul .\n",
      "     116.4   61.7  191.9  130.2   8.3%   LEFT | GRAPHIC: (see St. Paul St. Anthony\n",
      "     116.8  216.5  249.8   33.4 205.7%  RIGHT | Thursdays ;\n",
      "     116.9  255.4  332.7   77.3 196.0%  RIGHT | independent-democratic;\n",
      "     119.7  196.3  331.5  135.2 111.2%  RIGHT | dailyand weekly eight pages 19x24, Sunday\n",
      "     122.3   56.2  100.9   44.7   0.0%   LEFT | Hill Graphic.)\n",
      "     124.7  196.2  331.7  135.4 111.1%  RIGHT | sixteen pages; subscription-daily $8, weekly\n",
      "     128.6   57.2  164.0  106.8   0.0%   LEFT | ST. CHARLES, Winona Co.,\n",
      "     132.7  174.0  191.5   17.6  59.0% RIGHT* | 1,155†\n",
      "     133.3  196.0  332.1  136.2 110.9%  RIGHT | $1, Sunday$2; established 1878; LewisBaker,\n",
      "     135.5  229.4  331.0  101.6 147.5%  RIGHT | publisher ; circ'n-daily D1 ***,\n",
      "     137.7  195.8  215.4   19.5 175.1%  RIGHT | editor\n",
      "     139.2   61.6  190.5  128.9   7.3%   LEFT | pop., on Chicago & Northwestern Rd., 28 m.\n",
      "     140.1  196.2  301.8  105.6 114.2%  RIGHT | weekly E2 ***, Sunday C1 ***.\n",
      "     146.7  201.1  332.9  131.8 115.1%  RIGHT | NEWS; every evening except Sunday;.\n",
      "     147.2   61.3  157.0   95.7   0.0%   LEFT | W. of Winona, the county seat.\n",
      "     147.7  162.9  191.5   28.6  36.2% RIGHT* | Center of\n",
      "     153.7  194.9  331.0  136.2 110.0%  RIGHT | democratic; four pages 19x24; subscription\n",
      "     155.5   62.4  123.3   60.9   0.0%   LEFT | agricultural district.\n",
      "     158.5  264.2  332.1   67.9 222.2%  RIGHT | St. Paul Daily News.\n",
      "     158.8  195.8  258.8   63.0 123.2%  RIGHT | $4; established 1887;\n",
      "     159.8   62.2   99.4   37.2   0.0%   LEFT | TIMES ;\n",
      "     162.4  105.6  191.1   85.5  11.7%   LEFT | Fridays ; democratic; four\n",
      "     163.4  196.2  332.3  136.0 111.1%  RIGHT | Printing Co., editors and publishers; circula-\n",
      "     167.5   56.4  190.9  134.5   7.3%   LEFT | pages 18x24; subscription $2; established 1873;\n",
      "     167.7  196.1  221.1   25.0 159.7%  RIGHT | tion G 2.\n",
      "     172.8   56.2  191.0  134.8   7.3%   LEFT | Mrs. H. W. Hill, editor and publisher; circu-\n",
      "     173.9  201.6  330.9  129.3 115.8%  RIGHT | PIONEER PRESS ; every morning ex-\n",
      "     176.2   57.0   88.0   31.0   0.0%   LEFT | lationK1.\n",
      "     179.2  253.0  331.1   78.0 192.1%  RIGHT | cept Sunday, SUNDAY\n",
      "     181.5   62.4   92.2   29.8   0.0%   LEFT | UNION ;\n",
      "     182.5  132.6  191.0   58.4  16.9%   LEFT | independent-repub-\n",
      "     183.1  100.5  127.2   26.7   0.0%   LEFT | Fridays ;\n",
      "     184.0  254.2  330.1   75.9 196.2%  RIGHT | PIONEER PRESS,\n",
      "     187.6   56.3  190.3  134.0   6.8%   LEFT | lican; four pages 20x26; subscription $1.50;\n",
      "     189.7  253.0  330.4   77.4 192.9%  RIGHT | Sundays, and WEEK-\n",
      "     192.5   56.3  190.0  133.7   6.6%   LEFT | established 1876; J. S. Whiton & Son, editors\n",
      "     194.9  253.3  267.7   14.5 599.0%  RIGHT | LY,\n",
      "     196.0   56.2  150.8   94.6   0.0%   LEFT | and publishers; circulationK1.\n",
      "     197.2  274.4  307.8   33.4 378.9%  RIGHT | Thursdays ;\n",
      "     197.3  314.3  330.9   16.6 900.8%  RIGHT | daily\n",
      "     202.4  252.8  330.7   77.8 192.1%  RIGHT | and weekly eight pages\n",
      "     206.0   56.4  165.7  109.3   0.0%   LEFT | ST. CLOUD, c. h ., Stearns Co. ,\n",
      "     207.4  253.1  330.6   77.5 192.8%  RIGHT | 18x24, Sunday sixteen to\n",
      "     209.5  176.3  190.8   14.5  66.4% RIGHT* | 6,000\n",
      "     212.8  253.2  330.2   76.9 193.6%  RIGHT | twenty-eight pages; sub-\n",
      "     216.0   60.6  190.6  130.0   7.2%   LEFT | pop., on St. Paul, Minneapolis & Manitoba\n",
      "     216.3  253.1  331.7   78.6 191.5%  RIGHT | scription-daily $8, Sun-\n",
      "     221.9  252.4  330.2   77.8 191.6%  RIGHT | day $2; established-daily\n",
      "     222.0   60.5  189.8  129.4   6.7%   LEFT | Rd. and Mississippi r. , 75 m. from St. Paul ,\n",
      "     228.5  252.7  330.5   77.8 191.9%  RIGHT | 1853, weekly 1849, Sunday\n",
      "     229.2   60.7  189.4  128.7   6.4%   LEFT | the State capital. A manufacturing town.\n",
      "     233.8  253.1  332.1   79.0 191.1%  RIGHT | 1854; J. A. Wheelock, edi-\n",
      "     237.5   59.7  159.4   99.6   0.0%   LEFT | Agricultural and lumber district .\n",
      "     238.6  253.3  330.7   77.4 193.1%  RIGHT | tor; Pioneer Press Co.,\n",
      "     242.8   61.2  189.7  128.5   6.6%   LEFT | TIMES ; every evening except Sunday,\n",
      "     244.1  291.5  330.5   38.9 383.4%  RIGHT | circulation-\n",
      "     244.2  253.3  286.5   33.2 317.4%  RIGHT | publishers ;\n",
      "     247.9   55.7  189.8  134.1   6.4%   LEFT | andWEEKLY,Wednesdays ; democratic;\n",
      "     248.1  286.5  330.7   44.2 338.2%  RIGHT | Sunday D2,\n",
      "     249.5  252.2  269.0   16.9 520.8%  RIGHT | daily\n",
      "     250.8  195.0  328.6  133.5 110.4%  RIGHT | weekly F2 . Dated and published simulta\n",
      "     256.2   54.4  190.0  135.6   6.5%   LEFT | daily four pages 15x22; weekly eight pages;\n",
      "     257.1  195.3  304.8  109.5 112.9%  RIGHT | neously at St. PaulandMinneapolis.\n",
      "     258.5   55.7  190.1  134.3   6.6%   LEFT | subscription-daily $5, weekly $1.50; estab-\n",
      "     262.1  200.1  290.4   90.3 121.0%  RIGHT | APPEAL; (see Chicago, Ill.)\n",
      "     263.6   55.4  189.7  134.3   6.4%   LEFT | lished-daily 1887, weekly 1861 ; C. F. MacDon-\n",
      "     268.6  199.6  298.4   98.8 118.7%  RIGHT | ATLANTIC REPORTER;\n",
      "     269.0   56.2  189.2  133.0   6.1%   LEFT | ald, editor and publisher; circ'n-daily J 1 ,\n",
      "     270.4  302.7  331.4   28.8 522.0%  RIGHT | (see Na-\n",
      "     275.3   54.8   91.1   36.3   0.0%   LEFT | weekly J 1 .\n",
      "     275.6  194.8  266.9   72.1 118.9%  RIGHT | tional Reporter System .)\n",
      "     279.8   59.8  189.3  129.5   6.3%   LEFT | DER NORDSTERN ; Thursdays ; Ger-\n",
      "     279.9  199.4  330.3  130.9 113.9%  RIGHT | DER WANDERER ; Thursdays; Ger-\n",
      "     285.9  193.9  329.1  135.2 109.4%  RIGHT | man; catholic ; sixteen pages 18x24; subscrip-\n",
      "     286.8   54.8  189.6  134.7   6.2%   LEFT | man; four pages 25x31 ; subscription $2; es-\n",
      "     291.2   54.9  190.1  135.2   6.6%   LEFT | tablished 1874; Gerhard May, editor; Nord.\n",
      "     292.1  194.7  330.5  135.8 109.9%  RIGHT | tion $2.50 ; established 1867; Hugo Klapproth,\n",
      "     297.5   55.1  190.1  134.9   6.6%   LEFT | stern Publishing Co., publishers; circula-\n",
      "     297.8  194.6  328.7  134.1 110.0%  RIGHT | editor ; Wanderer Printing Co., publishers;\n",
      "     300.7   55.6   91.9   36.3   0.0%   LEFT | tion I1 **.\n",
      "     300.9  193.1  328.9  135.8 108.8%  RIGHT | circulation G 2 ***; dated at St. Paul and\n",
      "     307.3   60.7  134.4   73.7   0.0%   LEFT | JOURNAL-PRESS;\n",
      "     307.4  192.7  231.7   38.9 129.7%  RIGHT | Minneapolis.\n",
      "     309.3  140.6  174.9   34.3   0.0%   LEFT | Thursdays ;\n",
      "     310.5  181.1  188.8    7.7  99.1% RIGHT* | re-\n",
      "     311.3  198.7  329.6  130.9 113.4%  RIGHT | EYE; Saturdays; thirty-two pages 10x13;\n",
      "     314.7   54.3  188.8  134.5   5.7%   LEFT | publican; four pages 22x28; subscription $2;\n",
      "     317.7   53.6  189.0  135.4   5.8%   LEFT | established 1876; W. B. Mitchell, editor and\n",
      "     319.0  194.2  329.5  135.2 109.7%  RIGHT | subscription $4; established 1889; Eye Pub-\n",
      "     323.6   54.5  144.4   89.8   0.0%   LEFT | publisher; circulation I1 **.\n",
      "     324.0  194.0  329.2  135.2 109.5%  RIGHT | lishing Co., editors and publishers; dated at\n",
      "     328.5  192.5  270.7   78.2 114.5%  RIGHT | St. Paul and Minneapolis.\n",
      "     329.3   60.2  189.2  129.0   6.2%   LEFT | TRIBUNE; Saturdays; republican; eight\n",
      "     332.8  199.0  295.8   96.8 118.4%  RIGHT | FEDERAL REPORTER;\n",
      "     334.5  301.0  329.6   28.6 518.8%  RIGHT | (see Na-\n",
      "     336.0   54.2  188.9  134.7   5.7%   LEFT | pages 18x24 ; subscription $1.50; established\n",
      "     339.7   54.0  189.4  135.5   6.1%   LEFT | 1880; J. W. Jones, editorand publisher; circu-\n",
      "     340.8  194.2  266.9   72.7 117.9%  RIGHT | tional Reporter System.)\n",
      "     343.2  199.0  328.8  129.8 113.7%  RIGHT | GREAT WEST; Fridays ; neutral; eight\n",
      "     344.1   54.7   85.4   30.7   0.0%   LEFT | lationK1.\n",
      "     350.6  194.0  327.9  133.8 109.6%  RIGHT | pages 15x22; subscription$1; established 1889 ;\n",
      "     354.3   54.7  150.1   95.4   0.0%   LEFT | ST. HILAIRE, Polk Co. ,\n",
      "     355.1  193.0  329.9  136.9 108.6%  RIGHT | EverettW. Fish, editor; GreatWest Co. ,pub-\n",
      "     356.5  160.4  188.8   28.4  26.8%   LEFT | 400 pop ,\n",
      "     358.6  198.5  214.6   16.1 207.1%  RIGHT | sheet\n",
      "     363.8   59.6  188.9  129.3   6.0%   LEFT | on St. Paul, Minneapolis & Manitoba Rd. , 30\n",
      "     363.9  199.2  328.2  129.0 114.0%  RIGHT | HERALD ; Saturdays ; independent;\n",
      "     369.8   59.5   66.4    7.0   0.0%   LEFT | m.\n",
      "     371.6  147.5  168.9   21.4   0.0%   LEFT | county\n",
      "     371.6   73.2   88.1   14.9   0.0%   LEFT | from\n",
      "     371.7   93.8  142.0   48.2   0.0%   LEFT | Crookston, the\n",
      "     372.2  174.1  188.1   14.0  49.2% RIGHT* | seat.\n",
      "     372.3  256.3  328.0   71.8 204.6%  RIGHT | subscription $i; estab-\n",
      "     372.5  193.5  251.0   57.4 121.5%  RIGHT | eight pages 15x22;\n",
      "     376.5  193.8  328.7  134.9 109.4%  RIGHT | lished 1882; James H. Burns, editor andpub-\n",
      "     378.4   58.9  154.7   95.8   0.0%   LEFT | Manufactures lumber and flour.\n",
      "     380.8  193.4  267.6   74.2 116.4%  RIGHT | lisher; circulation G2 *.\n",
      "     383.0   59.8  188.0  128.2   5.3%   LEFT | SPECTATOR ; Wednesdays ; repub-\n",
      "     385.9  199.4  310.1  110.7 116.5%  RIGHT | ILLUSTRERET UGEBLAD ;\n",
      "     387.8  316.0  329.0   13.0 1135.2%  RIGHT | (sce\n",
      "     391.2  192.6  234.5   41.8 127.4%  RIGHT | Minneapolis.)\n",
      "     391.7   52.8  187.7  134.9   4.8%   LEFT | lican; four pages 18x24; subscription $1.50;\n",
      "     396.0   53.5  188.3  134.8   5.3%   LEFT | established 1882; H. E. Ives, editor and pub-\n",
      "     397.5  198.2  328.6  130.4 113.1%  RIGHT | JOURNAL OF COMMERCE ; Satur-\n",
      "     400.6   53.8  121.8   68.0   0.0%   LEFT | lisher; circulation L2 .\n",
      "     404.9  191.7  327.5  135.8 107.8%  RIGHT | days; commercial ; twenty pages 10x14; sub-\n",
      "     407.1   53.5  179.4  125.9   0.0%   LEFT | ST. JAMES, c. h., Watonwan Co.,\n",
      "     411.2  193.0  328.2  135.2 108.7%  RIGHT | scription $2; established 1886; J. R. Foulke,\n",
      "     415.2  192.7  327.7  135.0 108.5%  RIGHT | editor; Journal of Commerce Co. , publish-\n",
      "     419.0   57.8  187.7  129.9   5.0%   LEFT | 814 pop., on Chicago, St. Paul, Minneapolis\n",
      "     419.6  192.5  252.6   60.0 118.9%  RIGHT | ers ; circulation 12 .\n",
      "     424.8   57.8   97.0   39.2   0.0%   LEFT | & OmahaRd.\n",
      "     425.1  198.0  327.0  129.0 113.0%  RIGHT | LE CANADIEN ; Thursdays ; French;\n",
      "     426.3  102.5  153.4   51.0   0.0%   LEFT | Farmingdistrict.\n",
      "     430.1   59.3   99.5   40.2   0.0%   LEFT | JOURNAL\n",
      "     432.5  102.7  187.2   84.5   7.2%   LEFT | : Saturdays ; independent;\n",
      "     433.1  192.6  328.1  135.5 108.4%  RIGHT | independent; four pages 20x26 ; subscrip-\n",
      "     437.2   52.8  188.3  135.5   5.2%   LEFT | eight pages 13x20; subscription $2; estab.\n",
      "     437.8  192.8  328.7  136.0 108.5%  RIGHT | tion$2 ; established 1877; T. Levasseur, edi-\n",
      "     440.5  192.6  327.2  134.5 108.5%  RIGHT | tor; N. Ledoux, publisher; circulation H 1;\n",
      "     442.0   53.3  188.5  135.2   5.4%   LEFT | lished 1878; W. A. Chapman, editor and pub-\n",
      "     446.8  191.0  295.0  104.0 109.4%  RIGHT | dated at St. Pauland Minneapolis.\n",
      "     446.8   53.3  121.0   67.7   0.0%   LEFT | lisher; circulation K1.\n",
      "     451.3  197.3  327.3  130.0 112.4%  RIGHT | METHODIST HERALD ; (see Minne-\n",
      "     455.5   53.3  157.2  103.9   0.0%   LEFT | ST . PAUL, c. h., Ramsey Co. ,\n",
      "     458.0  167.8  187.5   19.7  32.0%   LEFT | State\n",
      "     459.4  192.2  213.8   21.6 150.9%  RIGHT | apolis.)\n",
      "     461.5  197.5  326.9  129.4 112.6%  RIGHT | MIDWAY NEWS ; Saturdays; demo-\n",
      "     465.9   58.1  187.0  128.9   4.5%   LEFT | Capital,190,000 pop., on 9 Rds . and Missis-\n",
      "     469.1  191.7  328.9  137.2 107.7%  RIGHT | cratic; four pages 18x24; subscription $1; es--\n",
      "     472.3   58.1  187.3  129.2   4.8%   LEFT | sippi r., 9 m. below the Falls of St.Anthony\n",
      "     473.7  191.8  327.5  135.7 107.8%  RIGHT | tablished 18SS; Ed. A. Paradis, editor and\n",
      "     480.4   57.5  187.2  129.7   4.6%   LEFT | andat the head of navigation. Engaged in\n",
      "     481.2  191.5  222.5   31.0 133.4%  RIGHT | publisher.\n",
      "     483.0  197.0  322.8  125.8 112.6%  RIGHT | MINNEHAHA ; (see Die Volkszeitung.)\n",
      "     487.7   57.4  150.5   93.1   0.0%   LEFT | manufacturing and commerce.\n",
      "     488.1  196.7  254.9   58.2 126.8%  RIGHT | MINNESOTA\n",
      "     489.0  261.3  328.2   66.8 220.0%  RIGHT | STAATS-ANΖΕΙ..\n",
      "     492.5   58.1  187.5  129.4   4.9%   LEFT | DIE VOLKSZEITUNG; every even-\n",
      "     493.3  192.1  326.6  134.5 108.1%  RIGHT | GER; Fridays; German; four pages 15x22;\n",
      "     497.1   52.8  186.6  133.7   4.0%   LEFT | ing except Sunday, WEEKLY, Wednes-\n",
      "     499.7  191.8  326.2  134.4 107.9%  RIGHT | established 1889; Rukgang & Dreis, editors\n",
      "     502.7   51.4  187.0  135.6   4.3%   LEFT | days, and MINNEHAHA , Sundays ; Ger-\n",
      "     506.3  191.7  237.1   45.4 123.1%  RIGHT | and publishers.\n",
      "     508.9   51.8  186.5  134.6   3.9%   LEFT | man; independent; four pages-daily 22x29,\n",
      "     509.7  196.6  326.3  129.7 111.9%  RIGHT | NATIONAL HOTEL NEWS; Satur-\n",
      "     512.8   51.5  327.1  275.6  52.9%  SPLIT | L:[weekly 25x32, Sunday 18x2] R:[days; commercial; eight p]\n",
      "     520.8   51.8  186.5  134.7   3.9%   LEFT | daily $6,weekly $2, Sunday $1.50; established\n",
      "     521.6  191.5  330.2  138.7 107.4%  RIGHT | scription$1; established 1888; William George-\n",
      "     525.4   50.5  185.6  135.1   3.3%   LEFT | -dally1819, weekly1877; C. H. Lienau, editor;\n",
      "     526.2  191.1  329.8  138.6 107.2%  RIGHT | Heath, editor ; Hotel News Co., publishers;.\n",
      "     529.8   50.9  186.3  135.4   3.8%   LEFT | Die Volkszeitung Printing & Publishing Co.,\n",
      "     531.5  191.0  295.2  104.2 109.4%  RIGHT | dated at St. Paul and Minneapolis.\n",
      "\n",
      "Total lines: 201\n",
      "--- Page 375 ---\n",
      "368\n",
      "GEO . P. ROWELL & CO'S\n",
      "MINNESOTA.\n",
      "water power, which is used in manufact-\n",
      "ures of various kinds.\n",
      "STAR ;\n",
      "Thursdays;\n",
      "republican ;\n",
      "pages20x26; subscription $2; established 1873;\n",
      "Geo. E.Kirkpatrick, editor and publisher; cir-\n",
      "culation K 2.\n",
      "ST. ANTHONY HILL, (ST. PAUL\n",
      "P. O.) Ramsey Co. ,\n",
      ". A suburb and part\n",
      "of St. Paul .\n",
      "GRAPHIC: (see St. Paul St. Anthony\n",
      "Hill Graphic.)\n",
      "ST. CHARLES, Winona Co.,\n",
      "pop., on Chicago & Northwestern Rd., 28 m.\n",
      "W. of Winona, the county seat.\n",
      "agricultural district.\n",
      "TIMES ;\n",
      "Fridays ; democratic; four\n",
      "pages 18x24; subscription $2; established 1873;\n",
      "Mrs. H. W. Hill, editor and publisher; circu-\n",
      "lationK1.\n",
      "UNION ;\n",
      "Fridays ;\n",
      "independent-repub-\n",
      "lican; four pages 20x26; subscription $1.50;\n",
      "established 1876; J. S. Whiton & Son, editors\n",
      "and publishers; circulationK1.\n",
      "ST. CLOUD, c. h ., Stearns Co. ,\n",
      "pop., on St. Paul, Minneapolis & Manitoba\n",
      "Rd. and Mississippi r. , 75 m. from St. Paul ,\n",
      "the State capital. A manufacturing town.\n",
      "Agricultural and lumber district .\n",
      "TIMES ; every evening except Sunday,\n",
      "andWEEKLY,Wednesdays ; democratic;\n",
      "daily four pages 15x22; weekly eight pages;\n",
      "subscription-daily $5, weekly $1.50; estab-\n",
      "lished-daily 1887, weekly 1861 ; C. F. MacDon-\n",
      "ald, editor and publisher; circ'n-daily J 1 ,\n",
      "weekly J 1 .\n",
      "DER NORDSTERN ; Thursdays ; Ger-\n",
      "man; four pages 25x31 ; subscription $2; es-\n",
      "tablished 1874; Gerhard May, editor; Nord.\n",
      "stern Publishing Co., publishers; circula-\n",
      "tion I1 **.\n",
      "JOURNAL-PRESS;\n",
      "Thursdays ;\n",
      "publican; four pages 22x28; subscription $2;\n",
      "established 1876; W. B. Mitchell, editor and\n",
      "publisher; circulation I1 **.\n",
      "TRIBUNE; Saturdays; republican; eight\n",
      "pages 18x24 ; subscription $1.50; established\n",
      "1880; J. W. Jones, editorand publisher; circu-\n",
      "lationK1.\n",
      "ST. HILAIRE, Polk Co. ,\n",
      "400 pop ,\n",
      "on St. Paul, Minneapolis & Manitoba Rd. , 30\n",
      "m.\n",
      "county\n",
      "from\n",
      "Crookston, the\n",
      "Manufactures lumber and flour.\n",
      "SPECTATOR ; Wednesdays ; repub-\n",
      "lican; four pages 18x24; subscription $1.50;\n",
      "established 1882; H. E. Ives, editor and pub-\n",
      "lisher; circulation L2 .\n",
      "ST. JAMES, c. h., Watonwan Co.,\n",
      "814 pop., on Chicago, St. Paul, Minneapolis\n",
      "& OmahaRd.\n",
      "Farmingdistrict.\n",
      "JOURNAL\n",
      ": Saturdays ; independent;\n",
      "eight pages 13x20; subscription $2; estab.\n",
      "lished 1878; W. A. Chapman, editor and pub-\n",
      "lisher; circulation K1.\n",
      "ST . PAUL, c. h., Ramsey Co. ,\n",
      "State\n",
      "Capital,190,000 pop., on 9 Rds . and Missis-\n",
      "sippi r., 9 m. below the Falls of St.Anthony\n",
      "andat the head of navigation. Engaged in\n",
      "manufacturing and commerce.\n",
      "DIE VOLKSZEITUNG; every even-\n",
      "ing except Sunday, WEEKLY, Wednes-\n",
      "days, and MINNEHAHA , Sundays ; Ger-\n",
      "man; independent; four pages-daily 22x29,\n",
      "weekly 25x32, Sunday 18x24 ; subscription-\n",
      "daily $6,weekly $2, Sunday $1.50; established\n",
      "-dally1819, weekly1877; C. H. Lienau, editor;\n",
      "Die Volkszeitung Printing & Publishing Co.,\n",
      "MINNESOTA.\n",
      "publishers ; circulation-daily H1 *, week-\n",
      "ly F 2 **; does not insert advertisements in\n",
      "the Minnehaha.\n",
      "four\n",
      "DISPATCH; every evening except Sun-\n",
      "day, and WEEKLY, Thursdays ; republi\n",
      "can; daily four pages 20x24, weekly eight\n",
      "pages; subscription-daily$4,weekly 75 cents;\n",
      "established 1868; George Thompson, editor;\n",
      "Dispatch PrintingCo., publishers; circulation\n",
      "-daily F2 **, weekly H 2 **.\n",
      "GLOBE ; every morning, and WEEK-\n",
      "Thursdays ;\n",
      "independent-democratic;\n",
      "dailyand weekly eight pages 19x24, Sunday\n",
      "sixteen pages; subscription-daily $8, weekly\n",
      "1,155†\n",
      "$1, Sunday$2; established 1878; LewisBaker,\n",
      "publisher ; circ'n-daily D1 ***,\n",
      "editor\n",
      "weekly E2 ***, Sunday C1 ***.\n",
      "Center of\n",
      "NEWS; every evening except Sunday;.\n",
      "democratic; four pages 19x24; subscription\n",
      "$4; established 1887;\n",
      "St. Paul Daily News.\n",
      "Printing Co., editors and publishers; circula-\n",
      "tion G 2.\n",
      "PIONEER PRESS ; every morning ex-\n",
      "cept Sunday, SUNDAY\n",
      "PIONEER PRESS,\n",
      "Sundays, and WEEK-\n",
      "LY,\n",
      "Thursdays ;\n",
      "daily\n",
      "and weekly eight pages\n",
      "18x24, Sunday sixteen to\n",
      "6,000\n",
      "twenty-eight pages; sub-\n",
      "scription-daily $8, Sun-\n",
      "day $2; established-daily\n",
      "1853, weekly 1849, Sunday\n",
      "1854; J. A. Wheelock, edi-\n",
      "tor; Pioneer Press Co.,\n",
      "publishers ;\n",
      "circulation-\n",
      "daily\n",
      "Sunday D2,\n",
      "weekly F2 . Dated and published simulta\n",
      "neously at St. PaulandMinneapolis.\n",
      "APPEAL; (see Chicago, Ill.)\n",
      "ATLANTIC REPORTER;\n",
      "(see Na-\n",
      "tional Reporter System .)\n",
      "DER WANDERER ; Thursdays; Ger-\n",
      "man; catholic ; sixteen pages 18x24; subscrip-\n",
      "tion $2.50 ; established 1867; Hugo Klapproth,\n",
      "editor ; Wanderer Printing Co., publishers;\n",
      "circulation G 2 ***; dated at St. Paul and\n",
      "Minneapolis.\n",
      "re-\n",
      "EYE; Saturdays; thirty-two pages 10x13;\n",
      "subscription $4; established 1889; Eye Pub-\n",
      "lishing Co., editors and publishers; dated at\n",
      "St. Paul and Minneapolis.\n",
      "FEDERAL REPORTER;\n",
      "(see Na-\n",
      "tional Reporter System.)\n",
      "GREAT WEST; Fridays ; neutral; eight\n",
      "pages 15x22; subscription$1; established 1889 ;\n",
      "EverettW. Fish, editor; GreatWest Co. ,pub-\n",
      "sheet\n",
      "HERALD ; Saturdays ; independent;\n",
      "seat.\n",
      "eight pages 15x22;\n",
      "subscription $i; estab-\n",
      "lished 1882; James H. Burns, editor andpub-\n",
      "lisher; circulation G2 *.\n",
      "ILLUSTRERET UGEBLAD ;\n",
      "(sce\n",
      "Minneapolis.)\n",
      "JOURNAL OF COMMERCE ; Satur-\n",
      "days; commercial ; twenty pages 10x14; sub-\n",
      "scription $2; established 1886; J. R. Foulke,\n",
      "editor; Journal of Commerce Co. , publish-\n",
      "ers ; circulation 12 .\n",
      "LE CANADIEN ; Thursdays ; French;\n",
      "independent; four pages 20x26 ; subscrip-\n",
      "tion$2 ; established 1877; T. Levasseur, edi-\n",
      "tor; N. Ledoux, publisher; circulation H 1;\n",
      "dated at St. Pauland Minneapolis.\n",
      "METHODIST HERALD ; (see Minne-\n",
      "apolis.)\n",
      "MIDWAY NEWS ; Saturdays; demo-\n",
      "cratic; four pages 18x24; subscription $1; es--\n",
      "tablished 18SS; Ed. A. Paradis, editor and\n",
      "publisher.\n",
      "MINNEHAHA ; (see Die Volkszeitung.)\n",
      "MINNESOTA\n",
      "STAATS-ANΖΕΙ..\n",
      "GER; Fridays; German; four pages 15x22;\n",
      "established 1889; Rukgang & Dreis, editors\n",
      "and publishers.\n",
      "NATIONAL HOTEL NEWS; Satur-\n",
      "days; commercial; eight pages 11x16; sub-\n",
      "scription$1; established 1888; William George-\n",
      "Heath, editor ; Hotel News Co., publishers;.\n",
      "dated at St. Paul and Minneapolis.\n"
     ]
    }
   ],
   "source": [
    "# OCR parser Script 1877 - 1890\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "\n",
    "def split_merged_line(line, midpoint):\n",
    "    \"\"\"\n",
    "    Split a merged line into left and right column content by examining spans.\n",
    "    \"\"\"\n",
    "    \n",
    "    left_spans = []\n",
    "    right_spans = []\n",
    "    \n",
    "    for span in line['spans']:\n",
    "        span_bbox = span[\"bbox\"]\n",
    "        span_x0 = span_bbox[0]\n",
    "        span_x1 = span_bbox[2]\n",
    "        span_center = (span_x0 + span_x1) / 2\n",
    "        span_text = span[\"text\"]\n",
    "        \n",
    "        if not span_text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Classify span by its center position\n",
    "        if span_center < midpoint:\n",
    "            left_spans.append(span_text)\n",
    "        else:\n",
    "            right_spans.append(span_text)\n",
    "    \n",
    "    left_text = \"\".join(left_spans).strip()\n",
    "    right_text = \"\".join(right_spans).strip()\n",
    "    \n",
    "    return left_text, right_text\n",
    "\n",
    "\n",
    "def extract_columns_v11(\n",
    "    pages_to_scan,\n",
    "    input_dir=r\"data\\Newspaper Directories\",\n",
    "    output_dir=r\"data\\Newspaper Directory Text\",\n",
    "    file_to_start_at=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Two-column extraction with dynamic boundary detection per page.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    years = sorted(pages_to_scan.keys())\n",
    "    \n",
    "    print(f\"Will process years: {years}\")\n",
    "    \n",
    "    if file_to_start_at is not None:\n",
    "        years = [y for y in years if y >= file_to_start_at]\n",
    "        print(f\"Starting from year {file_to_start_at}\")\n",
    "    \n",
    "    for year in years:\n",
    "        start_page, end_page = pages_to_scan[year]\n",
    "        \n",
    "        pdf_path = os.path.join(input_dir, f\"Rowell {year}.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"WARNING: {pdf_path} not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f\"Rowell {year} - v13.txt\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"SKIPPING {year}: Output already exists\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: Rowell {year}.pdf (pages {start_page}-{end_page})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        pdf = fitz.open(pdf_path)\n",
    "        all_text = []\n",
    "        \n",
    "        for page_num in range(start_page - 1, min(end_page, len(pdf))):\n",
    "            page = pdf[page_num]\n",
    "            page_width = page.rect.width\n",
    "            \n",
    "            page_text = process_page_v11(page, page_width, page_num + 1)\n",
    "            all_text.append(page_text)\n",
    "            \n",
    "            pages_done = page_num - (start_page - 1) + 1\n",
    "            total_pages = end_page - start_page + 1\n",
    "            if pages_done % 50 == 0 or pages_done == total_pages:\n",
    "                print(f\"  {pages_done}/{total_pages} pages ({100*pages_done/total_pages:.1f}%)\")\n",
    "        \n",
    "        pdf.close()\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(all_text))\n",
    "        \n",
    "        print(f\"  SAVED: {output_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "def process_page_v11(page, page_width, page_num):\n",
    "    \"\"\"\n",
    "    Process page with dynamic column boundary detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    midpoint = page_width / 2\n",
    "    \n",
    "    # Collect all lines\n",
    "    all_lines = []\n",
    "    for block in text_dict[\"blocks\"]:\n",
    "        if block[\"type\"] != 0:\n",
    "            continue\n",
    "        for line in block[\"lines\"]:\n",
    "            line_bbox = line[\"bbox\"]\n",
    "            line_text = \"\".join(span[\"text\"] for span in line[\"spans\"]).strip()\n",
    "            if not line_text:\n",
    "                continue\n",
    "            all_lines.append({\n",
    "                'text': line_text,\n",
    "                'x0': line_bbox[0],\n",
    "                'x1': line_bbox[2],\n",
    "                'y': line_bbox[1],\n",
    "                'spans': line[\"spans\"],  # Keep spans for potential splitting\n",
    "            })\n",
    "    \n",
    "    if not all_lines:\n",
    "        return f\"--- Page {page_num} ---\\n\"\n",
    "    \n",
    "    # Find the column boundary by analyzing line positions\n",
    "    clear_left_x1 = []\n",
    "    for line in all_lines:\n",
    "        if line['x1'] < midpoint * 0.95 and line['x0'] < midpoint * 0.5:\n",
    "            clear_left_x1.append(line['x1'])\n",
    "    \n",
    "    clear_right_x0 = []\n",
    "    for line in all_lines:\n",
    "        if line['x0'] > midpoint * 1.05:\n",
    "            clear_right_x0.append(line['x0'])\n",
    "    \n",
    "    if clear_left_x1 and clear_right_x0:\n",
    "        left_edge = max(clear_left_x1)\n",
    "        right_edge = min(clear_right_x0)\n",
    "        if right_edge > left_edge:\n",
    "            boundary = (left_edge + right_edge) / 2\n",
    "        else:\n",
    "            boundary = right_edge - 5\n",
    "    else:\n",
    "        boundary = midpoint\n",
    "    \n",
    "    # Classify lines using the computed boundary\n",
    "    left_lines = []\n",
    "    right_lines = []\n",
    "\n",
    "\n",
    "    # Threshold for \"line extends into right column territory\"\n",
    "    right_extent_threshold = midpoint * 1.4\n",
    "    \n",
    "    # Threshold for \"short line\" - if a line is shorter than the gap between midpoint and right_extent_threshold\n",
    "    short_line_threshold = right_extent_threshold - midpoint + 10  # e.g., 252.8 - 180.6 = 72.2\n",
    "    \n",
    "    for line in all_lines:\n",
    "        line_width = line['x1'] - line['x0']\n",
    "        \n",
    "        # Check if this line spans both columns (merged line)\n",
    "        # Criteria: starts in left column territory AND extends well past midpoint\n",
    "        starts_left = line['x0'] < midpoint * 0.8\n",
    "        extends_right = line['x1'] > midpoint * 1.3\n",
    "        is_wide = line_width > page_width * 0.5\n",
    "        \n",
    "        if starts_left and extends_right and is_wide:\n",
    "            # This is likely a merged line - split it by examining spans\n",
    "            left_text, right_text = split_merged_line(line, midpoint)\n",
    "            \n",
    "            if left_text:\n",
    "                left_lines.append({\n",
    "                    'text': left_text,\n",
    "                    'y': line['y'],\n",
    "                })\n",
    "            if right_text:\n",
    "                right_lines.append({\n",
    "                    'text': right_text,\n",
    "                    'y': line['y'],\n",
    "                })\n",
    "        # If line STARTS past boundary, it's right column\n",
    "        elif line['x0'] >= boundary:\n",
    "            right_lines.append(line)\n",
    "        # If line STARTS left of boundary BUT EXTENDS well into right territory,\n",
    "        # it's likely right column content with a slightly left-shifted start due to slant\n",
    "        elif line['x1'] > right_extent_threshold and line['x0'] > midpoint * 0.9:\n",
    "            right_lines.append(line)\n",
    "        # Short lines that start before boundary but extend past it - likely right column\n",
    "        # Must have at least 40% of line width past the boundary\n",
    "        elif (line_width < short_line_threshold \n",
    "              and line['x0'] < boundary \n",
    "              and line['x1'] > boundary\n",
    "              and (line['x1'] - boundary) / line_width >= 0.35):\n",
    "            right_lines.append(line)\n",
    "        else:\n",
    "            left_lines.append(line)\n",
    "    \n",
    "    # Sort each column by y-position\n",
    "    left_lines.sort(key=lambda l: l['y'])\n",
    "    right_lines.sort(key=lambda l: l['y'])\n",
    "\n",
    "    def fix_same_row_order(lines):\n",
    "        if len(lines) < 2:\n",
    "            return lines\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines) - 1:\n",
    "            curr = lines[i]\n",
    "            next_line = lines[i + 1]\n",
    "            \n",
    "            # Skip if either line is missing x0/x1 (e.g., from split lines)\n",
    "            if 'x0' not in curr or 'x0' not in next_line or 'x1' not in next_line:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # If lines are on same visual row (within 2 units of Y)\n",
    "            # and next line starts to the LEFT of current line\n",
    "            # and they are horizontally adjacent (curr x0 is within 20 units of next_line x1)\n",
    "            if (abs(curr['y'] - next_line['y']) <= 2 \n",
    "                and next_line['x0'] < curr['x0']\n",
    "                and abs(curr['x0'] - next_line['x1']) <= 20):\n",
    "                # Swap them\n",
    "                lines[i], lines[i + 1] = lines[i + 1], lines[i]\n",
    "                # Check if we need to swap backwards too\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "                    continue\n",
    "            i += 1\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    left_lines = fix_same_row_order(left_lines)\n",
    "    right_lines = fix_same_row_order(right_lines)\n",
    "    \n",
    "    # Build output\n",
    "    result = [f\"--- Page {page_num} ---\"]\n",
    "    for line in left_lines:\n",
    "        result.append(line['text'])\n",
    "    for line in right_lines:\n",
    "        result.append(line['text'])\n",
    "    \n",
    "    return '\\n'.join(result)\n",
    "\n",
    "\n",
    "def debug_page(pdf_path, page_num, search_text=None):\n",
    "    \"\"\"Debug: show boundary detection and line classifications.\"\"\"\n",
    "    \n",
    "    pdf = fitz.open(pdf_path)\n",
    "    page = pdf[page_num - 1]\n",
    "    page_width = page.rect.width\n",
    "    midpoint = page_width / 2\n",
    "    \n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    \n",
    "    all_lines = []\n",
    "    for block in text_dict[\"blocks\"]:\n",
    "        if block[\"type\"] != 0:\n",
    "            continue\n",
    "        for line in block[\"lines\"]:\n",
    "            line_bbox = line[\"bbox\"]\n",
    "            line_text = \"\".join(span[\"text\"] for span in line[\"spans\"]).strip()\n",
    "            if not line_text:\n",
    "                continue\n",
    "            all_lines.append({\n",
    "                'text': line_text,\n",
    "                'x0': line_bbox[0],\n",
    "                'x1': line_bbox[2],\n",
    "                'y': line_bbox[1],\n",
    "                'spans': line[\"spans\"],\n",
    "            })\n",
    "    \n",
    "    # Find boundary\n",
    "    clear_left_x1 = []\n",
    "    for line in all_lines:\n",
    "        if line['x1'] < midpoint * 0.95 and line['x0'] < midpoint * 0.5:\n",
    "            clear_left_x1.append(line['x1'])\n",
    "    \n",
    "    clear_right_x0 = []\n",
    "    for line in all_lines:\n",
    "        if line['x0'] > midpoint * 1.05:\n",
    "            clear_right_x0.append(line['x0'])\n",
    "    \n",
    "    if clear_left_x1 and clear_right_x0:\n",
    "        left_edge = max(clear_left_x1)\n",
    "        right_edge = min(clear_right_x0)\n",
    "        if right_edge > left_edge:\n",
    "            boundary = (left_edge + right_edge) / 2\n",
    "        else:\n",
    "            boundary = right_edge - 5\n",
    "    else:\n",
    "        boundary = midpoint\n",
    "    \n",
    "    # Thresholds\n",
    "    right_extent_threshold = midpoint * 1.4\n",
    "    short_line_threshold = right_extent_threshold - midpoint + 10\n",
    "    \n",
    "    print(f\"Page {page_num}\")\n",
    "    print(f\"  Page width: {page_width:.1f}\")\n",
    "    print(f\"  Midpoint: {midpoint:.1f}\")\n",
    "    print(f\"  Left col max x1: {max(clear_left_x1):.1f}\" if clear_left_x1 else \"  No clear left lines\")\n",
    "    print(f\"  Right col min x0: {min(clear_right_x0):.1f}\" if clear_right_x0 else \"  No clear right lines\")\n",
    "    print(f\"  Computed boundary: {boundary:.1f}\")\n",
    "    print(f\"  Right extent threshold: {right_extent_threshold:.1f}\")\n",
    "    print(f\"  Short line threshold: {short_line_threshold:.1f}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Show lines sorted by y\n",
    "    all_lines.sort(key=lambda l: l['y'])\n",
    "    \n",
    "    print(f\"{'Y':>6} {'X0':>6} {'X1':>6} {'WIDTH':>6} {'%PAST':>6} {'COL':>6} | Text (first 60 chars)\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for line in all_lines:\n",
    "        line_width = line['x1'] - line['x0']\n",
    "        \n",
    "        # Calculate percent past boundary (if applicable)\n",
    "        if line['x1'] > boundary and line_width > 0:\n",
    "            pct_past = (line['x1'] - boundary) / line_width * 100\n",
    "        else:\n",
    "            pct_past = 0\n",
    "        \n",
    "        # Check for merged line\n",
    "        starts_left = line['x0'] < midpoint * 0.8\n",
    "        extends_right = line['x1'] > midpoint * 1.3\n",
    "        is_wide = line_width > page_width * 0.5\n",
    "        \n",
    "        if starts_left and extends_right and is_wide:\n",
    "            col = \"SPLIT\"\n",
    "            left_text, right_text = split_merged_line(line, midpoint)\n",
    "            text_preview = f\"L:[{left_text[:25]}] R:[{right_text[:25]}]\"\n",
    "        elif line['x0'] >= boundary:\n",
    "            col = \"RIGHT\"\n",
    "            text_preview = line['text'][:60]\n",
    "        elif line['x1'] > right_extent_threshold and line['x0'] > midpoint * 0.9:\n",
    "            col = \"RIGHT\"\n",
    "            text_preview = line['text'][:60]\n",
    "        elif (line_width < short_line_threshold \n",
    "              and line['x0'] < boundary \n",
    "              and line['x1'] > boundary\n",
    "              and (line['x1'] - boundary) / line_width >= 0.35):\n",
    "            col = \"RIGHT*\"\n",
    "            text_preview = line['text'][:60]\n",
    "        else:\n",
    "            col = \"LEFT\"\n",
    "            text_preview = line['text'][:60]\n",
    "        \n",
    "        # Highlight search text if provided\n",
    "        marker = \">>>\" if search_text and search_text.upper() in line['text'].upper() else \"   \"\n",
    "        print(f\"{marker} {line['y']:6.1f} {line['x0']:6.1f} {line['x1']:6.1f} {line_width:6.1f} {pct_past:5.1f}% {col:>6} | {text_preview}\")\n",
    "    \n",
    "    print(f\"\\nTotal lines: {len(all_lines)}\")\n",
    "    \n",
    "    pdf.close()\n",
    "\n",
    "\n",
    "def test_page(pdf_path, page_num):\n",
    "    \"\"\"Test on a single page.\"\"\"\n",
    "    \n",
    "    pdf = fitz.open(pdf_path)\n",
    "    page = pdf[page_num - 1]\n",
    "    \n",
    "    result = process_page_v11(page, page.rect.width, page_num)\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "    pdf.close()\n",
    "\n",
    "\n",
    "def find_text_in_pdf(pdf_path, search_text, start_page=1, end_page=None):\n",
    "    \"\"\"Find which page(s) contain specific text.\"\"\"\n",
    "    \n",
    "    pdf = fitz.open(pdf_path)\n",
    "    if end_page is None:\n",
    "        end_page = len(pdf)\n",
    "    \n",
    "    for page_num in range(start_page - 1, min(end_page, len(pdf))):\n",
    "        page = pdf[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        if search_text.upper() in text.upper():\n",
    "            print(f\"Found '{search_text}' on page {page_num + 1}\")\n",
    "    \n",
    "    pdf.close()\n",
    "\n",
    "\n",
    "# USAGE\n",
    "\n",
    "pages_to_scan = {\n",
    "    1877: (17, 341),\n",
    "    1878: (13, 341),\n",
    "    1879: (26, 374),\n",
    "    1880: (23, 404),\n",
    "    1882: (15, 476),\n",
    "    1883: (15, 452),\n",
    "    1884: (15, 524),\n",
    "    1885: (35, 590),\n",
    "    1890: (63, 731),\n",
    "}\n",
    "\n",
    "# Debug specific pages:\n",
    "debug_page(r\"data\\Newspaper Directories\\Rowell 1890.pdf\", 375, \"FOLSOM\")\n",
    "\n",
    "# Test specific pages:\n",
    "test_page(r\"data\\Newspaper Directories\\Rowell 1890.pdf\", 375)\n",
    "\n",
    "# Run full extraction:\n",
    "# extract_columns_v11(pages_to_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 PDF(s) total\n",
      "  Already completed: 0\n",
      "  Pending (with page ranges): 13\n",
      "  Skipped (no page range defined): 1\n",
      "    - Rowell 1869.pdf\n",
      "\n",
      "Will process:\n",
      "  - Rowell 1871.pdf (pages 21-251)\n",
      "  - Rowell 1872.pdf (pages 15-290)\n",
      "  - Rowell 1873.pdf (pages 35-238)\n",
      "  - Rowell 1876.pdf (pages 25-260)\n",
      "  - Rowell 1877.pdf (pages 17-341)\n",
      "  - Rowell 1878.pdf (pages 13-341)\n",
      "  - Rowell 1879.pdf (pages 26-374)\n",
      "  - Rowell 1880.pdf (pages 23-404)\n",
      "  - Rowell 1882.pdf (pages 15-476)\n",
      "  - Rowell 1883.pdf (pages 15-452)\n",
      "  - Rowell 1884.pdf (pages 15-524)\n",
      "  - Rowell 1885.pdf (pages 35-590)\n",
      "  - Rowell 1890.pdf (pages 63-731)\n",
      "\n",
      "============================================================\n",
      "[1/13] \n",
      "Processing: Rowell 1871.pdf\n",
      "  Total pages in PDF: 593\n",
      "  Extracting pages 21 to 251 (231 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1871.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[2/13] \n",
      "Processing: Rowell 1872.pdf\n",
      "  Total pages in PDF: 689\n",
      "  Extracting pages 15 to 290 (276 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1872.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[3/13] \n",
      "Processing: Rowell 1873.pdf\n",
      "  Total pages in PDF: 617\n",
      "  Extracting pages 35 to 238 (204 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1873.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[4/13] \n",
      "Processing: Rowell 1876.pdf\n",
      "  Total pages in PDF: 1046\n",
      "  Extracting pages 25 to 260 (236 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1876.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[5/13] \n",
      "Processing: Rowell 1877.pdf\n",
      "  Total pages in PDF: 1057\n",
      "  Extracting pages 17 to 341 (325 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1877.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[6/13] \n",
      "Processing: Rowell 1878.pdf\n",
      "  Total pages in PDF: 399\n",
      "  Extracting pages 13 to 341 (329 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1878.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[7/13] \n",
      "Processing: Rowell 1879.pdf\n",
      "  Total pages in PDF: 568\n",
      "  Extracting pages 26 to 374 (349 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1879.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[8/13] \n",
      "Processing: Rowell 1880.pdf\n",
      "  Total pages in PDF: 1057\n",
      "  Extracting pages 23 to 404 (382 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1880.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[9/13] \n",
      "Processing: Rowell 1882.pdf\n",
      "  Total pages in PDF: 1221\n",
      "  Extracting pages 15 to 476 (462 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1882.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[10/13] \n",
      "Processing: Rowell 1883.pdf\n",
      "  Total pages in PDF: 1137\n",
      "  Extracting pages 15 to 452 (438 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1883.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[11/13] \n",
      "Processing: Rowell 1884.pdf\n",
      "  Total pages in PDF: 1269\n",
      "  Extracting pages 15 to 524 (510 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1884.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[12/13] \n",
      "Processing: Rowell 1885.pdf\n",
      "  Total pages in PDF: 1395\n",
      "  Extracting pages 35 to 590 (556 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1885.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "[13/13] \n",
      "Processing: Rowell 1890.pdf\n",
      "  Total pages in PDF: 1465\n",
      "  Extracting pages 63 to 731 (669 pages)\n",
      "  Saved: data\\Newspaper Directory text\\Rowell 1890.txt\n",
      "  ✓ Progress saved\n",
      "\n",
      "============================================================\n",
      "Complete! Processed 13 file(s) in 0:00:59.497702\n",
      "Output saved to 'data\\Newspaper Directory text' folder\n"
     ]
    }
   ],
   "source": [
    "# page scanner 1869 - 1876\n",
    "\n",
    "# %% Imports and setup\n",
    "import fitz  # pymupdf\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Page ranges for each year: {year: (start_page, end_page)}\n",
    "# Note: These are 1-indexed page numbers (as they appear in the PDF)\n",
    "pages_to_scan = {\n",
    "    1869: (15, 187),\n",
    "    1871: (21, 251),\n",
    "    1872: (15, 290),\n",
    "    1873: (35, 238),\n",
    "    1876: (25, 260),\n",
    "}\n",
    "\n",
    "# %% Configuration - EDIT THESE\n",
    "INPUT_FOLDER = Path(\"data/Newspaper Directories\")\n",
    "OUTPUT_FOLDER = Path(\"data/Newspaper Directory text\")\n",
    "\n",
    "PROGRESS_FILE = Path(\"extract_progress.json\")\n",
    "\n",
    "# %% Functions\n",
    "def load_progress():\n",
    "    if PROGRESS_FILE.exists():\n",
    "        return set(json.loads(PROGRESS_FILE.read_text()))\n",
    "    return set()\n",
    "\n",
    "def save_progress(completed: set):\n",
    "    PROGRESS_FILE.write_text(json.dumps(list(completed)))\n",
    "\n",
    "def clear_progress():\n",
    "    \"\"\"Call this if you want to start fresh.\"\"\"\n",
    "    if PROGRESS_FILE.exists():\n",
    "        PROGRESS_FILE.unlink()\n",
    "        print(\"Progress cleared. Will reprocess all files.\")\n",
    "    else:\n",
    "        print(\"No progress file found.\")\n",
    "\n",
    "def extract_year_from_filename(filename: str) -> int | None:\n",
    "    \"\"\"Extract the year (18XX) from a filename like 'Rowell 1871.pdf'.\"\"\"\n",
    "    match = re.search(r'18\\d{2}', filename)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path, output_path: Path, start_page: int, end_page: int):\n",
    "    \"\"\"Extract embedded text from PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        output_path: Path for the output text file\n",
    "        start_page: First page to process (1-indexed)\n",
    "        end_page: Last page to process (1-indexed, inclusive)\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing: {pdf_path.name}\")\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    \n",
    "    # Convert to 0-indexed and clamp to valid range\n",
    "    start_idx = max(0, start_page - 1)\n",
    "    end_idx = min(total_pages, end_page)\n",
    "    \n",
    "    print(f\"  Total pages in PDF: {total_pages}\")\n",
    "    print(f\"  Extracting pages {start_idx + 1} to {end_idx} ({end_idx - start_idx} pages)\")\n",
    "    \n",
    "    all_text = []\n",
    "    \n",
    "    for page_num in range(start_idx, end_idx):\n",
    "        text = doc[page_num].get_text()\n",
    "        all_text.append(f\"--- Page {page_num + 1} ---\\n{text}\")\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # Save output\n",
    "    output_path.write_text(\"\\n\\n\".join(all_text), encoding=\"utf-8\")\n",
    "    print(f\"  Saved: {output_path}\")\n",
    "\n",
    "# %% Run extraction - THIS IS THE MAIN CELL\n",
    "INPUT_FOLDER.mkdir(exist_ok=True)\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "# Find PDFs and check progress\n",
    "pdfs = sorted(INPUT_FOLDER.glob(\"*.pdf\"))\n",
    "completed = load_progress()\n",
    "\n",
    "# Filter to only PDFs we have page ranges for\n",
    "valid_pdfs = []\n",
    "skipped_no_range = []\n",
    "\n",
    "for p in pdfs:\n",
    "    year = extract_year_from_filename(p.name)\n",
    "    if year and year in pages_to_scan:\n",
    "        if p.name not in completed:\n",
    "            valid_pdfs.append((p, year))\n",
    "    else:\n",
    "        skipped_no_range.append(p.name)\n",
    "\n",
    "skipped_completed = [p.name for p in pdfs if p.name in completed]\n",
    "\n",
    "print(f\"Found {len(pdfs)} PDF(s) total\")\n",
    "print(f\"  Already completed: {len(skipped_completed)}\")\n",
    "print(f\"  Pending (with page ranges): {len(valid_pdfs)}\")\n",
    "if skipped_no_range:\n",
    "    print(f\"  Skipped (no page range defined): {len(skipped_no_range)}\")\n",
    "    for name in skipped_no_range:\n",
    "        print(f\"    - {name}\")\n",
    "\n",
    "if not valid_pdfs:\n",
    "    print(\"\\nNo files to process. Run clear_progress() if you want to start over.\")\n",
    "else:\n",
    "    print(f\"\\nWill process:\")\n",
    "    for p, year in valid_pdfs:\n",
    "        start, end = pages_to_scan[year]\n",
    "        print(f\"  - {p.name} (pages {start}-{end})\")\n",
    "    \n",
    "    # Process all pending PDFs\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i, (pdf_path, year) in enumerate(valid_pdfs, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[{i}/{len(valid_pdfs)}]\", end=\" \")\n",
    "        \n",
    "        output_path = OUTPUT_FOLDER / f\"{pdf_path.stem}.txt\"\n",
    "        start_page, end_page = pages_to_scan[year]\n",
    "        \n",
    "        extract_text_from_pdf(pdf_path, output_path, start_page=start_page, end_page=end_page)\n",
    "        \n",
    "        # Mark as completed and save progress immediately\n",
    "        completed.add(pdf_path.name)\n",
    "        save_progress(completed)\n",
    "        print(f\"  ✓ Progress saved\")\n",
    "    \n",
    "    # Summary\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Complete! Processed {len(valid_pdfs)} file(s) in {elapsed}\")\n",
    "    print(f\"Output saved to '{OUTPUT_FOLDER}' folder\")\n",
    "\n",
    "# %% Utility: Clear progress and start over (run manually if needed)\n",
    "# clear_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: data/Newspaper Directory Text/Rowell 1869.txt\n",
      "Parsing entries...\n",
      "Found 3079 raw entries\n",
      "Processing: 100/3079 (3.2%) - 0.4s elapsed\n",
      "Processing: 200/3079 (6.5%) - 0.6s elapsed\n",
      "Processing: 300/3079 (9.7%) - 0.9s elapsed\n",
      "Processing: 400/3079 (13.0%) - 1.1s elapsed\n",
      "Processing: 500/3079 (16.2%) - 1.3s elapsed\n",
      "Processing: 600/3079 (19.5%) - 1.5s elapsed\n",
      "Processing: 700/3079 (22.7%) - 1.7s elapsed\n",
      "Processing: 800/3079 (26.0%) - 1.8s elapsed\n",
      "Processing: 900/3079 (29.2%) - 1.9s elapsed\n",
      "Processing: 1000/3079 (32.5%) - 2.1s elapsed\n",
      "Processing: 1100/3079 (35.7%) - 2.2s elapsed\n",
      "Processing: 1200/3079 (39.0%) - 2.4s elapsed\n",
      "Processing: 1300/3079 (42.2%) - 2.5s elapsed\n",
      "Processing: 1400/3079 (45.5%) - 2.7s elapsed\n",
      "Processing: 1500/3079 (48.7%) - 2.8s elapsed\n",
      "Processing: 1600/3079 (52.0%) - 3.0s elapsed\n",
      "Processing: 1700/3079 (55.2%) - 3.2s elapsed\n",
      "Processing: 1800/3079 (58.5%) - 3.4s elapsed\n",
      "Processing: 1900/3079 (61.7%) - 3.6s elapsed\n",
      "Processing: 2000/3079 (65.0%) - 3.8s elapsed\n",
      "Processing: 2100/3079 (68.2%) - 3.9s elapsed\n",
      "Processing: 2200/3079 (71.5%) - 4.1s elapsed\n",
      "Processing: 2300/3079 (74.7%) - 4.2s elapsed\n",
      "Processing: 2400/3079 (77.9%) - 4.4s elapsed\n",
      "Processing: 2500/3079 (81.2%) - 4.6s elapsed\n",
      "Processing: 2600/3079 (84.4%) - 4.7s elapsed\n",
      "Processing: 2700/3079 (87.7%) - 4.8s elapsed\n",
      "Processing: 2800/3079 (90.9%) - 5.0s elapsed\n",
      "Processing: 2900/3079 (94.2%) - 5.1s elapsed\n",
      "Processing: 3000/3079 (97.4%) - 5.3s elapsed\n",
      "Processing: 3079/3079 (100.0%) - 17.0s elapsed\n",
      "\n",
      "==================================================\n",
      "Completed in 17.2 seconds\n",
      "Processed 3072 valid entries\n",
      "Output written to: data/Newspaper Directory Excel/Rowell 1869.csv\n",
      "==================================================\n",
      "Entries with state: 3072\n",
      "Entries with frequency: 2502\n",
      "Entries with political affiliation: 1359\n",
      "Entries with subscription price: 1141\n",
      "Entries with established date: 1500\n",
      "Entries with editor: 2418\n",
      "Entries with publisher: 2335\n",
      "Entries with circulation: 1178\n"
     ]
    }
   ],
   "source": [
    "# 1869 data extraction\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean OCR artifacts and normalize text.\"\"\"\n",
    "    replacements = {\n",
    "        'Î': 'A', 'Î•': 'E', 'Îœ': 'M', 'Î': 'N', 'Ð¡': 'C', 'Ð¢': 'T',\n",
    "        'Ã‰': 'E', 'Ñ': 'c', 'Ðµ': 'e', 'Ñ€': 'p', 'Ð': 'N',\n",
    "        '`': \"'\", \"'\": \"'\", '\"': '\"', '\"': '\"',\n",
    "        '\\xad': '', '­': '',\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def normalize_for_matching(text):\n",
    "    \"\"\"Normalize text for pattern matching - removes extra spaces.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def normalize_editor_publisher_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text specifically for editor/publisher extraction.\n",
    "    Handles OCR artifacts like hyphenated line breaks and missing spaces.\n",
    "    \"\"\"\n",
    "    normalized = text\n",
    "    \n",
    "    # Remove hyphenated line breaks (e.g., \"edi- tors\" -> \"editors\", \"pub- lisher\" -> \"publisher\")\n",
    "    normalized = re.sub(r'-\\s+', '', normalized)\n",
    "    \n",
    "    # Fix common OCR run-together patterns\n",
    "    normalized = re.sub(r'(editors?)(and)(pub)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(editors?)(and)(prop)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(publishers?)(and)(prop)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(publishers?)(and)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(proprietors?)(and)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(publishers?)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(proprietors?)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalize multiple spaces\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def extract_circulation(text):\n",
    "    \"\"\"Extract circulation number from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'circulation[:\\s]+(?:about\\s+)?(\\d[\\d,\\.]+)',\n",
    "        r'claims?\\s+(?:about\\s+)?(\\d[\\d,\\.]+)\\s+circulation',\n",
    "        r'circ(?:ulation|\\'?l?n)[:\\s\\.]+(?:about\\s+)?(\\d[\\d,\\.]+)',\n",
    "        r'(\\d[\\d,\\.]+)\\s+circ(?:ulation|\\'?l?n)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '').replace('.', '')\n",
    "    return ''\n",
    "\n",
    "def extract_political_affiliation(text):\n",
    "    \"\"\"Extract political affiliation from text.\"\"\"\n",
    "    affiliations = ['democratic', 'republican', 'independent', 'neutral',\n",
    "                    'whig', 'conservative', 'liberal', 'radical']\n",
    "    text_lower = text.lower()\n",
    "    for affiliation in affiliations:\n",
    "        if re.search(rf';\\s*{affiliation}\\b', text_lower):\n",
    "            return affiliation.capitalize()\n",
    "    return ''\n",
    "\n",
    "def extract_subscription_details(text):\n",
    "    \"\"\"Extract detailed subscription info.\"\"\"\n",
    "    daily_match = re.search(r'subscription[-\\s]+daily\\s+\\$(\\d+(?:\\s+\\d{2})?)', text, re.IGNORECASE)\n",
    "    if daily_match:\n",
    "        return f\"${daily_match.group(1).replace(' ', '.')}\"\n",
    "    \n",
    "    weekly_match = re.search(r'(?:subscription[-\\s]+)?weekly\\s+\\$(\\d+(?:\\s+\\d{2})?)', text, re.IGNORECASE)\n",
    "    if weekly_match:\n",
    "        return f\"${weekly_match.group(1).replace(' ', '.')}\"\n",
    "    \n",
    "    std_match = re.search(r'subscription[:\\s]+\\$(\\d+(?:\\s+\\d{2})?)', text, re.IGNORECASE)\n",
    "    if std_match:\n",
    "        return f\"${std_match.group(1).replace(' ', '.')}\"\n",
    "    \n",
    "    cents_match = re.search(r'subscription\\s+(\\d+)\\s+cents', text, re.IGNORECASE)\n",
    "    if cents_match:\n",
    "        return f\"${int(cents_match.group(1))/100:.2f}\"\n",
    "    \n",
    "    return ''\n",
    "\n",
    "def extract_frequency(text):\n",
    "    \"\"\"Extract publication frequency from text.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if re.search(r'every\\s+(?:morning|evening|day)', text_lower):\n",
    "        return 'Daily & Weekly' if 'and weekly' in text_lower or 'weekly,' in text_lower else 'Daily'\n",
    "    if re.search(r'tri-?weekly', text_lower):\n",
    "        return 'Tri-weekly & Weekly' if 'and weekly' in text_lower else 'Tri-weekly'\n",
    "    if re.search(r'semi-?weekly', text_lower):\n",
    "        return 'Semi-weekly & Weekly' if 'and weekly' in text_lower else 'Semi-weekly'\n",
    "    if re.search(r'semi-?monthly', text_lower):\n",
    "        return 'Semi-monthly'\n",
    "    if 'quarterly' in text_lower:\n",
    "        return 'Quarterly'\n",
    "    if 'monthly' in text_lower:\n",
    "        return 'Monthly'\n",
    "    \n",
    "    days = ['sundays', 'mondays', 'tuesdays', 'wednesdays', 'thursdays', 'fridays', 'saturdays']\n",
    "    for day in days:\n",
    "        if day in text_lower:\n",
    "            return 'Weekly'\n",
    "    return ''\n",
    "\n",
    "def extract_established(text):\n",
    "    \"\"\"Extract establishment year from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'establish[e]?d\\s+(\\d{4})',\n",
    "        r'estab[-\\s]*lished\\s+(\\d{4})',\n",
    "        r'es[-\\s]*tablished\\s+(\\d{4})',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            if 1700 <= int(year) <= 1900:\n",
    "                return year\n",
    "    return ''\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"Clean and validate an extracted name.\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    name = name.strip().strip(',;:.')\n",
    "    \n",
    "    if len(name) < 3:\n",
    "        return None\n",
    "    \n",
    "    false_positives = ['Four', 'Eight', 'The', 'And', 'Weekly', 'Daily', 'Semi', \n",
    "                       'Tri', 'Monthly', 'Sunday', 'Saturday', 'Friday', 'Thursday',\n",
    "                       'Wednesday', 'Tuesday', 'Monday', 'About', 'Claims', 'Size',\n",
    "                       'Subscription', 'Established', 'Circulation', 'Pages',\n",
    "                       'Democratic', 'Republican', 'Independent', 'Neutral',\n",
    "                       'Temperance', 'Association']\n",
    "    if name in false_positives:\n",
    "        return None\n",
    "    \n",
    "    if name.replace(',', '').replace('.', '').isdigit():\n",
    "        return None\n",
    "    \n",
    "    if re.search(r'\\b(editor|publisher|proprietor|and)\\s*$', name, re.IGNORECASE):\n",
    "        return None\n",
    "    \n",
    "    if name[0].islower():\n",
    "        return None\n",
    "    \n",
    "    return name\n",
    "\n",
    "def add_name_if_unique(name, name_list):\n",
    "    \"\"\"Add a name to the list if it's not a duplicate.\"\"\"\n",
    "    cleaned = clean_name(name)\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    \n",
    "    cleaned_lower = cleaned.lower()\n",
    "    \n",
    "    for existing in name_list:\n",
    "        if cleaned_lower == existing.lower() or cleaned_lower in existing.lower():\n",
    "            return False\n",
    "    \n",
    "    to_remove = [e for e in name_list if e.lower() in cleaned_lower]\n",
    "    for item in to_remove:\n",
    "        name_list.remove(item)\n",
    "    \n",
    "    name_list.append(cleaned)\n",
    "    return True\n",
    "\n",
    "def extract_editor_publisher(text):\n",
    "    \"\"\"Extract editor and publisher names from text.\"\"\"\n",
    "    editors, publishers = [], []\n",
    "    \n",
    "    normalized = normalize_editor_publisher_text(text)\n",
    "    normalized = normalize_for_matching(normalized)\n",
    "    \n",
    "    normalized = re.sub(r'(\\w)(and)(\\w)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(editor[s]?)(and)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(pub)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(prop)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Split by semicolons to process each segment separately\n",
    "    segments = re.split(r';', normalized)\n",
    "    \n",
    "    for segment in segments:\n",
    "        segment = segment.strip()\n",
    "        if not segment:\n",
    "            continue\n",
    "        \n",
    "        name_pattern = r'([A-Z][A-Za-z\\.\\s&,]+?)'\n",
    "        \n",
    "        combined_patterns = [\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s+and\\s+publishers?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s+and\\s+proprietors?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?andpublishers?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?andproprietors?', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        editor_patterns = [\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s*$', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s*,', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editor-in-chief', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        publisher_patterns = [\n",
    "            re.compile(name_pattern + r',?\\s+publishers?\\s*$', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+proprietors?\\s*$', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+publishers?\\s+and\\s+proprietors?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+publishersandproprietors?', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        matched_combined = False\n",
    "        for pattern in combined_patterns:\n",
    "            match = pattern.search(segment)\n",
    "            if match:\n",
    "                add_name_if_unique(match.group(1), editors)\n",
    "                add_name_if_unique(match.group(1), publishers)\n",
    "                matched_combined = True\n",
    "                break\n",
    "        \n",
    "        if matched_combined:\n",
    "            continue\n",
    "        \n",
    "        for pattern in editor_patterns:\n",
    "            match = pattern.search(segment)\n",
    "            if match:\n",
    "                add_name_if_unique(match.group(1), editors)\n",
    "                break\n",
    "        \n",
    "        for pattern in publisher_patterns:\n",
    "            match = pattern.search(segment)\n",
    "            if match:\n",
    "                add_name_if_unique(match.group(1), publishers)\n",
    "                break\n",
    "    \n",
    "    return {'editor': '; '.join(editors), 'publisher': '; '.join(publishers)}\n",
    "\n",
    "def is_state_header(line):\n",
    "    \"\"\"Check if a line is a state header (e.g., 'ALABAMA.' or 'ALABAMA .')\"\"\"\n",
    "    # List of valid US states and territories for the period\n",
    "    valid_states = [\n",
    "        'ALABAMA', 'ARKANSAS', 'ARIZONA', 'CALIFORNIA', 'COLORADO', \n",
    "        'CONNECTICUT', 'DAKOTA', 'DELAWARE', 'DISTRICT OF COLUMBIA',\n",
    "        'FLORIDA', 'GEORGIA', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA',\n",
    "        'KANSAS', 'KENTUCKY', 'LOUISIANA', 'MAINE', 'MARYLAND',\n",
    "        'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI',\n",
    "        'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE',\n",
    "        'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA',\n",
    "        'OHIO', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND',\n",
    "        'SOUTH CAROLINA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT',\n",
    "        'VIRGINIA', 'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING',\n",
    "        # Territories and other regions\n",
    "        'INDIAN TERRITORY', 'DOMINION OF CANADA', 'BRITISH COLONIES',\n",
    "        'CANADA', 'NEWFOUNDLAND', 'NOVA SCOTIA', 'NEW BRUNSWICK',\n",
    "        'PRINCE EDWARD ISLAND', 'MANITOBA', 'ONTARIO', 'QUEBEC',\n",
    "        'BRITISH COLUMBIA'\n",
    "    ]\n",
    "    \n",
    "    # Clean the line: remove periods, extra spaces, and strip\n",
    "    # This handles both \"ALABAMA.\" and \"ALABAMA .\" patterns\n",
    "    cleaned = re.sub(r'\\s*\\.\\s*', '', line).strip().upper()\n",
    "    \n",
    "    # Check if it matches a state name\n",
    "    if cleaned in valid_states:\n",
    "        return cleaned\n",
    "    \n",
    "    return None\n",
    "\n",
    "def is_valid_town_name(town):\n",
    "    \"\"\"Check if the extracted town name is a valid town (not an index/header entry).\"\"\"\n",
    "    invalid_patterns = [\n",
    "        r'^A\\s+LIST', r'DOMINION', r'CANADA', r'BRITISH', r'COLONIES',\n",
    "        r'UNITED\\s+STATES', r'TERRITORIES', r'^INDEX', r'^PAGE\\s*\\d*',\n",
    "        r'NEWSPAPERS?', r'PERIODICALS?', r'ALPHABETICALLY', r'ARRANGED',\n",
    "        r'GIVING\\s+NAME', r'DAYS\\s+OF\\s+ISSUE', r'SUBSCRIPTION\\s+PRICE',\n",
    "        r'EDITOR.?S?\\s+AND\\s+PUBLISHER', r'CIRCULATION', r'ADVERTISEMENTS?',\n",
    "        r'PRINTING\\s+MATERIAL', r'IN\\s+WHICH', r'ARE\\s+PUBLISHED',\n",
    "        r'^NOTE', r'^\\d+$', r'^THE\\s+', r'ALIST\\s+OF',\n",
    "    ]\n",
    "    \n",
    "    town_upper = town.upper().strip()\n",
    "    for pattern in invalid_patterns:\n",
    "        if re.search(pattern, town_upper):\n",
    "            return False\n",
    "    \n",
    "    if len(town) > 50 or len(town.split()) > 4:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def is_valid_entry_text(entry_text):\n",
    "    \"\"\"Check if the entry text looks like a valid newspaper entry.\"\"\"\n",
    "    invalid_patterns = [\n",
    "        r'ALPHABETICALLY\\s+BY\\s+TOWNS', r'DAYS\\s+OF\\s+ISSUE',\n",
    "        r'POLITICS\\s+OR\\s+GENERAL\\s+CHARACTER', r'DATE\\s+OF\\s+ESTABLISHMENT',\n",
    "        r'EDITOR.?S?\\s+AND\\s+PUBLISHER.?S?\\s+NAMES', r'GIV-?\\s*ING\\s+NAME',\n",
    "        r'DOMINION\\s+OF\\s+CANADA', r'BRITISH\\s+COLONIES',\n",
    "        r'UNITED\\s+STATES\\s+AND\\s+TERRITORIES',\n",
    "        r'A\\s*LIST\\s+OF\\s+THE\\s+NEWSPAPERS',\n",
    "    ]\n",
    "    \n",
    "    text_upper = entry_text.upper()\n",
    "    for pattern in invalid_patterns:\n",
    "        if re.search(pattern, text_upper):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def parse_newspaper_entries(text):\n",
    "    \"\"\"Parse the text into individual newspaper entries with state tracking.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    lines = text.split('\\n')\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_town = None\n",
    "    current_state = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Check if this line is a state header\n",
    "        state_match = is_state_header(line)\n",
    "        if state_match:\n",
    "            # Update state if it's different from current (handles repeated page headers)\n",
    "            if state_match != current_state:\n",
    "                # Save any pending entry before changing state\n",
    "                if current_entry and current_town:\n",
    "                    entries.append((current_state, current_town, ' '.join(current_entry)))\n",
    "                current_state = state_match\n",
    "                current_entry = []\n",
    "                current_town = None\n",
    "            # Skip the state header line either way\n",
    "            continue\n",
    "        \n",
    "        new_entry_match = re.match(\n",
    "            r'^([A-Z][A-Z\\s,\\.]+?)(?:,\\s*|\\s+)([A-Z][a-zA-Z\\s&\\'\\.\\-]+?)\\s*[;:]',\n",
    "            line\n",
    "        )\n",
    "        \n",
    "        if new_entry_match:\n",
    "            if current_entry and current_town:\n",
    "                entries.append((current_state, current_town, ' '.join(current_entry)))\n",
    "            current_town = new_entry_match.group(1).strip().rstrip(',')\n",
    "            current_entry = [line]\n",
    "        elif current_entry:\n",
    "            current_entry.append(line)\n",
    "    \n",
    "    if current_entry and current_town:\n",
    "        entries.append((current_state, current_town, ' '.join(current_entry)))\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def parse_entry_details(state, town, entry_text):\n",
    "    \"\"\"Extract structured data from a single entry.\"\"\"\n",
    "    result = {\n",
    "        'state': state if state else '',\n",
    "        'town': town.strip().title(),\n",
    "        'newspaper_name': '',\n",
    "        'frequency': extract_frequency(entry_text),\n",
    "        'political_affiliation': extract_political_affiliation(entry_text),\n",
    "        'subscription_price': extract_subscription_details(entry_text),\n",
    "        'established': extract_established(entry_text),\n",
    "        'circulation': extract_circulation(entry_text),\n",
    "        'raw_text': entry_text[:300] + '...' if len(entry_text) > 300 else entry_text\n",
    "    }\n",
    "    \n",
    "    name_match = re.match(\n",
    "        rf'^{re.escape(town)}[,\\s]+([A-Za-z][A-Za-z\\s&\\'\\.\\-,]+?)\\s*[;:]',\n",
    "        entry_text, re.IGNORECASE\n",
    "    )\n",
    "    if name_match:\n",
    "        result['newspaper_name'] = name_match.group(1).strip().rstrip(',;:')\n",
    "    \n",
    "    people = extract_editor_publisher(entry_text)\n",
    "    result['editor'] = people['editor']\n",
    "    result['publisher'] = people['publisher']\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_file(input_path, output_path=None):\n",
    "    \"\"\"Process the input file and write results to CSV.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Reading file: {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(\"Parsing entries...\")\n",
    "    raw_entries = parse_newspaper_entries(text)\n",
    "    total_entries = len(raw_entries)\n",
    "    print(f\"Found {total_entries} raw entries\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (state, town, entry_text) in enumerate(raw_entries):\n",
    "        if (i + 1) % 100 == 0 or i == total_entries - 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            pct = (i + 1) / total_entries * 100\n",
    "            print(f\"Processing: {i + 1}/{total_entries} ({pct:.1f}%) - {elapsed:.1f}s elapsed\")\n",
    "        \n",
    "        if not is_valid_town_name(town):\n",
    "            continue\n",
    "        if not is_valid_entry_text(entry_text):\n",
    "            continue\n",
    "        \n",
    "        details = parse_entry_details(state, town, entry_text)\n",
    "        if details['newspaper_name'] and len(details['newspaper_name']) > 1:\n",
    "            results.append(details)\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = Path(input_path).stem + '_extracted.csv'\n",
    "    \n",
    "    fieldnames = ['state', 'town', 'newspaper_name', 'frequency', 'political_affiliation', \n",
    "                  'subscription_price', 'established', 'editor', 'publisher',\n",
    "                  'circulation', 'raw_text']\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Completed in {elapsed:.1f} seconds\")\n",
    "    print(f\"Processed {len(results)} valid entries\")\n",
    "    print(f\"Output written to: {output_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Entries with state: {sum(1 for r in results if r['state'])}\")\n",
    "    print(f\"Entries with frequency: {sum(1 for r in results if r['frequency'])}\")\n",
    "    print(f\"Entries with political affiliation: {sum(1 for r in results if r['political_affiliation'])}\")\n",
    "    print(f\"Entries with subscription price: {sum(1 for r in results if r['subscription_price'])}\")\n",
    "    print(f\"Entries with established date: {sum(1 for r in results if r['established'])}\")\n",
    "    print(f\"Entries with editor: {sum(1 for r in results if r['editor'])}\")\n",
    "    print(f\"Entries with publisher: {sum(1 for r in results if r['publisher'])}\")\n",
    "    print(f\"Entries with circulation: {sum(1 for r in results if r['circulation'])}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# USAGE\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"data/Newspaper Directory Text/\")[:1]:\n",
    "    input = \"data/Newspaper Directory Text/\" + file\n",
    "    output = \"data/Newspaper Directory Excel/\" + file[:-3] + 'csv'\n",
    "    results = process_file(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e11ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Newspaper Directory Text/Rowell 1871.txt: 5878 entries found\n",
      "  States detected at positions: [(1645, 'ALABAMA'), (6202, 'ALABAMA'), (6936, 'ALABAMA'), (10647, 'ALABAMA'), (14436, 'ALABAMA'), (16217, 'ARKANSAS'), (17901, 'ARKANSAS'), (21657, 'ARKANSAS'), (25257, 'CALIFORNIA'), (28971, 'CALIFORNIA')]...\n",
      "data/Newspaper Directory Text/Rowell 1872.txt: 6241 entries found\n",
      "  States detected at positions: [(3031, 'ALABAMA'), (5793, 'ALABAMA'), (8365, 'ALABAMA'), (9568, 'ALABAMA'), (9807, 'ALABAMA'), (15944, 'ALABAMA'), (17629, 'ARKANSAS'), (19444, 'ARKANSAS'), (23221, 'ARKANSAS'), (26938, 'ARKANSAS')]...\n",
      "data/Newspaper Directory Text/Rowell 1873.txt: 6550 entries found\n",
      "  States detected at positions: [(4234, 'ALABAMA'), (6070, 'ALABAMA'), (8785, 'ALABAMA'), (9804, 'ALABAMA'), (16230, 'ALABAMA'), (18164, 'ARKANSAS'), (21031, 'ARKANSAS'), (23281, 'ARKANSAS'), (23656, 'ARKANSAS'), (27325, 'ARKANSAS')]...\n",
      "data/Newspaper Directory Text/Rowell 1876.txt: 7825 entries found\n",
      "  States detected at positions: [(2844, 'ALABAMA'), (5246, 'ALABAMA'), (7929, 'ALABAMA'), (10045, 'ALABAMA'), (15288, 'ALABAMA'), (18118, 'ARKANSAS'), (19086, 'ARKANSAS'), (22502, 'ARKANSAS'), (26260, 'ARKANSAS'), (27636, 'ARKANSAS')]...\n"
     ]
    }
   ],
   "source": [
    "# 1871 - 1876 extraction\n",
    "import csv\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "# Known US states and territories from that era\n",
    "STATES = {\n",
    "    \"ALABAMA\", \"ARKANSAS\", \"ARIZONA\", \"CALIFORNIA\", \"COLORADO\", \"CONNECTICUT\",\n",
    "    \"DELAWARE\", \"DISTRICT OF COLUMBIA\", \"FLORIDA\", \"GEORGIA\", \"IDAHO\", \"ILLINOIS\",\n",
    "    \"INDIANA\", \"IOWA\", \"KANSAS\", \"KENTUCKY\", \"LOUISIANA\", \"MAINE\", \"MARYLAND\",\n",
    "    \"MASSACHUSETTS\", \"MICHIGAN\", \"MINNESOTA\", \"MISSISSIPPI\", \"MISSOURI\", \"MONTANA\",\n",
    "    \"NEBRASKA\", \"NEVADA\", \"NEW HAMPSHIRE\", \"NEW JERSEY\", \"NEW MEXICO\", \"NEW YORK\",\n",
    "    \"NORTH CAROLINA\", \"OHIO\", \"OREGON\", \"PENNSYLVANIA\", \"RHODE ISLAND\",\n",
    "    \"SOUTH CAROLINA\", \"TENNESSEE\", \"TEXAS\", \"UTAH\", \"VERMONT\", \"VIRGINIA\",\n",
    "    \"WASHINGTON\", \"WEST VIRGINIA\", \"WISCONSIN\", \"WYOMING\",\n",
    "    \"INDIAN TERRITORY\", \"DAKOTA\", \"DOMINION OF CANADA\", \"BRITISH COLONIES\"\n",
    "}\n",
    "\n",
    "\n",
    "def extract_frequency(text: str) -> str:\n",
    "    \"\"\"Extract publication frequency.\"\"\"\n",
    "    freq_map = [\n",
    "        (r'every\\s*morning', 'Daily'),\n",
    "        (r'every\\s*evening', 'Daily'),\n",
    "        (r'every\\s*afternoon', 'Daily'),\n",
    "        (r'every\\s*day', 'Daily'),\n",
    "        (r'semi-weekly', 'Semi-weekly'), (r'tri-weekly', 'Tri-weekly'),\n",
    "        (r'bi-weekly', 'Bi-weekly'), (r'bi-monthly', 'Bi-monthly'),\n",
    "        (r'semi-month', 'Semi-monthly'),\n",
    "        (r'sundays?', 'Sundays'), (r'mondays?', 'Mondays'), (r'tuesdays?', 'Tuesdays'),\n",
    "        (r'wednesdays?', 'Wednesdays'), (r'thursdays?', 'Thursdays'),\n",
    "        (r'fridays?', 'Fridays'), (r'saturdays?', 'Saturdays'),\n",
    "        (r'\\bdaily\\b', 'Daily'), (r'\\bweekly\\b', 'Weekly'),\n",
    "        (r'\\bmonthly\\b', 'Monthly'), (r'\\bquarterly\\b', 'Quarterly'),\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    for pattern, freq in freq_map:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return freq\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_political(text: str) -> str:\n",
    "    \"\"\"Extract political affiliation.\"\"\"\n",
    "    affil_map = [\n",
    "        (r'\\bdemocrat', 'Democratic'), (r'\\brepublican', 'Republican'),\n",
    "        (r'\\bindependent', 'Independent'), (r'\\bneutral', 'Neutral'),\n",
    "        (r'\\bliberal', 'Liberal'), (r'\\bconservative', 'Conservative'),\n",
    "        (r'\\bgreenback', 'Greenback'), (r'\\bprohibition', 'Prohibition'),\n",
    "        (r'\\bbaptist', 'Baptist'), (r'\\bcongregational', 'Congregational'),\n",
    "        (r'\\bmethodist', 'Methodist'), (r'\\buniversalist', 'Universalist'),\n",
    "        (r'\\breligious', 'Religious'), (r'\\bagricultural', 'Agricultural'),\n",
    "        (r'\\bliterary', 'Literary'), (r'\\bgerman', 'German'),\n",
    "        (r'\\bcomic', 'Comic'),\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    for pattern, affil in affil_map:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return affil\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_price(text: str) -> str:\n",
    "    \"\"\"Extract subscription price.\"\"\"\n",
    "    patterns = [\n",
    "        r'subscription\\s*\\$\\s*(\\d+)\\s+(\\d+)',\n",
    "        r'subscription\\s*\\$\\s*(\\d+\\.\\d+)',\n",
    "        r'subscription\\s*\\$\\s*(\\d+)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text.lower())\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            if len(groups) == 2:\n",
    "                return f\"${groups[0]}.{groups[1]}\"\n",
    "            return f\"${groups[0]}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_established(text: str) -> str:\n",
    "    \"\"\"Extract year established.\"\"\"\n",
    "    patterns = [\n",
    "        r'estab-?\\s*lished\\s*(\\d{4})',\n",
    "        r'established\\s*(\\d{4})',\n",
    "        r're-established\\s*(\\d{4})',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text.lower())\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_editor_publisher(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Extract editor and publisher names.\"\"\"\n",
    "    editor = \"\"\n",
    "    publisher = \"\"\n",
    "    \n",
    "    # Pattern for \"X, editor(s) and publisher(s)\" (same person/people)\n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+?),?\\s*ed[-\\s]*i[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?\\s*and\\s+pub[-\\s]*l[-\\s]*i[-\\s]*s[-\\s]*h[-\\s]*e[-\\s]*r[-\\s]*s?', text)\n",
    "    if match:\n",
    "        name = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "        return name, name\n",
    "    \n",
    "    # Pattern for \"X, editor(s) and proprietor(s)\" (same person/people)\n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+?),?\\s*ed[-\\s]*i[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?\\s*and\\s+pro[-\\s]*p[-\\s]*r[-\\s]*i[-\\s]*e[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?', text)\n",
    "    if match:\n",
    "        name = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "        return name, name\n",
    "    \n",
    "    # Pattern for editor\n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+?),?\\s*ed[-\\s]*i[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?[;,:\\s]', text)\n",
    "    if match:\n",
    "        editor = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "    \n",
    "    # Pattern for publisher\n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+(?:\\s+Co\\.)?),?\\s*pub[-\\s]*l[-\\s]*i[-\\s]*s[-\\s]*h[-\\s]*e[-\\s]*r[-\\s]*s?[;,:\\s\\.]', text)\n",
    "    if match:\n",
    "        publisher = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "    \n",
    "    # Pattern for proprietor (extracted as publisher)\n",
    "    if not publisher:\n",
    "        match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+(?:\\s+Co\\.)?),?\\s*pro[-\\s]*p[-\\s]*r[-\\s]*i[-\\s]*e[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?[;,:\\s\\.]', text)\n",
    "        if match:\n",
    "            publisher = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "    \n",
    "    return editor, publisher\n",
    "\n",
    "\n",
    "def extract_circulation(text: str) -> str:\n",
    "    \"\"\"Extract circulation number.\"\"\"\n",
    "    # Normalize line-break hyphens first\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "    \n",
    "    # Pattern explanation:\n",
    "    # - circ?(?:ulation|'n|e'n) : matches circulation, crculation, circ'n, cire'n\n",
    "    # - [\\s\\-]* : optional whitespace or hyphens after the word\n",
    "    # - (?:about|approximately|nearly|over|around)? : optional qualifier words\n",
    "    # - [\\s\\-]* : optional whitespace or hyphens\n",
    "    # - (?:\\w+\\s+)? : optional word like \"daily\" before the number\n",
    "    # - (\\d[\\d,]*) : the circulation number\n",
    "    pattern = r\"circ?(?:ulation|'n|e'n)[\\s\\-]*(?:about|approximately|nearly|over|around)?[\\s\\-]*(?:\\w+\\s+)?(\\d[\\d,]*)\"\n",
    "    \n",
    "    # Also check for \"claims XXX\" pattern\n",
    "    claims_pattern = r\"claims[\\s\\-]+(\\d[\\d,]*)\"\n",
    "    \n",
    "    match = re.search(pattern, text.lower())\n",
    "    if not match:\n",
    "        match = re.search(claims_pattern, text.lower())\n",
    "    \n",
    "    if match:\n",
    "        circ = match.group(1).replace(',', '')\n",
    "        if 'estimated' in text.lower() or \"est'd\" in text.lower():\n",
    "            return f\"{circ} (estimated)\"\n",
    "        return circ\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Process a newspaper directory file and extract entries to CSV.\"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove page markers\n",
    "    content = re.sub(r'---\\s*Page\\s+\\d+\\s*---', ' ', content)\n",
    "    \n",
    "    # Fix common OCR Greek letter substitutions (uppercase)\n",
    "    content = content.replace('Α', 'A')  # Greek Alpha -> A\n",
    "    content = content.replace('Β', 'B')  # Greek Beta -> B\n",
    "    content = content.replace('Ε', 'E')  # Greek Epsilon -> E\n",
    "    content = content.replace('Η', 'H')  # Greek Eta -> H\n",
    "    content = content.replace('Ι', 'I')  # Greek Iota -> I\n",
    "    content = content.replace('Κ', 'K')  # Greek Kappa -> K\n",
    "    content = content.replace('Μ', 'M')  # Greek Mu -> M\n",
    "    content = content.replace('Ν', 'N')  # Greek Nu -> N\n",
    "    content = content.replace('Ο', 'O')  # Greek Omicron -> O\n",
    "    content = content.replace('Ρ', 'P')  # Greek Rho -> P\n",
    "    content = content.replace('Τ', 'T')  # Greek Tau -> T\n",
    "    content = content.replace('Χ', 'X')  # Greek Chi -> X\n",
    "    content = content.replace('Ζ', 'Z')  # Greek Zeta -> Z\n",
    "    content = content.replace('Θ', 'O')  # Greek Theta -> O (visually similar)\n",
    "    content = content.replace('Φ', 'O')  # Greek Phi -> O (visually similar)\n",
    "    \n",
    "    # Fix common OCR Cyrillic letter substitutions\n",
    "    content = content.replace('С', 'C')  # Cyrillic Es -> C\n",
    "    content = content.replace('О', 'O')  # Cyrillic O -> O\n",
    "    content = content.replace('Р', 'P')  # Cyrillic Er -> P\n",
    "    content = content.replace('Ф', 'O')  # Cyrillic Ef -> O\n",
    "    content = content.replace('А', 'A')  # Cyrillic A -> A\n",
    "    content = content.replace('Е', 'E')  # Cyrillic Ie -> E\n",
    "    content = content.replace('Н', 'H')  # Cyrillic En -> H\n",
    "    content = content.replace('В', 'B')  # Cyrillic Ve -> B\n",
    "    content = content.replace('К', 'K')  # Cyrillic Ka -> K\n",
    "    content = content.replace('М', 'M')  # Cyrillic Em -> M\n",
    "    content = content.replace('Т', 'T')  # Cyrillic Te -> T\n",
    "    \n",
    "    # Fix lowercase Greek/Cyrillic\n",
    "    content = content.replace('ο', 'o')  # Greek lowercase omicron -> o\n",
    "    content = content.replace('а', 'a')  # Cyrillic lowercase a -> a\n",
    "    content = content.replace('е', 'e')  # Cyrillic lowercase ie -> e\n",
    "    content = content.replace('о', 'o')  # Cyrillic lowercase o -> o\n",
    "    content = content.replace('р', 'p')  # Cyrillic lowercase er -> p\n",
    "    content = content.replace('с', 'c')  # Cyrillic lowercase es -> c\n",
    "    \n",
    "    # Fix OCR diacritical errors\n",
    "    content = content.replace('Ü', 'U')\n",
    "    content = content.replace('Ö', 'O')\n",
    "    content = content.replace('Ä', 'A')\n",
    "    content = content.replace('É', 'E')\n",
    "    content = content.replace('È', 'E')\n",
    "    content = content.replace('Ñ', 'N')\n",
    "    content = content.replace('Ç', 'C')\n",
    "    \n",
    "    # Remove OCR artifacts: sequences of O-like characters before town names\n",
    "    # Matches patterns like \"OOO \", \"OOD \", \"COO \", \"CODO \", etc.\n",
    "    content = re.sub(r'\\b[OoCcDd0ΘΦ]{2,}\\s+([A-Z]{2,})', r'\\1', content)\n",
    "    \n",
    "    # Fix missing space between ALL CAPS town and Capitalized newspaper name\n",
    "    # e.g., \"VAN BURENArgus\" -> \"VAN BUREN Argus\"\n",
    "    content = re.sub(r'([A-Z]{4})([A-Z][a-z])', r'\\1 \\2', content)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(content.split())\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Build state position index\n",
    "    state_positions = []\n",
    "    \n",
    "    for state in STATES:\n",
    "        # Pattern allows optional space before period: \"ARKANSAS .\" or \"ARKANSAS.\"\n",
    "        pattern = re.compile(r'\\b' + re.escape(state) + r'\\s*\\.', re.IGNORECASE)\n",
    "        for m in pattern.finditer(text):\n",
    "            state_positions.append((m.start(), state))\n",
    "    \n",
    "    state_positions.sort()\n",
    "    \n",
    "    # Remove duplicate state entries at same/nearby positions\n",
    "    filtered_positions = []\n",
    "    for pos, state in state_positions:\n",
    "        if not filtered_positions or pos - filtered_positions[-1][0] > 10:\n",
    "            filtered_positions.append((pos, state))\n",
    "    state_positions = filtered_positions\n",
    "    \n",
    "    # Main pattern for newspaper entries\n",
    "    pattern = re.compile(\n",
    "        r'\\b'\n",
    "        r'([A-Z][A-Z\\'\\-]+(?:\\s+[A-Z][A-Z\\'\\-]+)*)'  # Group 1: Town (ALL CAPS words)\n",
    "        r'\\s*[,.\\s]\\s*'                              # Separator\n",
    "        r'([A-Z][a-z][^;:†]*?)'                      # Group 2: Newspaper name\n",
    "        r'\\s*[;:†]'                                  # Delimiter\n",
    "    )\n",
    "    \n",
    "    matches = list(pattern.finditer(text))\n",
    "    \n",
    "    # First pass: identify valid entries\n",
    "    valid_matches = []\n",
    "    for match in matches:\n",
    "        pos = match.start()\n",
    "        \n",
    "        # Determine current state based on position\n",
    "        match_state = None\n",
    "        for sp, st in reversed(state_positions):\n",
    "            if sp < pos:\n",
    "                match_state = st\n",
    "                break\n",
    "        \n",
    "        if not match_state:\n",
    "            continue\n",
    "        \n",
    "        town = match.group(1).strip().rstrip(' ,.')\n",
    "        newspaper = match.group(2).strip().rstrip(' ,.')\n",
    "        \n",
    "        # Skip index/header content\n",
    "        if any(kw in newspaper.lower() for kw in ['list of', 'index', 'page']):\n",
    "            continue\n",
    "            \n",
    "        # Skip if newspaper contains what looks like a page header\n",
    "        if re.search(r'\\b\\d+\\s+[A-Z]{4,}\\.', newspaper):\n",
    "            continue\n",
    "        \n",
    "        if len(town) >= 2 and len(newspaper) >= 2:\n",
    "            valid_matches.append((match, match_state, town, newspaper))\n",
    "    \n",
    "    # Second pass: build results\n",
    "    for i, (match, match_state, town, newspaper) in enumerate(valid_matches):\n",
    "        if i + 1 < len(valid_matches):\n",
    "            raw_text = text[match.start():valid_matches[i + 1][0].start()].strip()\n",
    "        else:\n",
    "            raw_text = text[match.start():].strip()\n",
    "        \n",
    "        # Extract additional fields from raw_text\n",
    "        frequency = extract_frequency(raw_text)\n",
    "        political = extract_political(raw_text)\n",
    "        editor, publisher = extract_editor_publisher(raw_text)\n",
    "        circulation = extract_circulation(raw_text)\n",
    "        \n",
    "        results.append({\n",
    "            'state': match_state,\n",
    "            'town': town.title(),\n",
    "            'newspaper': newspaper,\n",
    "            'frequency': frequency,\n",
    "            'political': political,\n",
    "            'editor': editor,\n",
    "            'publisher': publisher,\n",
    "            'circulation': circulation,\n",
    "            'raw_text': raw_text\n",
    "        })\n",
    "    \n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in results:\n",
    "        key = (r['state'], r['town'], r['newspaper'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(r)\n",
    "    \n",
    "    # Remove known false positive entries from document header\n",
    "    false_positives = {\n",
    "        (\"NEW YORK\", \"York\", \"January 1, 1869. ee ~~ CONTENTS\"),\n",
    "        (\"NEW YORK\", \"Xiv\", \"Newspaper Directory Advertiser. XV. A circular to Advertisers, containing the names of more than one thousand newspapers, among which will be found the best advertising mediums in America\"),\n",
    "    }\n",
    "    unique = [r for r in unique if (r['state'], r['town'], r['newspaper']) not in false_positives]\n",
    "    \n",
    "    print(f\"{input_file}: {len(unique)} entries found\")\n",
    "    print(f\"  States detected at positions: {state_positions[:10]}...\")  # Debug\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['state', 'town', 'newspaper', 'frequency', 'political', 'editor', 'publisher', 'circulation', 'raw_text'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique)\n",
    "    \n",
    "    return unique\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"data/Newspaper Directory Text/\")[1:5]:\n",
    "    input_file = \"data/Newspaper Directory Text/\" + file\n",
    "    output_file = \"data/Newspaper Directory Excel/\" + file[:-3] + 'csv'\n",
    "    results = process_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96cfbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/Newspaper Directory Text\\Rowell 1877 - v13.txt (Year: 1877)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1877.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1877_comparison.txt\n",
      "Extracted 7274 newspaper entries to data/Newspaper Directory Excel\\Rowell 1877.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 81\n",
      "  ARIZONA: 3\n",
      "  ARKANSAS: 60\n",
      "  CALIFORNIA: 218\n",
      "  COLORADO: 41\n",
      "  CONNECTICUT: 100\n",
      "  DAKOTA: 15\n",
      "  DELAWARE: 39\n",
      "  DISTRICT OF COLUMBIA: 25\n",
      "  FLORIDA: 26\n",
      "  GEORGIA: 123\n",
      "  IDAHO: 9\n",
      "  ILLINOIS: 649\n",
      "  INDIANA: 360\n",
      "  IOWA: 330\n",
      "  KANSAS: 159\n",
      "  KENTUCKY: 119\n",
      "  LOUISIANA: 82\n",
      "  MAINE: 79\n",
      "  MARYLAND: 112\n",
      "  MASSACHUSETTS: 289\n",
      "  MICHIGAN: 285\n",
      "  MINNESOTA: 141\n",
      "  MISSISSIPPI: 86\n",
      "  MISSOURI: 335\n",
      "  MONTANA: 10\n",
      "  NEBRASKA: 98\n",
      "  NEVADA: 19\n",
      "  NEW BRUNSWICK: 18\n",
      "  NEW HAMPSHIRE: 62\n",
      "  NEW JERSEY: 162\n",
      "  NEW MEXICO: 7\n",
      "  NEW YORK: 950\n",
      "  NORTH CAROLINA: 81\n",
      "  OHIO: 538\n",
      "  OREGON: 42\n",
      "  PENNSYLVANIA: 633\n",
      "  RHODE ISLAND: 27\n",
      "  SOUTH CAROLINA: 63\n",
      "  TENNESSEE: 134\n",
      "  TEXAS: 147\n",
      "  UTAH: 9\n",
      "  VERMONT: 59\n",
      "  VIRGINIA: 122\n",
      "  WASHINGTON: 20\n",
      "  WEST VIRGINIA: 72\n",
      "  WISCONSIN: 232\n",
      "  WYOMING: 3\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 7274\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1877_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1878 - v13.txt (Year: 1878)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1878.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1878_comparison.txt\n",
      "Extracted 7548 newspaper entries to data/Newspaper Directory Excel\\Rowell 1878.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 80\n",
      "  ARIZONA: 6\n",
      "  ARKANSAS: 67\n",
      "  CALIFORNIA: 231\n",
      "  COLORADO: 46\n",
      "  CONNECTICUT: 105\n",
      "  DAKOTA: 21\n",
      "  DELAWARE: 51\n",
      "  DISTRICT OF COLUMBIA: 24\n",
      "  FLORIDA: 35\n",
      "  GEORGIA: 115\n",
      "  IDAHO: 19\n",
      "  ILLINOIS: 641\n",
      "  INDIANA: 331\n",
      "  IOWA: 397\n",
      "  KANSAS: 151\n",
      "  KENTUCKY: 125\n",
      "  LOUISIANA: 72\n",
      "  MAINE: 80\n",
      "  MARYLAND: 104\n",
      "  MASSACHUSETTS: 319\n",
      "  MICHIGAN: 286\n",
      "  MINNESOTA: 136\n",
      "  MISSISSIPPI: 106\n",
      "  MISSOURI: 323\n",
      "  NEBRASKA: 116\n",
      "  NEVADA: 22\n",
      "  NEW BRUNSWICK: 23\n",
      "  NEW HAMPSHIRE: 65\n",
      "  NEW JERSEY: 173\n",
      "  NEW MEXICO: 10\n",
      "  NEW YORK: 959\n",
      "  NORTH CAROLINA: 91\n",
      "  OHIO: 544\n",
      "  OREGON: 35\n",
      "  PENNSYLVANIA: 665\n",
      "  RHODE ISLAND: 30\n",
      "  SOUTH CAROLINA: 60\n",
      "  TENNESSEE: 128\n",
      "  TEXAS: 184\n",
      "  UTAH: 9\n",
      "  VERMONT: 60\n",
      "  VIRGINIA: 126\n",
      "  WASHINGTON: 51\n",
      "  WEST VIRGINIA: 70\n",
      "  WISCONSIN: 234\n",
      "  WYOMING: 22\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 7548\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1878_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1879 - v13.txt (Year: 1879)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1879.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1879_comparison.txt\n",
      "Extracted 7785 newspaper entries to data/Newspaper Directory Excel\\Rowell 1879.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 90\n",
      "  ARIZONA: 19\n",
      "  ARKANSAS: 75\n",
      "  BRITISH COLUMBIA: 5\n",
      "  CALIFORNIA: 212\n",
      "  COLORADO: 42\n",
      "  CONNECTICUT: 102\n",
      "  DAKOTA: 18\n",
      "  DELAWARE: 19\n",
      "  DISTRICT OF COLUMBIA: 25\n",
      "  FLORIDA: 29\n",
      "  GEORGIA: 145\n",
      "  IDAHO: 21\n",
      "  ILLINOIS: 640\n",
      "  INDIANA: 369\n",
      "  IOWA: 386\n",
      "  KANSAS: 190\n",
      "  KENTUCKY: 130\n",
      "  LOUISIANA: 65\n",
      "  MAINE: 88\n",
      "  MARYLAND: 106\n",
      "  MASSACHUSETTS: 335\n",
      "  MICHIGAN: 316\n",
      "  MINNESOTA: 132\n",
      "  MISSISSIPPI: 93\n",
      "  MISSOURI: 314\n",
      "  NEBRASKA: 119\n",
      "  NEVADA: 23\n",
      "  NEW BRUNSWICK: 12\n",
      "  NEW HAMPSHIRE: 73\n",
      "  NEW JERSEY: 153\n",
      "  NEW MEXICO: 10\n",
      "  NEW YORK: 1009\n",
      "  NORTH CAROLINA: 102\n",
      "  OHIO: 573\n",
      "  ONTARIO: 8\n",
      "  OREGON: 40\n",
      "  PENNSYLVANIA: 688\n",
      "  RHODE ISLAND: 36\n",
      "  SOUTH CAROLINA: 67\n",
      "  TENNESSEE: 136\n",
      "  TEXAS: 177\n",
      "  UTAH: 7\n",
      "  VERMONT: 63\n",
      "  VIRGINIA: 136\n",
      "  WASHINGTON: 74\n",
      "  WEST VIRGINIA: 76\n",
      "  WISCONSIN: 230\n",
      "  WYOMING: 7\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 7785\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1879_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1880 - v13.txt (Year: 1880)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1880.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1880_comparison.txt\n",
      "Extracted 8561 newspaper entries to data/Newspaper Directory Excel\\Rowell 1880.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 99\n",
      "  ARIZONA: 8\n",
      "  ARKANSAS: 101\n",
      "  BRITISH COLUMBIA: 9\n",
      "  CALIFORNIA: 246\n",
      "  COLORADO: 57\n",
      "  CONNECTICUT: 109\n",
      "  DAKOTA: 48\n",
      "  DELAWARE: 47\n",
      "  DISTRICT OF COLUMBIA: 34\n",
      "  FLORIDA: 33\n",
      "  GEORGIA: 162\n",
      "  IDAHO: 26\n",
      "  ILLINOIS: 740\n",
      "  INDIANA: 375\n",
      "  IOWA: 456\n",
      "  KANSAS: 222\n",
      "  KENTUCKY: 162\n",
      "  LOUISIANA: 93\n",
      "  MAINE: 73\n",
      "  MARYLAND: 122\n",
      "  MASSACHUSETTS: 355\n",
      "  MICHIGAN: 357\n",
      "  MINNESOTA: 161\n",
      "  MISSISSIPPI: 106\n",
      "  MISSOURI: 359\n",
      "  NEBRASKA: 151\n",
      "  NEVADA: 33\n",
      "  NEW BRUNSWICK: 4\n",
      "  NEW HAMPSHIRE: 78\n",
      "  NEW JERSEY: 168\n",
      "  NEW MEXICO: 10\n",
      "  NEW YORK: 1064\n",
      "  NORTH CAROLINA: 103\n",
      "  OHIO: 580\n",
      "  OREGON: 46\n",
      "  PENNSYLVANIA: 728\n",
      "  RHODE ISLAND: 43\n",
      "  SOUTH CAROLINA: 55\n",
      "  TENNESSEE: 126\n",
      "  TEXAS: 194\n",
      "  UTAH: 13\n",
      "  VERMONT: 67\n",
      "  VIRGINIA: 143\n",
      "  WASHINGTON: 67\n",
      "  WEST VIRGINIA: 65\n",
      "  WISCONSIN: 252\n",
      "  WYOMING: 11\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 8561\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1880_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1882 - v13.txt (Year: 1882)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1882.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1882_comparison.txt\n",
      "Extracted 10435 newspaper entries to data/Newspaper Directory Excel\\Rowell 1882.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 242\n",
      "  ARIZONA: 13\n",
      "  ARKANSAS: 215\n",
      "  BRITISH COLUMBIA: 16\n",
      "  CALIFORNIA: 531\n",
      "  COLORADO: 101\n",
      "  CONNECTICUT: 111\n",
      "  DAKOTA: 75\n",
      "  DELAWARE: 21\n",
      "  DISTRICT OF COLUMBIA: 37\n",
      "  FLORIDA: 40\n",
      "  GEORGIA: 163\n",
      "  IDAHO: 31\n",
      "  ILLINOIS: 812\n",
      "  INDIANA: 407\n",
      "  IOWA: 498\n",
      "  KANSAS: 256\n",
      "  KENTUCKY: 164\n",
      "  LOUISIANA: 92\n",
      "  MAINE: 98\n",
      "  MARYLAND: 129\n",
      "  MASSACHUSETTS: 408\n",
      "  MICHIGAN: 391\n",
      "  MINNESOTA: 178\n",
      "  MISSISSIPPI: 113\n",
      "  MISSOURI: 460\n",
      "  NEBRASKA: 158\n",
      "  NEVADA: 59\n",
      "  NEW BRUNSWICK: 4\n",
      "  NEW HAMPSHIRE: 109\n",
      "  NEW JERSEY: 217\n",
      "  NEW MEXICO: 22\n",
      "  NEW YORK: 1161\n",
      "  NORTH CAROLINA: 114\n",
      "  OHIO: 632\n",
      "  ONTARIO: 8\n",
      "  OREGON: 25\n",
      "  PENNSYLVANIA: 1010\n",
      "  RHODE ISLAND: 42\n",
      "  SOUTH CAROLINA: 68\n",
      "  TENNESSEE: 151\n",
      "  TEXAS: 292\n",
      "  UTAH: 16\n",
      "  VERMONT: 62\n",
      "  VIRGINIA: 195\n",
      "  WASHINGTON: 60\n",
      "  WEST VIRGINIA: 91\n",
      "  WISCONSIN: 305\n",
      "  WYOMING: 32\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 10435\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1882_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1883 - v13.txt (Year: 1883)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1883.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1883_comparison.txt\n",
      "Extracted 10171 newspaper entries to data/Newspaper Directory Excel\\Rowell 1883.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 108\n",
      "  ARIZONA: 7\n",
      "  ARKANSAS: 108\n",
      "  BRITISH COLUMBIA: 25\n",
      "  CALIFORNIA: 272\n",
      "  COLORADO: 107\n",
      "  CONNECTICUT: 119\n",
      "  DAKOTA: 119\n",
      "  DELAWARE: 33\n",
      "  DISTRICT OF COLUMBIA: 42\n",
      "  FLORIDA: 55\n",
      "  GEORGIA: 169\n",
      "  IDAHO: 39\n",
      "  ILLINOIS: 832\n",
      "  INDIANA: 420\n",
      "  IOWA: 511\n",
      "  KANSAS: 302\n",
      "  KENTUCKY: 162\n",
      "  LOUISIANA: 91\n",
      "  MAINE: 105\n",
      "  MARYLAND: 134\n",
      "  MASSACHUSETTS: 410\n",
      "  MICHIGAN: 435\n",
      "  MINNESOTA: 213\n",
      "  MISSISSIPPI: 102\n",
      "  MISSOURI: 472\n",
      "  NEBRASKA: 189\n",
      "  NEVADA: 23\n",
      "  NEW HAMPSHIRE: 90\n",
      "  NEW JERSEY: 191\n",
      "  NEW MEXICO: 27\n",
      "  NEW YORK: 1254\n",
      "  NORTH CAROLINA: 125\n",
      "  OHIO: 676\n",
      "  ONTARIO: 18\n",
      "  OREGON: 69\n",
      "  PENNSYLVANIA: 857\n",
      "  RHODE ISLAND: 40\n",
      "  SOUTH CAROLINA: 73\n",
      "  TENNESSEE: 158\n",
      "  TEXAS: 259\n",
      "  UTAH: 14\n",
      "  VERMONT: 63\n",
      "  VIRGINIA: 167\n",
      "  WASHINGTON: 34\n",
      "  WEST VIRGINIA: 95\n",
      "  WISCONSIN: 348\n",
      "  WYOMING: 9\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 10171\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1883_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1884 - v13.txt (Year: 1884)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1884.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1884_comparison.txt\n",
      "Extracted 11484 newspaper entries to data/Newspaper Directory Excel\\Rowell 1884.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 107\n",
      "  ARIZONA: 23\n",
      "  ARKANSAS: 122\n",
      "  BRITISH COLUMBIA: 18\n",
      "  CALIFORNIA: 314\n",
      "  COLORADO: 128\n",
      "  CONNECTICUT: 129\n",
      "  DAKOTA: 203\n",
      "  DELAWARE: 69\n",
      "  FLORIDA: 71\n",
      "  GEORGIA: 176\n",
      "  IDAHO: 22\n",
      "  ILLINOIS: 934\n",
      "  INDIANA: 464\n",
      "  IOWA: 596\n",
      "  KANSAS: 360\n",
      "  KENTUCKY: 173\n",
      "  LOUISIANA: 95\n",
      "  MAINE: 107\n",
      "  MARYLAND: 151\n",
      "  MASSACHUSETTS: 474\n",
      "  MICHIGAN: 499\n",
      "  MINNESOTA: 236\n",
      "  MISSISSIPPI: 106\n",
      "  MISSOURI: 550\n",
      "  MONTANA: 23\n",
      "  NEBRASKA: 241\n",
      "  NEVADA: 26\n",
      "  NEW HAMPSHIRE: 112\n",
      "  NEW JERSEY: 210\n",
      "  NEW MEXICO: 26\n",
      "  NEW YORK: 1351\n",
      "  NORTH CAROLINA: 137\n",
      "  NORTH DAKOTA: 5\n",
      "  OHIO: 782\n",
      "  ONTARIO: 6\n",
      "  OREGON: 79\n",
      "  PENNSYLVANIA: 937\n",
      "  RHODE ISLAND: 51\n",
      "  SOUTH CAROLINA: 78\n",
      "  TENNESSEE: 181\n",
      "  TEXAS: 320\n",
      "  UTAH: 22\n",
      "  VERMONT: 68\n",
      "  VIRGINIA: 206\n",
      "  WASHINGTON: 42\n",
      "  WEST VIRGINIA: 87\n",
      "  WISCONSIN: 362\n",
      "  WYOMING: 5\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 11484\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1884_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1885 - v13.txt (Year: 1885)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1885.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1885_comparison.txt\n",
      "Extracted 12282 newspaper entries to data/Newspaper Directory Excel\\Rowell 1885.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 124\n",
      "  ARIZONA: 22\n",
      "  ARKANSAS: 121\n",
      "  BRITISH COLUMBIA: 22\n",
      "  CALIFORNIA: 349\n",
      "  COLORADO: 105\n",
      "  CONNECTICUT: 140\n",
      "  DAKOTA: 255\n",
      "  DELAWARE: 59\n",
      "  FLORIDA: 95\n",
      "  GEORGIA: 198\n",
      "  IDAHO: 37\n",
      "  ILLINOIS: 1018\n",
      "  INDIANA: 498\n",
      "  IOWA: 635\n",
      "  KANSAS: 431\n",
      "  KENTUCKY: 177\n",
      "  LOUISIANA: 76\n",
      "  MAINE: 121\n",
      "  MARYLAND: 165\n",
      "  MASSACHUSETTS: 511\n",
      "  MICHIGAN: 542\n",
      "  MINNESOTA: 271\n",
      "  MISSISSIPPI: 119\n",
      "  MISSOURI: 568\n",
      "  MONTANA: 15\n",
      "  NEBRASKA: 301\n",
      "  NEVADA: 37\n",
      "  NEW HAMPSHIRE: 84\n",
      "  NEW JERSEY: 220\n",
      "  NEW MEXICO: 27\n",
      "  NEW YORK: 1418\n",
      "  NORTH CAROLINA: 146\n",
      "  OHIO: 798\n",
      "  OKLAHOMA: 7\n",
      "  ONTARIO: 16\n",
      "  OREGON: 87\n",
      "  PENNSYLVANIA: 987\n",
      "  RHODE ISLAND: 48\n",
      "  SOUTH CAROLINA: 95\n",
      "  TENNESSEE: 178\n",
      "  TEXAS: 343\n",
      "  UTAH: 22\n",
      "  VERMONT: 77\n",
      "  VIRGINIA: 177\n",
      "  WASHINGTON: 54\n",
      "  WEST VIRGINIA: 113\n",
      "  WISCONSIN: 362\n",
      "  WYOMING: 11\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 12282\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1885_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processing: data/Newspaper Directory Text\\Rowell 1890 - v13.txt (Year: 1890)\n",
      "  -> Output: data/Newspaper Directory Excel\\Rowell 1890.csv\n",
      "  -> Comparison: data/Newspaper Directory Excel\\Rowell 1890_comparison.txt\n",
      "Extracted 15629 newspaper entries to data/Newspaper Directory Excel\\Rowell 1890.csv\n",
      "\n",
      "=== Newspaper Entries by State ===\n",
      "  ALABAMA: 154\n",
      "  ARIZONA: 34\n",
      "  ARKANSAS: 175\n",
      "  BRITISH COLUMBIA: 11\n",
      "  CALIFORNIA: 480\n",
      "  COLORADO: 248\n",
      "  CONNECTICUT: 165\n",
      "  DAKOTA: 54\n",
      "  DELAWARE: 53\n",
      "  DISTRICT OF COLUMBIA: 73\n",
      "  FLORIDA: 103\n",
      "  GEORGIA: 239\n",
      "  IDAHO: 57\n",
      "  ILLINOIS: 1237\n",
      "  INDIANA: 584\n",
      "  IOWA: 707\n",
      "  KANSAS: 742\n",
      "  KENTUCKY: 211\n",
      "  MAINE: 148\n",
      "  MARYLAND: 162\n",
      "  MASSACHUSETTS: 671\n",
      "  MICHIGAN: 619\n",
      "  MINNESOTA: 430\n",
      "  MISSISSIPPI: 149\n",
      "  MISSOURI: 713\n",
      "  MONTANA: 46\n",
      "  NEBRASKA: 528\n",
      "  NEVADA: 21\n",
      "  NEW HAMPSHIRE: 114\n",
      "  NEW JERSEY: 305\n",
      "  NEW MEXICO: 37\n",
      "  NEW YORK: 1663\n",
      "  NORTH CAROLINA: 184\n",
      "  NORTH DAKOTA: 85\n",
      "  OHIO: 949\n",
      "  OREGON: 118\n",
      "  PENNSYLVANIA: 1214\n",
      "  RHODE ISLAND: 72\n",
      "  SOUTH CAROLINA: 94\n",
      "  SOUTH DAKOTA: 181\n",
      "  TENNESSEE: 216\n",
      "  TEXAS: 462\n",
      "  UTAH: 35\n",
      "  VERMONT: 79\n",
      "  VIRGINIA: 196\n",
      "  WASHINGTON: 167\n",
      "  WEST VIRGINIA: 115\n",
      "  WISCONSIN: 502\n",
      "  WYOMING: 27\n",
      "  ──────────────────────────────\n",
      "  TOTAL: 15629\n",
      "\n",
      "Comparison file written to data/Newspaper Directory Excel\\Rowell 1890_comparison.txt\n",
      "  Done!\n",
      "\n",
      "Processed 9 files.\n"
     ]
    }
   ],
   "source": [
    "# post 1877 data extraction\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# US states and territories (1877 era)\n",
    "US_STATES = {\n",
    "    'ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO',\n",
    "    'CONNECTICUT', 'DELAWARE', 'DISTRICT OF COLUMBIA', 'FLORIDA', 'GEORGIA',\n",
    "    'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA',\n",
    "    'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI',\n",
    "    'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY',\n",
    "    'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO',\n",
    "    'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA',\n",
    "    'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA',\n",
    "    'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING', 'DAKOTA',\n",
    "    'INDIAN TERRITORY', 'MONTANA TERRITORY', 'NEW MEXICO TERRITORY',\n",
    "    'UTAH TERRITORY', 'WASHINGTON TERRITORY', 'WYOMING TERRITORY',\n",
    "    'ONTARIO', 'QUEBEC', 'NOVA SCOTIA', 'NEW BRUNSWICK', 'MANITOBA',\n",
    "    'BRITISH COLUMBIA', 'PRINCE EDWARD ISLAND', 'NEWFOUNDLAND',\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_state(state: str) -> str:\n",
    "    \"\"\"Normalize state names (handle OCR issues).\"\"\"\n",
    "    replacements = {'Μ': 'M', 'Α': 'A', 'Ε': 'E', 'Ο': 'O', 'Ι': 'I', \n",
    "                    'Îœ': 'M', 'Î': 'A', 'œ': 'M', 'Ñ€Ð¾Ñ€': 'pop'}\n",
    "    for old, new in replacements.items():\n",
    "        state = state.replace(old, new)\n",
    "    state = ''.join(c for c in state if ord(c) < 128 or c.isalpha())\n",
    "    return state.strip().rstrip('. ').strip()\n",
    "\n",
    "\n",
    "def is_state_header(line: str) -> bool:\n",
    "    \"\"\"Check if a line is a state header.\"\"\"\n",
    "    line = line.strip()\n",
    "    if not line or len(line) < 4:\n",
    "        return False\n",
    "    cleaned = normalize_state(line)\n",
    "    alpha_chars = [c for c in cleaned if c.isalpha()]\n",
    "    if not alpha_chars or len(alpha_chars) < 4:\n",
    "        return False\n",
    "    if sum(1 for c in alpha_chars if c.isupper()) / len(alpha_chars) < 0.8:\n",
    "        return False\n",
    "    return cleaned in US_STATES\n",
    "\n",
    "\n",
    "def is_town_line(line: str) -> bool:\n",
    "    \"\"\"Check if line starts a town entry.\"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return False\n",
    "    \n",
    "    # Normalize \"ST . \" to \"ST. \" (OCR artifact)\n",
    "    line = re.sub(r'^(ST|MT|FT|PT)\\s+\\.', r'\\1.', line)\n",
    "    \n",
    "    if not re.match(r'^(?:(?:ST|MT|FT|PT)\\s?\\.| EL|LA|LE|DE|[A-Z]{3})', line):\n",
    "        return False\n",
    "    \n",
    "    # Town: all caps, may include hyphens/periods, AND may have space-separated words\n",
    "    town = r'[A-Z][A-Z\\-\\.]*(?:\\s+[A-Z][A-Z\\-\\.]*)*'\n",
    "    \n",
    "    # County: Initial cap word(s) like \"Mariposa\" or \"Contra Costa\"\n",
    "    county = r'[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*'\n",
    "    \n",
    "    patterns = [\n",
    "        # === County seat patterns (C. H.) ===\n",
    "        rf'^{town}\\s?,\\s*C\\.\\s*H\\.\\s*,',\n",
    "        rf'^{town}\\s?,\\s*C\\.H\\.\\s*,',\n",
    "        rf'^{town}\\s?,\\s*C\\.\\s*H\\.\\s*,\\s*{county}\\s+Co\\.?\\s*[,\\s]',\n",
    "        rf'^{town}\\s?,\\s*C\\.H\\.\\s*,\\s*{county}\\s+Co\\.?\\s*[,\\s]',\n",
    "        rf'^{town}\\s?,\\s*C\\.\\s*H\\.\\s*$',\n",
    "        rf'^{town}\\s?,\\s*C\\.H\\.\\s*$',\n",
    "        rf'^{town}\\s?,\\s*c\\.\\s*h\\.\\s*,\\s*{county}\\s+Co\\.?\\s*[,\\s]',\n",
    "        \n",
    "        # === Standard \"County Co.\" patterns ===\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.\\s*,',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.\\s*$',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.\\s*;',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\s*,',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\s*$',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.?\\s*,?\\s*[\\d,]+.*pop',\n",
    "        \n",
    "        # === OCR variation patterns ===\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.\\s+,',\n",
    "        rf'^{town}\\s?,\\s*C\\.\\s*H\\.\\s*,\\s*{county}\\s+Co\\.\\s+,',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.\\s*\\d',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co,\\.',\n",
    "        rf'^{town}\\s?,\\s*C\\.\\s*H\\.\\s*,\\s*{county}\\s+Co,\\.',\n",
    "        rf'^{town}\\s?,\\s*{county}\\s+Co\\.\\s*,\\s*[a-z]',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if re.match(pattern, line):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_town_name(line: str) -> str:\n",
    "    \"\"\"Extract town name from the start of a town line.\"\"\"\n",
    "    # Normalize \"ST . \" to \"ST. \" at start (OCR artifact)\n",
    "    line = re.sub(r'^(ST|MT|FT|PT)\\s+\\.', r'\\1.', line)\n",
    "\n",
    "    match = re.match(r'^([A-Z][A-Z\\s\\-\\.]+?)\\s?,', line)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "def find_newspapers_in_block(block_text: str, town_name: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Find all newspaper entries in a block of text.\"\"\"\n",
    "    newspapers = []\n",
    "    \n",
    "    # Frequency words\n",
    "    freq_words = r'(?:every\\s*(?:morning|evening|afternoon|day)|Sundays?|Mondays?|Tuesdays?|Wednesdays?|Thursdays?|Fridays?|Saturdays?|daily|weekly|semi-weekly|tri-weekly|monthly|bi-monthly|semi-monthly|quarterly)'\n",
    "    \n",
    "    # Political/type affiliations\n",
    "    political_words = r'(?:democratic|republican|independent|neutral|liberal|conservative|greenback|prohibition|baptist|congregational|methodist|universalist|religious|agricultural|literary|german|french|spanish|comic)'\n",
    "    \n",
    "    # Combined indicator pattern\n",
    "    indicator_pattern = rf'(?:{freq_words}|{political_words})'\n",
    "    \n",
    "    # Find all potential newspaper names (ALL CAPS sequences)\n",
    "    name_pattern = r'([A-Z][A-Z\\s\\-\\&\\.\\']+[A-Z])\\s*[;:]'\n",
    "    \n",
    "    valid_newspapers = []\n",
    "    \n",
    "    for name_match in re.finditer(name_pattern, block_text):\n",
    "        name = name_match.group(1).strip()\n",
    "        name_end = name_match.end()\n",
    "        \n",
    "        if name == town_name:\n",
    "            continue\n",
    "        \n",
    "        clean_name = name.replace('.', '').replace(' ', '').replace('-', '').replace('&', '').replace(\"'\", '')\n",
    "        if len(clean_name) < 3:\n",
    "            continue\n",
    "        if name.strip('. ') in ['CO', 'RD', 'POP', 'THE', 'AND', 'FOR', 'N', 'S', 'E', 'W', 'C', 'H']:\n",
    "            continue\n",
    "        if name.strip('. ') in US_STATES:\n",
    "            continue\n",
    "        \n",
    "        lookahead_text = block_text[name_end:name_end + 200]\n",
    "        \n",
    "        next_caps_match = re.search(r'[;:]\\s*([A-Z][A-Z\\s\\-\\&\\.\\']{2,}[A-Z])\\s*[;:]', lookahead_text)\n",
    "        if next_caps_match:\n",
    "            lookahead_text = lookahead_text[:next_caps_match.start()]\n",
    "        \n",
    "        sections = re.split(r'\\s*;\\s*', lookahead_text)[:3]\n",
    "        lookahead_limited = ' ; '.join(sections)\n",
    "        \n",
    "        if re.search(indicator_pattern, lookahead_limited, re.IGNORECASE):\n",
    "            valid_newspapers.append((name_match.start(), name_end, name))\n",
    "    \n",
    "    for i, (name_start, name_end, name) in enumerate(valid_newspapers):\n",
    "        if i + 1 < len(valid_newspapers):\n",
    "            entry_end = valid_newspapers[i + 1][0]\n",
    "        else:\n",
    "            entry_end = len(block_text)\n",
    "        \n",
    "        entry_text = block_text[name_start:entry_end].strip()\n",
    "        entry_text = re.sub(r'\\s+[A-Z]{3,}[A-Z\\s\\-\\.]*,\\s*(?:C\\.\\s*H\\.|[A-Z][a-z]+\\s+Co).*$', '', entry_text, flags=re.DOTALL)\n",
    "        \n",
    "        newspapers.append((name, entry_text))\n",
    "    \n",
    "    suspended_pattern = r'([A-Z][A-Z\\s\\-\\&\\.\\']+[A-Z])\\s*[\\.:]?\\s*(?:††|â€\\s*â€|‡‡|\\.\\s*â€)'\n",
    "    for match in re.finditer(suspended_pattern, block_text):\n",
    "        name = match.group(1).strip()\n",
    "        clean_name = name.replace('.', '').replace(' ', '')\n",
    "        if name == town_name or len(clean_name) < 3:\n",
    "            continue\n",
    "        if name.strip('. ') in US_STATES:\n",
    "            continue\n",
    "        if not any(n[0] == name for n in newspapers):\n",
    "            newspapers.append((name, f\"{name} †† (suspended publication)\"))\n",
    "    \n",
    "    return newspapers\n",
    "\n",
    "\n",
    "def extract_frequency(text: str) -> str:\n",
    "    \"\"\"Extract publication frequency.\"\"\"\n",
    "    freq_map = [\n",
    "        (r'every\\s*morning', 'Daily'),\n",
    "        (r'every\\s*evening', 'Daily'),\n",
    "        (r'every\\s*afternoon', 'Daily'),\n",
    "        (r'every\\s*day', 'Daily'),\n",
    "        (r'semi-weekly', 'Semi-weekly'), (r'tri-weekly', 'Tri-weekly'),\n",
    "        (r'bi-weekly', 'Bi-weekly'), (r'bi-monthly', 'Bi-monthly'),\n",
    "        (r'semi-month', 'Semi-monthly'),\n",
    "        (r'sundays?', 'Sundays'), (r'mondays?', 'Mondays'), (r'tuesdays?', 'Tuesdays'),\n",
    "        (r'wednesdays?', 'Wednesdays'), (r'thursdays?', 'Thursdays'),\n",
    "        (r'fridays?', 'Fridays'), (r'saturdays?', 'Saturdays'),\n",
    "        (r'\\bdaily\\b', 'Daily'), (r'\\bweekly\\b', 'Weekly'),\n",
    "        (r'\\bmonthly\\b', 'Monthly'), (r'\\bquarterly\\b', 'Quarterly'),\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    for pattern, freq in freq_map:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return freq\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_political(text: str) -> str:\n",
    "    \"\"\"Extract political affiliation.\"\"\"\n",
    "    affil_map = [\n",
    "        (r'\\bdemocrat', 'Democratic'), (r'\\brepublican', 'Republican'),\n",
    "        (r'\\bindependent', 'Independent'), (r'\\bneutral', 'Neutral'),\n",
    "        (r'\\bliberal', 'Liberal'), (r'\\bconservative', 'Conservative'),\n",
    "        (r'\\bgreenback', 'Greenback'), (r'\\bprohibition', 'Prohibition'),\n",
    "        (r'\\bbaptist', 'Baptist'), (r'\\bcongregational', 'Congregational'),\n",
    "        (r'\\bmethodist', 'Methodist'), (r'\\buniversalist', 'Universalist'),\n",
    "        (r'\\breligious', 'Religious'), (r'\\bagricultural', 'Agricultural'),\n",
    "        (r'\\bliterary', 'Literary'), (r'\\bgerman', 'German'),\n",
    "        (r'\\bcomic', 'Comic'),\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    for pattern, affil in affil_map:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return affil\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_price(text: str) -> str:\n",
    "    \"\"\"Extract subscription price.\"\"\"\n",
    "    patterns = [\n",
    "        r'subscription\\s*\\$\\s*(\\d+)\\s+(\\d+)',\n",
    "        r'subscription\\s*\\$\\s*(\\d+\\.\\d+)',\n",
    "        r'subscription\\s*\\$\\s*(\\d+)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text.lower())\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            if len(groups) == 2:\n",
    "                return f\"${groups[0]}.{groups[1]}\"\n",
    "            return f\"${groups[0]}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_established(text: str) -> str:\n",
    "    \"\"\"Extract year established.\"\"\"\n",
    "    patterns = [\n",
    "        r'estab-?\\s*lished\\s*(\\d{4})',\n",
    "        r'established\\s*(\\d{4})',\n",
    "        r're-established\\s*(\\d{4})',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text.lower())\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_editor_publisher(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Extract editor and publisher names.\"\"\"\n",
    "    editor = \"\"\n",
    "    publisher = \"\"\n",
    "    \n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+?),?\\s*ed[-\\s]*i[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?\\s*and\\s+pub[-\\s]*l[-\\s]*i[-\\s]*s[-\\s]*h[-\\s]*e[-\\s]*r[-\\s]*s?', text)\n",
    "    if match:\n",
    "        name = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "        return name, name\n",
    "    \n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+?),?\\s*ed[-\\s]*i[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?\\s*and\\s+pro[-\\s]*p[-\\s]*r[-\\s]*i[-\\s]*e[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?', text)\n",
    "    if match:\n",
    "        name = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "        return name, name\n",
    "    \n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+?),?\\s*ed[-\\s]*i[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?[;,:\\s]', text)\n",
    "    if match:\n",
    "        editor = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "    \n",
    "    match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+(?:\\s+Co\\.)?),?\\s*pub[-\\s]*l[-\\s]*i[-\\s]*s[-\\s]*h[-\\s]*e[-\\s]*r[-\\s]*s?[;,:\\s\\.]', text)\n",
    "    if match:\n",
    "        publisher = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "    \n",
    "    if not publisher:\n",
    "        match = re.search(r'([A-Z][A-Za-z\\.\\s\\&\\-,]+(?:\\s+Co\\.)?),?\\s*pro[-\\s]*p[-\\s]*r[-\\s]*i[-\\s]*e[-\\s]*t[-\\s]*o[-\\s]*r[-\\s]*s?[;,:\\s\\.]', text)\n",
    "        if match:\n",
    "            publisher = re.sub(r'\\s+', ' ', match.group(1).strip().rstrip(',;'))\n",
    "    \n",
    "    return editor, publisher\n",
    "\n",
    "\n",
    "def extract_circulation_early(text: str) -> str:\n",
    "    \"\"\"Extract circulation number for 1877-1878 (numeric values).\"\"\"\n",
    "    # Normalize line-break hyphens first\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "    \n",
    "    # Pattern for circulation with numeric values\n",
    "    pattern = r\"circ?(?:ulation|'n|e'n)[\\s\\-]*(?:about|approximately|nearly|over|around)?[\\s\\-]*(?:\\w+\\s+)?(\\d[\\d,]*)\"\n",
    "    \n",
    "    # Also check for \"claims XXX\" pattern\n",
    "    claims_pattern = r\"claims[\\s\\-]+(\\d[\\d,]*)\"\n",
    "    \n",
    "    match = re.search(pattern, text.lower())\n",
    "    if not match:\n",
    "        match = re.search(claims_pattern, text.lower())\n",
    "    \n",
    "    if match:\n",
    "        circ = match.group(1).replace(',', '')\n",
    "        if 'estimated' in text.lower() or \"est'd\" in text.lower():\n",
    "            return f\"{circ} (estimated)\"\n",
    "        return circ\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "CIRCULATION_RATINGS = {\n",
    "    \"A1\": 100000, \"A\": 75000,\n",
    "    \"B1\": 50000, \"B\": 37500,\n",
    "    \"C1\": 25000, \"C\": 22500,\n",
    "    \"D1\": 20000, \"D\": 17500,\n",
    "    \"E1\": 15000, \"E\": 12500,\n",
    "    \"F1\": 10000, \"F\": 7500,\n",
    "    \"G1\": 5000, \"G\": 4000,\n",
    "    \"H1\": 3000, \"H\": 2500,\n",
    "    \"I1\": 2000, \"I\": 1500,\n",
    "    \"J1\": 1000, \"J\": 750,\n",
    "    \"K1\": 500, \"K\": 250,\n",
    "    \"X1\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def extract_circulation_late(text: str) -> Optional[int]:\n",
    "    \"\"\"Extract circulation for 1879+ (letter rating codes).\"\"\"\n",
    "    # First try letter codes (e.g., \"circulation K 1\", \"circulationK\", \"circulation G\")\n",
    "    match = re.search(r\"circ(?:ulation|'n)\\s*([A-KX])\\s*([1])?\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        letter = match.group(1).upper()\n",
    "        suffix = match.group(2) or \"\"\n",
    "        code = letter + suffix\n",
    "        return CIRCULATION_RATINGS.get(code)\n",
    "    \n",
    "    # Then try numeric values (e.g., \"circulation 350\", \"circ'n 800\")\n",
    "    match = re.search(r\"circ(?:ulation|'n)\\s*(\\d[\\d,]*)\", text.lower())\n",
    "    if match:\n",
    "        circ = match.group(1).replace(',', '')\n",
    "        return int(circ)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_newspaper_entry(name: str, text: str, state: str, town: str, year: int) -> Dict:\n",
    "    \"\"\"Parse a newspaper entry and return structured data.\"\"\"\n",
    "    editor, publisher = extract_editor_publisher(text)\n",
    "    \n",
    "    # Use different circulation extraction based on year\n",
    "    if year <= 1878:\n",
    "        circulation = extract_circulation_early(text)\n",
    "    else:\n",
    "        circulation = extract_circulation_late(text)\n",
    "    \n",
    "    return {\n",
    "        'state': state,\n",
    "        'town': town,\n",
    "        'newspaper_name': name,\n",
    "        'frequency': extract_frequency(text),\n",
    "        'political_affiliation': extract_political(text),\n",
    "        'subscription_price': extract_price(text),\n",
    "        'established': extract_established(text),\n",
    "        'editor': editor,\n",
    "        'publisher': publisher,\n",
    "        'circulation': circulation,\n",
    "        'raw_text': re.sub(r'\\s+', ' ', text).strip()\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_text(content: str) -> str:\n",
    "    \"\"\"Preprocess text to handle formatting issues.\"\"\"\n",
    "    content = re.sub(r'---\\s*Page\\s*\\d+\\s*---', '\\n<<PAGE_BREAK>>\\n', content)\n",
    "    \n",
    "    removals = [\n",
    "        r'^GEO\\s*\\.\\s*P\\.?\\s*ROWELL.*$',\n",
    "        r'^P\\.\\s*ROWELL\\s*&.*$',\n",
    "        r'^AMERICAN NEWSPAPER DIRECTORY\\s*\\.?\\s*$',\n",
    "        r'^LIBRARY\\s*$', r'^UNIVERSITY\\s+OF\\s*$',\n",
    "        r'^EXPLANATIONS\\s*\\.?\\s*$', r'^POPULATION\\s*\\.?\\s*$',\n",
    "        r'^CIRCULATION\\s*\\.?\\s*$', r'^ITALIC WORDS\\s*\\.?\\s*$',\n",
    "        r'^\\d+\\s*$', r\"^CO'S\\s*$\", r'^GEO\\s*\\.\\s*$',\n",
    "    ]\n",
    "    for pattern in removals:\n",
    "        content = re.sub(pattern, '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    for state in US_STATES:\n",
    "        pattern = r'(<<PAGE_BREAK>>)\\s*\\n?\\s*' + re.escape(state) + r'\\s*\\.?\\s*$'\n",
    "        content = re.sub(pattern, r'\\1', content, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    content = re.sub(r'<<PAGE_BREAK>>', '\\n', content)\n",
    "    content = re.sub(r'A LIST,\\s*ARRANGED ALPHABETICALLY.*?ETC\\.', '', content, flags=re.DOTALL)\n",
    "    return content\n",
    "\n",
    "\n",
    "def extract_newspapers(content: str, year: int) -> List[Dict]:\n",
    "    \"\"\"Main extraction function.\"\"\"\n",
    "    content = preprocess_text(content)\n",
    "    lines = content.split('\\n')\n",
    "    \n",
    "    results = []\n",
    "    current_state = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if not line:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        if is_state_header(line):\n",
    "            current_state = normalize_state(line)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        if current_state and is_town_line(line):\n",
    "            town_name = extract_town_name(line)\n",
    "            if not town_name:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            block_lines = [line]\n",
    "            j = i + 1\n",
    "            while j < len(lines):\n",
    "                next_line = lines[j].strip()\n",
    "                if not next_line:\n",
    "                    block_lines.append('')\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                if is_state_header(next_line):\n",
    "                    normalized_next = normalize_state(next_line)\n",
    "                    if normalized_next == current_state:\n",
    "                        j += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if is_town_line(next_line):\n",
    "                    break\n",
    "                    \n",
    "                block_lines.append(next_line)\n",
    "                j += 1\n",
    "            \n",
    "            block_text = ' '.join(block_lines)\n",
    "            block_text = re.sub(r'-\\s+', '', block_text)\n",
    "            block_text = re.sub(r'\\s+', ' ', block_text)\n",
    "            \n",
    "            newspapers = find_newspapers_in_block(block_text, town_name)\n",
    "            \n",
    "            for name, entry_text in newspapers:\n",
    "                entry = parse_newspaper_entry(name, entry_text, current_state, town_name, year)\n",
    "                results.append(entry)\n",
    "            \n",
    "            i = j\n",
    "            continue\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_state_counts(newspapers: List[Dict]):\n",
    "    \"\"\"Print the number of newspaper entries found for each state.\"\"\"\n",
    "    state_counts = {}\n",
    "    for entry in newspapers:\n",
    "        state = entry['state']\n",
    "        state_counts[state] = state_counts.get(state, 0) + 1\n",
    "    \n",
    "    print(\"\\n=== Newspaper Entries by State ===\")\n",
    "    for state in sorted(state_counts.keys()):\n",
    "        print(f\"  {state}: {state_counts[state]}\")\n",
    "    print(f\"  {'─' * 30}\")\n",
    "    print(f\"  TOTAL: {len(newspapers)}\\n\")\n",
    "\n",
    "\n",
    "def main(input_file: str, output_file: str, year: int):\n",
    "    \"\"\"Process input file and write CSV output.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    newspapers = extract_newspapers(content, year)\n",
    "    \n",
    "    fieldnames = ['state', 'town', 'newspaper_name', 'frequency', 'political_affiliation',\n",
    "                  'subscription_price', 'established', 'editor', 'publisher', 'circulation', 'raw_text']\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(newspapers)\n",
    "    \n",
    "    print(f\"Extracted {len(newspapers)} newspaper entries to {output_file}\")\n",
    "    print_state_counts(newspapers)\n",
    "    return newspapers\n",
    "\n",
    "\n",
    "def output_comparison_file(input_file: str, output_file: str, comparison_file: str):\n",
    "    \"\"\"Create a comparison file with chars 15000-35000 of input and output.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        f.seek(40000)\n",
    "        input_content = f.read(20000)\n",
    "    \n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        f.seek(37000)\n",
    "        output_content = f.read(20000)\n",
    "    \n",
    "    with open(comparison_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== INPUT FILE (chars 15000-35000) ===\\n\\n\")\n",
    "        f.write(input_content)\n",
    "        f.write(\"\\n\\n=== OUTPUT FILE (chars 15000-35000) ===\\n\\n\")\n",
    "        f.write(output_content)\n",
    "    \n",
    "    print(f\"Comparison file written to {comparison_file}\")\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = 'data/Newspaper Directory Text'\n",
    "    output_folder = 'data/Newspaper Directory Excel'\n",
    "    \n",
    "    # Find all files matching the pattern with v13 in the name\n",
    "    pattern = os.path.join(input_folder, 'Rowell * - v13.txt')\n",
    "    input_files = glob.glob(pattern)\n",
    "    \n",
    "    for input_file in input_files:\n",
    "        # Extract the year from the filename\n",
    "        filename = os.path.basename(input_file)\n",
    "        match = re.search(r'Rowell (\\d{4}) - v13\\.txt', filename)\n",
    "        \n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            \n",
    "            # Only process files from 1877 onwards\n",
    "            if year < 1877:\n",
    "                continue\n",
    "            \n",
    "            output_file = os.path.join(output_folder, f'Rowell {year}.csv')\n",
    "            comparison_file = os.path.join(output_folder, f'Rowell {year}_comparison.txt')\n",
    "            \n",
    "            print(f\"Processing: {input_file} (Year: {year})\")\n",
    "            print(f\"  -> Output: {output_file}\")\n",
    "            print(f\"  -> Comparison: {comparison_file}\")\n",
    "            \n",
    "            main(input_file, output_file, year)\n",
    "            output_comparison_file(input_file, output_file, comparison_file)\n",
    "            \n",
    "            print(f\"  Done!\\n\")\n",
    "\n",
    "    print(f\"Processed {len(input_files)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb3189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CSVs from: data\\Newspaper Directory Excel\n",
      "============================================================\n",
      "Loaded Rowell 1869.csv (year 1869): 3072 records\n",
      "Loaded Rowell 1871.csv (year 1871): 5878 records\n",
      "Loaded Rowell 1872.csv (year 1872): 6241 records\n",
      "Loaded Rowell 1873.csv (year 1873): 6550 records\n",
      "Loaded Rowell 1876.csv (year 1876): 7825 records\n",
      "Loaded Rowell 1877.csv (year 1877): 7274 records\n",
      "Loaded Rowell 1878.csv (year 1878): 7548 records\n",
      "Loaded Rowell 1879.csv (year 1879): 7785 records\n",
      "Loaded Rowell 1880.csv (year 1880): 8561 records\n",
      "Loaded Rowell 1882.csv (year 1882): 10435 records\n",
      "Loaded Rowell 1883.csv (year 1883): 10171 records\n",
      "Loaded Rowell 1884.csv (year 1884): 11484 records\n",
      "Loaded Rowell 1885.csv (year 1885): 12282 records\n",
      "Loaded Rowell 1890.csv (year 1890): 15629 records\n",
      "Processing 14 files...\n",
      "Strategy: exact match first, then fuzzy match (same first letter, 90% similarity),\n",
      "          with days of week removed from BOTH current and existing names,\n",
      "          80% threshold if established dates match\n",
      "\n",
      "  Processing year 1869 (3072 records)...\n",
      "    -> 0 matched to existing, 3072 new records\n",
      "  Processing year 1871 (5878 records)...\n",
      "    -> 1735 matched to existing, 4143 new records\n",
      "  Processing year 1872 (6241 records)...\n",
      "    -> 4716 matched to existing, 1525 new records\n",
      "  Processing year 1873 (6550 records)...\n",
      "    -> 5038 matched to existing, 1512 new records\n",
      "  Processing year 1876 (7825 records)...\n",
      "    -> 4225 matched to existing, 3600 new records\n",
      "  Processing year 1877 (7274 records)...\n",
      "    -> 5062 matched to existing, 2212 new records\n",
      "  Processing year 1878 (7548 records)...\n",
      "    -> 5583 matched to existing, 1965 new records\n",
      "  Processing year 1879 (7785 records)...\n",
      "    -> 5652 matched to existing, 2133 new records\n",
      "  Processing year 1880 (8561 records)...\n",
      "    -> 5809 matched to existing, 2752 new records\n",
      "  Processing year 1882 (10435 records)...\n",
      "    -> 6264 matched to existing, 4171 new records\n",
      "  Processing year 1883 (10171 records)...\n",
      "    -> 7402 matched to existing, 2769 new records\n",
      "  Processing year 1884 (11484 records)...\n",
      "    -> 7875 matched to existing, 3609 new records\n",
      "  Processing year 1885 (12282 records)...\n",
      "    -> 8285 matched to existing, 3997 new records\n",
      "  Processing year 1890 (15629 records)...\n",
      "    -> 4584 matched to existing, 11045 new records\n",
      "\n",
      "Total unique newspapers found: 47956\n",
      "\n",
      "Success! Output saved to: master.csv\n",
      "Total newspapers: 47956\n",
      "\n",
      "Columns in output:\n",
      "  - state\n",
      "  - town\n",
      "  - newspaper_name\n",
      "  - 1869 circulation\n",
      "  - 1869 editor\n",
      "  - 1869 established\n",
      "  - 1869 frequency\n",
      "  - 1869 political_affiliation\n",
      "  - 1869 publisher\n",
      "  - 1869 subscription_price\n",
      "  - 1871 circulation\n",
      "  - 1871 editor\n",
      "  - 1871 frequency\n",
      "  - 1871 political_affiliation\n",
      "  - 1871 publisher\n",
      "  - 1872 circulation\n",
      "  - 1872 editor\n",
      "  - 1872 frequency\n",
      "  - 1872 political_affiliation\n",
      "  - 1872 publisher\n",
      "  - 1873 circulation\n",
      "  - 1873 editor\n",
      "  - 1873 frequency\n",
      "  - 1873 political_affiliation\n",
      "  - 1873 publisher\n",
      "  - 1876 circulation\n",
      "  - 1876 editor\n",
      "  - 1876 frequency\n",
      "  - 1876 political_affiliation\n",
      "  - 1876 publisher\n",
      "  - 1877 circulation\n",
      "  - 1877 editor\n",
      "  - 1877 established\n",
      "  - 1877 frequency\n",
      "  - 1877 political_affiliation\n",
      "  - 1877 publisher\n",
      "  - 1877 subscription_price\n",
      "  - 1878 circulation\n",
      "  - 1878 editor\n",
      "  - 1878 established\n",
      "  - 1878 frequency\n",
      "  - 1878 political_affiliation\n",
      "  - 1878 publisher\n",
      "  - 1878 subscription_price\n",
      "  - 1879 circulation\n",
      "  - 1879 editor\n",
      "  - 1879 established\n",
      "  - 1879 frequency\n",
      "  - 1879 political_affiliation\n",
      "  - 1879 publisher\n",
      "  - 1879 subscription_price\n",
      "  - 1880 circulation\n",
      "  - 1880 editor\n",
      "  - 1880 established\n",
      "  - 1880 frequency\n",
      "  - 1880 political_affiliation\n",
      "  - 1880 publisher\n",
      "  - 1880 subscription_price\n",
      "  - 1882 circulation\n",
      "  - 1882 editor\n",
      "  - 1882 established\n",
      "  - 1882 frequency\n",
      "  - 1882 political_affiliation\n",
      "  - 1882 publisher\n",
      "  - 1882 subscription_price\n",
      "  - 1883 circulation\n",
      "  - 1883 editor\n",
      "  - 1883 established\n",
      "  - 1883 frequency\n",
      "  - 1883 political_affiliation\n",
      "  - 1883 publisher\n",
      "  - 1883 subscription_price\n",
      "  - 1884 circulation\n",
      "  - 1884 editor\n",
      "  - 1884 established\n",
      "  - 1884 frequency\n",
      "  - 1884 political_affiliation\n",
      "  - 1884 publisher\n",
      "  - 1884 subscription_price\n",
      "  - 1885 circulation\n",
      "  - 1885 editor\n",
      "  - 1885 established\n",
      "  - 1885 frequency\n",
      "  - 1885 political_affiliation\n",
      "  - 1885 publisher\n",
      "  - 1885 subscription_price\n",
      "  - 1890 circulation\n",
      "  - 1890 editor\n",
      "  - 1890 established\n",
      "  - 1890 frequency\n",
      "  - 1890 political_affiliation\n",
      "  - 1890 publisher\n",
      "  - 1890 subscription_price\n"
     ]
    }
   ],
   "source": [
    "# new merger\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "DAYS_OF_WEEK = ['sundays', 'mondays', 'tuesdays', 'wednesdays', 'thursdays', 'fridays', 'saturdays']\n",
    "\n",
    "# Precompile regex patterns for day removal and normalization\n",
    "_DAYS_PATTERN = re.compile('|'.join(re.escape(day) for day in DAYS_OF_WEEK), re.IGNORECASE)\n",
    "_NORMALIZE_PATTERN = re.compile(r\"[.,'\\s]\")\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for matching: lowercase, strip whitespace, remove punctuation.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return _NORMALIZE_PATTERN.sub(\"\", str(s).lower().strip())\n",
    "\n",
    "def normalize_text_no_days(s):\n",
    "    \"\"\"Normalize text and also remove days of the week.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    text = _DAYS_PATTERN.sub(\"\", str(s).lower().strip())\n",
    "    return _NORMALIZE_PATTERN.sub(\"\", text)\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"Calculate similarity ratio between two strings (0 to 1).\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def is_fuzzy_match(town1, name1, town2, name2, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Check if two newspaper records match using fuzzy matching.\n",
    "    More strict: requires high similarity on both fields.\n",
    "    \"\"\"\n",
    "    town_sim = similarity(town1, town2)\n",
    "    name_sim = similarity(name1, name2)\n",
    "    \n",
    "    if town_sim >= threshold and name_sim >= threshold:\n",
    "        return True\n",
    "    if town_sim == 1.0 and name_sim >= 0.85:\n",
    "        return True\n",
    "    if name_sim == 1.0 and town_sim >= 0.85:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def remove_days_from_name(name):\n",
    "    \"\"\"Remove days of the week from a newspaper name, preserving original formatting.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    result = _DAYS_PATTERN.sub('', str(name))\n",
    "    # Clean up extra spaces\n",
    "    return ' '.join(result.split()).strip()\n",
    "\n",
    "def find_best_match(town, name, position_pct, existing_records_flat, existing_records_by_letter,\n",
    "                    existing_records_no_days, current_established=None, established_lookup=None,\n",
    "                    threshold=0.90):\n",
    "    \"\"\"\n",
    "    Find the best matching key from existing records.\n",
    "    First tries exact match, then fuzzy match for towns with same first letter,\n",
    "    then tries again with days of week removed from BOTH current and existing names.\n",
    "    Uses lower threshold (80%) if established dates match.\n",
    "    \n",
    "    Returns tuple: (matched_key or None, matched_via_days_removal: bool)\n",
    "    \"\"\"\n",
    "    town_norm = normalize_text(town)\n",
    "    name_norm = normalize_text(name)\n",
    "    name_norm_no_days = normalize_text_no_days(name)\n",
    "    \n",
    "    # First: try exact match (using flat dict)\n",
    "    exact_key = (town_norm, name_norm)\n",
    "    if exact_key in existing_records_flat:\n",
    "        return exact_key, False\n",
    "    \n",
    "    # Get first letter of town for filtering\n",
    "    town_first_letter = town_norm[0] if town_norm else \"\"\n",
    "    if not town_first_letter:\n",
    "        return None, False\n",
    "    \n",
    "    # Get candidates with same first letter (pre-indexed)\n",
    "    candidates = existing_records_by_letter.get(town_first_letter, {})\n",
    "    if not candidates:\n",
    "        return None, False\n",
    "    \n",
    "    # Second: try fuzzy match, only considering towns with same first letter\n",
    "    # Now also compares with days removed from BOTH names\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    matched_via_days = False\n",
    "    \n",
    "    for (ex_town, ex_name), ex_position in candidates.items():\n",
    "        # Check if established dates match for lower threshold\n",
    "        effective_threshold = threshold\n",
    "        if current_established and established_lookup:\n",
    "            ex_established = established_lookup.get((ex_town, ex_name))\n",
    "            if ex_established and str(current_established).strip() == str(ex_established).strip():\n",
    "                effective_threshold = 0.80\n",
    "        \n",
    "        # Try standard fuzzy match first\n",
    "        if is_fuzzy_match(town_norm, name_norm, ex_town, ex_name, effective_threshold):\n",
    "            score = similarity(town_norm, ex_town) + similarity(name_norm, ex_name)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = (ex_town, ex_name)\n",
    "                matched_via_days = False\n",
    "            continue  # Found a match, no need to try days-removed for this record\n",
    "        \n",
    "        # Try with days removed from BOTH current and existing names\n",
    "        # Use pre-computed no-days version\n",
    "        ex_name_no_days = existing_records_no_days.get((ex_town, ex_name), ex_name)\n",
    "        if is_fuzzy_match(town_norm, name_norm_no_days, ex_town, ex_name_no_days, effective_threshold):\n",
    "            score = similarity(town_norm, ex_town) + similarity(name_norm_no_days, ex_name_no_days)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = (ex_town, ex_name)\n",
    "                matched_via_days = True\n",
    "    \n",
    "    return best_match, matched_via_days\n",
    "\n",
    "def load_and_tag_csvs(directory):\n",
    "    \"\"\"Load all CSVs from directory, tag with year, and split into pre/post 1877.\"\"\"\n",
    "    csv1_frames = []\n",
    "    csv2_frames = []\n",
    "    \n",
    "    for file in Path(directory).glob(\"*.csv\"):\n",
    "        filename = file.stem\n",
    "        year = None\n",
    "        \n",
    "        match = re.search(r'(1[89]\\d{2})', filename)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "        \n",
    "        if year is None:\n",
    "            print(f\"Warning: Could not extract year from {file.name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')\n",
    "        df['_year'] = year\n",
    "        \n",
    "        print(f\"Loaded {file.name} (year {year}): {len(df)} records\")\n",
    "        \n",
    "        if year <= 1876:\n",
    "            csv1_frames.append(df)\n",
    "        else:\n",
    "            csv2_frames.append(df)\n",
    "    \n",
    "    return csv1_frames, csv2_frames\n",
    "\n",
    "def process_dataframe(df, year, has_state=False):\n",
    "    if 'raw_text' in df.columns:\n",
    "        df = df.drop(columns=['raw_text'])\n",
    "    \n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    \n",
    "    # Normalize variant column names to standard names\n",
    "    column_aliases = {\n",
    "        'newspaper': 'newspaper_name',\n",
    "        'political': 'political_affiliation',\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in column_aliases.items() if k in df.columns})\n",
    "    \n",
    "    # ... rest of function\n",
    "    \"\"\"Process a single dataframe: standardize and prepare for merging.\"\"\"\n",
    "    if 'raw_text' in df.columns:\n",
    "        df = df.drop(columns=['raw_text'])\n",
    "    \n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    \n",
    "    data_cols = ['frequency', 'political_affiliation', 'subscription_price', \n",
    "                 'established', 'editor', 'publisher', 'circulation']\n",
    "    \n",
    "    rename_map = {}\n",
    "    for col in data_cols:\n",
    "        if col in df.columns:\n",
    "            rename_map[col] = f\"{year} {col}\"\n",
    "    \n",
    "    df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "def merge_newspapers_core(all_frames):\n",
    "    \"\"\"Core merge logic used by both full and test functions.\"\"\"\n",
    "    merged_records = {}\n",
    "    # Flat dict for exact lookups\n",
    "    record_positions_flat = {}\n",
    "    # Index by first letter of town for faster fuzzy lookups\n",
    "    record_positions_by_letter = defaultdict(dict)\n",
    "    # Pre-computed normalized names with days removed\n",
    "    existing_records_no_days = {}\n",
    "    original_names = {}\n",
    "    established_lookup = {}  # Track established dates for each record\n",
    "    \n",
    "    print(f\"Processing {len(all_frames)} files...\")\n",
    "    print(\"Strategy: exact match first, then fuzzy match (same first letter, 90% similarity),\")\n",
    "    print(\"          with days of week removed from BOTH current and existing names,\")\n",
    "    print(\"          80% threshold if established dates match\\n\")\n",
    "    \n",
    "    for df, year, has_state in all_frames:\n",
    "        total_rows = len(df)\n",
    "        print(f\"  Processing year {year} ({total_rows} records)...\")\n",
    "        matches_found = 0\n",
    "        new_records = 0\n",
    "        \n",
    "        # Collect new records for this year, add to main dict after processing\n",
    "        year_new_keys = []\n",
    "        \n",
    "        # Find the established column for this year\n",
    "        established_col = f\"{year} established\"\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            row_num = df.index.get_loc(idx)\n",
    "            position_pct = row_num / max(total_rows - 1, 1)\n",
    "            \n",
    "            town = row.get('town', '')\n",
    "            name = row.get('newspaper_name', '')\n",
    "            state = row.get('state', '') if has_state else ''\n",
    "            current_established = row.get(established_col, None)\n",
    "            \n",
    "            town_norm = normalize_text(town)\n",
    "            name_norm = normalize_text(name)\n",
    "            \n",
    "            if not town_norm or not name_norm:\n",
    "                continue\n",
    "            \n",
    "            # Only match against records from previous years\n",
    "            existing_key, matched_via_days = find_best_match(\n",
    "                town, name, position_pct,\n",
    "                record_positions_flat,\n",
    "                record_positions_by_letter,\n",
    "                existing_records_no_days,\n",
    "                current_established=current_established,\n",
    "                established_lookup=established_lookup,\n",
    "                threshold=0.90\n",
    "            )\n",
    "            \n",
    "            if existing_key:\n",
    "                key = existing_key\n",
    "                matches_found += 1\n",
    "                old_pos = record_positions_flat[key]\n",
    "                new_pos = (old_pos + position_pct) / 2\n",
    "                record_positions_flat[key] = new_pos\n",
    "                # Update indexed version too\n",
    "                first_letter = key[0][0] if key[0] else \"\"\n",
    "                if first_letter:\n",
    "                    record_positions_by_letter[first_letter][key] = new_pos\n",
    "                \n",
    "                # If matched via days removal, update the stored name to remove days\n",
    "                if matched_via_days:\n",
    "                    old_town, old_name, old_state = original_names[key]\n",
    "                    cleaned_name = remove_days_from_name(old_name)\n",
    "                    original_names[key] = (old_town, cleaned_name, old_state)\n",
    "            else:\n",
    "                key = (town_norm, name_norm)\n",
    "                merged_records[key] = {}\n",
    "                original_names[key] = (town, name, state)\n",
    "                # Queue this to be added after processing this year\n",
    "                year_new_keys.append((key, position_pct, current_established))\n",
    "                new_records += 1\n",
    "            \n",
    "            if state and not original_names[key][2]:\n",
    "                original_names[key] = (original_names[key][0], original_names[key][1], state)\n",
    "            \n",
    "            year_cols = [c for c in row.index if c.startswith(f\"{year} \")]\n",
    "            for col in year_cols:\n",
    "                merged_records[key][col] = row[col]\n",
    "        \n",
    "        # Now add this year's new records to positions for next year's matching\n",
    "        for key, pos, estab in year_new_keys:\n",
    "            record_positions_flat[key] = pos\n",
    "            # Index by first letter\n",
    "            first_letter = key[0][0] if key[0] else \"\"\n",
    "            if first_letter:\n",
    "                record_positions_by_letter[first_letter][key] = pos\n",
    "            # Pre-compute no-days version\n",
    "            existing_records_no_days[key] = normalize_text_no_days(key[1])\n",
    "            if estab:\n",
    "                established_lookup[key] = estab\n",
    "        \n",
    "        print(f\"    -> {matches_found} matched to existing, {new_records} new records\")\n",
    "    \n",
    "    print(f\"\\nTotal unique newspapers found: {len(merged_records)}\")\n",
    "    \n",
    "    rows = []\n",
    "    for key, data in merged_records.items():\n",
    "        town, name, state = original_names[key]\n",
    "        row = {'state': state, 'town': town, 'newspaper_name': name}\n",
    "        row.update(data)\n",
    "        rows.append(row)\n",
    "    \n",
    "    result = pd.DataFrame(rows)\n",
    "    \n",
    "    id_cols = ['state', 'town', 'newspaper_name']\n",
    "    year_cols = [c for c in result.columns if c not in id_cols]\n",
    "    \n",
    "    def sort_key(col):\n",
    "        parts = col.split(' ', 1)\n",
    "        if len(parts) == 2 and parts[0].isdigit():\n",
    "            return (int(parts[0]), parts[1])\n",
    "        return (9999, col)\n",
    "    \n",
    "    year_cols = sorted(year_cols, key=sort_key)\n",
    "    final_cols = id_cols + year_cols\n",
    "    result = result[final_cols]\n",
    "    result = result.sort_values(['state', 'town', 'newspaper_name'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def prepare_frames(directory, max_years=None):\n",
    "    \"\"\"Load and prepare frames, optionally limiting to first N years.\"\"\"\n",
    "    csv1_frames, csv2_frames = load_and_tag_csvs(directory)\n",
    "    \n",
    "    if not csv1_frames and not csv2_frames:\n",
    "        print(\"No CSV files found!\")\n",
    "        return None\n",
    "    \n",
    "    all_frames_raw = []\n",
    "    \n",
    "    for df in csv1_frames:\n",
    "        year = df['_year'].iloc[0]\n",
    "        all_frames_raw.append((df, year, False))\n",
    "    \n",
    "    for df in csv2_frames:\n",
    "        year = df['_year'].iloc[0]\n",
    "        all_frames_raw.append((df, year, True))\n",
    "    \n",
    "    all_frames_raw.sort(key=lambda x: x[1])\n",
    "    \n",
    "    if max_years is not None:\n",
    "        all_frames_raw = all_frames_raw[:max_years]\n",
    "        years_processing = [f[1] for f in all_frames_raw]\n",
    "        print(f\"\\nTEST MODE: Processing only first {max_years} years: {years_processing}\\n\")\n",
    "    \n",
    "    all_frames = []\n",
    "    for df, year, has_state in all_frames_raw:\n",
    "        df = df.drop(columns=['_year'])\n",
    "        df = process_dataframe(df, year, has_state=has_state)\n",
    "        all_frames.append((df, year, has_state))\n",
    "    \n",
    "    return all_frames\n",
    "\n",
    "def merge_newspapers_fuzzy(directory):\n",
    "    \"\"\"Main function to merge all newspaper CSVs with fuzzy matching.\"\"\"\n",
    "    all_frames = prepare_frames(directory)\n",
    "    if all_frames is None:\n",
    "        return None\n",
    "    return merge_newspapers_core(all_frames)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    directory = r\"data\\Newspaper Directory Excel\"\n",
    "    \n",
    "    print(f\"Processing CSVs from: {directory}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = merge_newspapers_fuzzy(directory)\n",
    "    \n",
    "    if result is not None:\n",
    "        output_path = \"master.csv\"\n",
    "        result.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccess! Output saved to: {output_path}\")\n",
    "        print(f\"Total newspapers: {len(result)}\")\n",
    "        print(f\"\\nColumns in output:\")\n",
    "        for col in result.columns:\n",
    "            print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(\"Failed to create merged CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1493945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samwt\\AppData\\Local\\Temp\\ipykernel_5792\\3684759370.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/master.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years in dataset: ['1869', '1871', '1872', '1873', '1876', '1877', '1878', '1879', '1880', '1882', '1883', '1884', '1885', '1890']\n",
      "\n",
      "Newspapers by number of years with data:\n",
      "----------------------------------------\n",
      "  0 year(s): 1277 newspapers\n",
      "  1 year(s): 28675 newspapers\n",
      "  2 year(s): 6484 newspapers\n",
      "  3 year(s): 3584 newspapers\n",
      "  4 year(s): 2496 newspapers\n",
      "  5 year(s): 1467 newspapers\n",
      "  6 year(s): 949 newspapers\n",
      "  7 year(s): 756 newspapers\n",
      "  8 year(s): 747 newspapers\n",
      "  9 year(s): 608 newspapers\n",
      "  10 year(s): 521 newspapers\n",
      "  11 year(s): 525 newspapers\n",
      "  12 year(s): 565 newspapers\n",
      "  13 year(s): 325 newspapers\n",
      "  14 year(s): 65 newspapers\n",
      "\n",
      "Total newspapers: 49044\n"
     ]
    }
   ],
   "source": [
    "# descriptive stats\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/master.csv\")\n",
    "\n",
    "# Get year columns (those starting with a 4-digit year)\n",
    "year_prefixes = set()\n",
    "for col in df.columns:\n",
    "    parts = col.split(' ', 1)\n",
    "    if len(parts) == 2 and parts[0].isdigit() and len(parts[0]) == 4:\n",
    "        year_prefixes.add(parts[0])\n",
    "\n",
    "years = sorted(year_prefixes)\n",
    "print(f\"Years in dataset: {years}\\n\")\n",
    "\n",
    "# For each row, count how many years have any data\n",
    "def count_years_with_data(row):\n",
    "    count = 0\n",
    "    for year in years:\n",
    "        year_cols = [c for c in df.columns if c.startswith(f\"{year} \")]\n",
    "        if any(pd.notna(row[c]) for c in year_cols):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "df['years_of_data'] = df.apply(count_years_with_data, axis=1)\n",
    "\n",
    "# Summary\n",
    "print(\"Newspapers by number of years with data:\")\n",
    "print(\"-\" * 40)\n",
    "counts = df['years_of_data'].value_counts().sort_index()\n",
    "for num_years, count in counts.items():\n",
    "    print(f\"  {num_years} year(s): {count} newspapers\")\n",
    "\n",
    "print(f\"\\nTotal newspapers: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2daa227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samwt\\AppData\\Local\\Temp\\ipykernel_11944\\1346512040.py:6: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/master.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 570145 publisher entries\n",
      "Removed trailing ', editorand' and ', editor and'\n",
      "Saved to data/master.csv\n"
     ]
    }
   ],
   "source": [
    "# CLEANING UP master.csv \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/master.csv')\n",
    "\n",
    "# Define the years we're tracking\n",
    "years = [1869, 1871, 1872, 1873, 1876, 1877, 1878, 1879, 1880, 1882, 1883, 1884, 1885, 1890]\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = prev_row[j + 1] + 1\n",
    "            deletions = curr_row[j] + 1\n",
    "            substitutions = prev_row[j] + (c1 != c2)\n",
    "            curr_row.append(min(insertions, deletions, substitutions))\n",
    "        prev_row = curr_row\n",
    "    return prev_row[-1]\n",
    "\n",
    "# Clean publisher columns - remove trailing ', editorand' or ', editor and' (with fuzzy matching)\n",
    "def clean_publisher(val):\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val = str(val).strip()\n",
    "    lower_val = val.lower()\n",
    "    \n",
    "    # Check for ', editor and' first (exact match)\n",
    "    if lower_val.endswith(', editor and'):\n",
    "        return val[:-len(', editor and')]\n",
    "    \n",
    "    # Check for ', editorand' with fuzzy matching (1 char tolerance)\n",
    "    # Look for ', ' followed by something close to 'editorand'\n",
    "    if ', ' in lower_val:\n",
    "        last_comma_idx = lower_val.rfind(', ')\n",
    "        suffix = lower_val[last_comma_idx + 2:]  # text after ', '\n",
    "        if levenshtein_distance(suffix, 'editorand') <= 1:\n",
    "            return val[:last_comma_idx]\n",
    "        if levenshtein_distance(suffix, 'editor and') <= 1:\n",
    "            return val[:last_comma_idx]\n",
    "    \n",
    "    return val\n",
    "\n",
    "# Apply cleaning to all publisher columns\n",
    "publisher_cols = [f'{year} publisher' for year in years]\n",
    "changes_made = 0\n",
    "\n",
    "for col in publisher_cols:\n",
    "    if col in df.columns:\n",
    "        original = df[col].copy()\n",
    "        df[col] = df[col].apply(clean_publisher)\n",
    "        changes_made += (original != df[col]).sum()\n",
    "\n",
    "print(f\"Cleaned {changes_made} publisher entries\")\n",
    "print(\"Removed trailing ', editorand' and ', editor and'\")\n",
    "\n",
    "# Save back to master.csv\n",
    "df.to_csv('data/master.csv', index=False)\n",
    "print(\"Saved to data/master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd664602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
