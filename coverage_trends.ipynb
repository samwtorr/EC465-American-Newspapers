{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b61306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison file created: extraction_comparison.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "years = [1869, 1871, 1872, 1873, 1876]\n",
    "char_limit = 20000\n",
    "\n",
    "txt_dir = r\"data\\Newspaper Directory Text\"\n",
    "csv_dir = r\"data\\Newspaper Directory Excel\"\n",
    "output_file = \"extraction_comparison.md\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(\"# Data Extraction Comparison\\n\\n\")\n",
    "    out.write(\"This file contains the first 20,000 characters of each source text file \")\n",
    "    out.write(\"alongside the corresponding CSV output for comparison.\\n\\n\")\n",
    "    out.write(\"---\\n\\n\")\n",
    "    \n",
    "    for year in years:\n",
    "        txt_path = os.path.join(txt_dir, f\"Rowell {year}.txt\")\n",
    "        csv_path = os.path.join(csv_dir, f\"Rowell {year}.csv\")\n",
    "        \n",
    "        out.write(f\"# Year: {year}\\n\\n\")\n",
    "        \n",
    "        # Read text file\n",
    "        out.write(f\"## Source Text (first {char_limit:,} characters)\\n\\n\")\n",
    "        out.write(\"```\\n\")\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                txt_content = f.read(char_limit)\n",
    "            out.write(txt_content)\n",
    "        except FileNotFoundError:\n",
    "            out.write(f\"[FILE NOT FOUND: {txt_path}]\")\n",
    "        except Exception as e:\n",
    "            out.write(f\"[ERROR READING FILE: {e}]\")\n",
    "        out.write(\"\\n```\\n\\n\")\n",
    "        \n",
    "        # Read CSV file\n",
    "        out.write(f\"## CSV Output (first {char_limit:,} characters)\\n\\n\")\n",
    "        out.write(\"```csv\\n\")\n",
    "        try:\n",
    "            with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                csv_content = f.read(char_limit)\n",
    "            out.write(csv_content)\n",
    "        except FileNotFoundError:\n",
    "            out.write(f\"[FILE NOT FOUND: {csv_path}]\")\n",
    "        except Exception as e:\n",
    "            out.write(f\"[ERROR READING FILE: {e}]\")\n",
    "        out.write(\"\\n```\\n\\n\")\n",
    "        \n",
    "        out.write(\"---\\n\\n\")\n",
    "\n",
    "print(f\"Comparison file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac257805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the newspaper metadata\n",
    "newspapers = pd.read_csv('final_list.csv')\n",
    "\n",
    "# Load the topic counts JSON\n",
    "with open('topic_counts.json', 'r') as f:\n",
    "    topic_data = json.load(f)\n",
    "\n",
    "# Define the topic categories we're tracking\n",
    "TOPICS = [\n",
    "    'labor_workers', 'politics_elections', 'congress_government',\n",
    "    'business_commerce', 'railroads_transportation', 'agriculture_farming',\n",
    "    'courts_law', 'finance_money', 'immigration_foreign', 'crime_police'\n",
    "]\n",
    "\n",
    "# Build a long-form panel from the JSON\n",
    "records = []\n",
    "for year, papers in topic_data.items():\n",
    "    for paper_name, data in papers.items():\n",
    "        if 'normalized_counts' in data:\n",
    "            record = {\n",
    "                'year': int(year),\n",
    "                'newspapers_all_years_name': paper_name.lower().strip()\n",
    "            }\n",
    "            # Add each topic's normalized share\n",
    "            for topic in TOPICS:\n",
    "                record[topic] = data['normalized_counts'].get(topic, 0.0)\n",
    "            records.append(record)\n",
    "\n",
    "topic_panel = pd.DataFrame(records)\n",
    "\n",
    "# Normalize newspaper names in metadata for matching\n",
    "newspapers['newspapers_all_years_name'] = (\n",
    "    newspapers['newspapers_all_years_name'].str.lower().str.strip()\n",
    ")\n",
    "\n",
    "# Merge topic panel with newspaper metadata\n",
    "panel = topic_panel.merge(\n",
    "    newspapers[['newspapers_all_years_name', 'master_id', 'master_name', 'publisher_change_year']],\n",
    "    on='newspapers_all_years_name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Sort for lag calculation\n",
    "panel = panel.sort_values(['master_id', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Calculate Y_it: sqrt of sum of squared differences in topic shares from t-1 to t\n",
    "def calc_volatility(group):\n",
    "    group = group.sort_values('year').copy()\n",
    "    volatility = []\n",
    "    for i, row in group.iterrows():\n",
    "        if i == group.index[0]:\n",
    "            # First observation for this paper: no prior year\n",
    "            volatility.append(np.nan)\n",
    "        else:\n",
    "            prev_idx = group.index[group.index.get_loc(i) - 1]\n",
    "            prev_row = group.loc[prev_idx]\n",
    "            # Only calculate if years are consecutive\n",
    "            if row['year'] == prev_row['year'] + 1:\n",
    "                sq_diffs = sum((row[t] - prev_row[t])**2 for t in TOPICS)\n",
    "                volatility.append(np.sqrt(sq_diffs))\n",
    "            else:\n",
    "                volatility.append(np.nan)\n",
    "    group['Y_it'] = volatility\n",
    "    return group\n",
    "\n",
    "panel = panel.groupby('master_id', group_keys=False).apply(calc_volatility)\n",
    "\n",
    "# Calculate Time_to_Treat (k): year - publisher_change_year\n",
    "panel['Treat_Year'] = panel['publisher_change_year']\n",
    "panel['Time_to_Treat'] = panel.apply(\n",
    "    lambda r: r['year'] - r['Treat_Year'] if pd.notna(r['Treat_Year']) else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create the final output table\n",
    "output = panel[[\n",
    "    'master_id', 'master_name', 'year', 'Y_it', 'Treat_Year', 'Time_to_Treat'\n",
    "]].rename(columns={\n",
    "    'master_id': 'Newspaper_ID',\n",
    "    'master_name': 'Newspaper_Name',\n",
    "    'year': 'Year',\n",
    "    'Treat_Year': 'Treat_Year',\n",
    "    'Time_to_Treat': 'Time_to_Treat'\n",
    "})\n",
    "\n",
    "# Sort for readability\n",
    "output = output.sort_values(['Newspaper_ID', 'Year']).reset_index(drop=True)\n",
    "\n",
    "# Display sample\n",
    "print(\"Sample of final panel data:\")\n",
    "print(output.head(20).to_string(index=False))\n",
    "print(f\"\\nTotal observations: {len(output)}\")\n",
    "print(f\"Unique newspapers: {output['Newspaper_ID'].nunique()}\")\n",
    "print(f\"Year range: {output['Year'].min()} - {output['Year'].max()}\")\n",
    "print(f\"Treated newspapers: {output[output['Treat_Year'].notna()]['Newspaper_ID'].nunique()}\")\n",
    "\n",
    "# Save to CSV\n",
    "output.to_csv('panel_for_did.csv', index=False)\n",
    "print(\"\\nSaved to 'panel_for_did.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the panel data created by the previous script\n",
    "panel = pd.read_csv('panel_for_did.csv')\n",
    "\n",
    "# Drop observations with missing outcome (first year of each paper)\n",
    "panel = panel.dropna(subset=['Y_it'])\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Create event-time dummies D^k_it\n",
    "# We use k ∈ [-5, +5] as our event window; bin endpoints to avoid collinearity\n",
    "# =============================================================================\n",
    "K_MIN, K_MAX = -5, 5\n",
    "\n",
    "def bin_time_to_treat(k):\n",
    "    \"\"\"Bin extreme values and mark never-treated as separate category.\"\"\"\n",
    "    if pd.isna(k):\n",
    "        return 'never_treated'\n",
    "    elif k < K_MIN:\n",
    "        return f'k_{K_MIN}'  # bin early periods\n",
    "    elif k > K_MAX:\n",
    "        return f'k_{K_MAX}'  # bin late periods\n",
    "    else:\n",
    "        return f'k_{int(k)}'\n",
    "\n",
    "panel['event_time'] = panel['Time_to_Treat'].apply(bin_time_to_treat)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Set reference period (k = -1) for identification\n",
    "# This is standard: we normalize to the year before treatment\n",
    "# =============================================================================\n",
    "panel['event_time'] = pd.Categorical(\n",
    "    panel['event_time'],\n",
    "    categories=['never_treated'] + [f'k_{k}' for k in range(K_MIN, K_MAX + 1)],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: Estimate Two-Way Fixed Effects (TWFE) model\n",
    "# Y_it = α_i + δ_t + Σ β_k D^k_it + ε_it\n",
    "# Using k=-1 as reference (omitted) category\n",
    "# =============================================================================\n",
    "model = smf.ols(\n",
    "    'Y_it ~ C(event_time, Treatment(\"k_-1\")) + C(Newspaper_ID) + C(Year)',\n",
    "    data=panel\n",
    ").fit(cov_type='cluster', cov_kwds={'groups': panel['Newspaper_ID']})\n",
    "\n",
    "print(\"=== TWFE Event Study Results ===\\n\")\n",
    "print(model.summary().tables[1])\n",
    "\n",
    "# =============================================================================\n",
    "# Step 4: Extract β_k coefficients for plotting\n",
    "# =============================================================================\n",
    "coefs = []\n",
    "for k in range(K_MIN, K_MAX + 1):\n",
    "    if k == -1:  # reference period\n",
    "        coefs.append({'k': k, 'beta': 0, 'se': 0, 'ci_lower': 0, 'ci_upper': 0})\n",
    "    else:\n",
    "        param_name = f'C(event_time, Treatment(\"k_-1\"))[T.k_{k}]'\n",
    "        if param_name in model.params:\n",
    "            coefs.append({\n",
    "                'k': k,\n",
    "                'beta': model.params[param_name],\n",
    "                'se': model.bse[param_name],\n",
    "                'ci_lower': model.conf_int().loc[param_name, 0],\n",
    "                'ci_upper': model.conf_int().loc[param_name, 1]\n",
    "            })\n",
    "\n",
    "coef_df = pd.DataFrame(coefs)\n",
    "\n",
    "# =============================================================================\n",
    "# Step 5: Plot the event study graph\n",
    "# Pre-treatment coefficients (k < -1) test parallel trends assumption\n",
    "# Post-treatment coefficients (k >= 0) show causal effect\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.errorbar(coef_df['k'], coef_df['beta'], \n",
    "            yerr=[coef_df['beta'] - coef_df['ci_lower'], \n",
    "                  coef_df['ci_upper'] - coef_df['beta']],\n",
    "            fmt='o', capsize=4, color='steelblue', markersize=8)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.axvline(x=-0.5, color='red', linestyle='--', linewidth=1, label='Treatment')\n",
    "\n",
    "ax.set_xlabel('Years Relative to Publisher Change (k)', fontsize=12)\n",
    "ax.set_ylabel('Effect on Content Volatility (βₖ)', fontsize=12)\n",
    "ax.set_title('Event Study: Publisher Change Effect on Editorial Content', fontsize=14)\n",
    "ax.set_xticks(range(K_MIN, K_MAX + 1))\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('event_study_plot.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Interpretation Guide ===\")\n",
    "print(\"• Pre-trend test: β coefficients for k < -1 should be ≈ 0 (not significant)\")\n",
    "print(\"• Causal effect: β coefficients for k ≥ 0 show treatment impact\")\n",
    "print(\"• Positive β means MORE content volatility after publisher change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfa6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHIVED OLD VERSION OF PUBLISHER CHANGE: \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/master.csv')\n",
    "\n",
    "# Define the years we're tracking\n",
    "years = [1869, 1871, 1872, 1873, 1876, 1877, 1878, 1879, 1880, 1882, 1883, 1884, 1885, 1890]\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    prev_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = prev_row[j + 1] + 1\n",
    "            deletions = curr_row[j] + 1\n",
    "            substitutions = prev_row[j] + (c1 != c2)\n",
    "            curr_row.append(min(insertions, deletions, substitutions))\n",
    "        prev_row = curr_row\n",
    "    return prev_row[-1]\n",
    "\n",
    "def strings_match(s1, s2, max_distance=1):\n",
    "    \"\"\"Check if two strings match within max_distance edits.\"\"\"\n",
    "    s1_clean = s1.strip().lower()\n",
    "    s2_clean = s2.strip().lower()\n",
    "    if s1_clean == s2_clean:\n",
    "        return True\n",
    "    return levenshtein_distance(s1_clean, s2_clean) <= max_distance\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize_publisher(publisher_str):\n",
    "    \"\"\"\n",
    "    Tokenize publisher string by splitting on whitespace, semicolons, and commas.\n",
    "    Keeps periods for names like J.D.\n",
    "    Returns list of tokens that are 4+ characters long.\n",
    "    \"\"\"\n",
    "    if not publisher_str:\n",
    "        return []\n",
    "    # Replace semicolons and commas with spaces, then split on whitespace\n",
    "    cleaned = re.sub(r'[;,]', ' ', publisher_str)\n",
    "    tokens = cleaned.split()\n",
    "    # Only keep tokens 4+ characters long\n",
    "    return [t.strip() for t in tokens if len(t.strip()) >= 4]\n",
    "\n",
    "def publishers_match(pub1, pub2):\n",
    "    \"\"\"\n",
    "    Check if two publisher strings match.\n",
    "    Tokenizes both strings and checks if ANY token (4+ chars) from pub1 \n",
    "    matches ANY token from pub2 (with 1 char tolerance for OCR errors).\n",
    "    \"\"\"\n",
    "    tokens1 = tokenize_publisher(pub1)\n",
    "    tokens2 = tokenize_publisher(pub2)\n",
    "    \n",
    "    if not tokens1 or not tokens2:\n",
    "        return False\n",
    "    \n",
    "    # Check if any token from pub1 matches any token from pub2\n",
    "    for t1 in tokens1:\n",
    "        for t2 in tokens2:\n",
    "            if strings_match(t1, t2, max_distance=1):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def analyze_publisher_changes(row):\n",
    "    \"\"\"\n",
    "    Analyze a newspaper row for publisher changes.\n",
    "    Returns: (category, year_of_first_change)\n",
    "    Categories: 'publisher_change', 'publisher_change_same_editor', 'same_publisher', 'insufficient_data'\n",
    "    \"\"\"\n",
    "    # Collect (year, publisher, editor) tuples where publisher is not empty\n",
    "    data_points = []\n",
    "    for year in years:\n",
    "        pub_col = f'{year} publisher'\n",
    "        ed_col = f'{year} editor'\n",
    "        publisher = row.get(pub_col, '')\n",
    "        editor = row.get(ed_col, '')\n",
    "        \n",
    "        # Convert to string and check if not empty\n",
    "        publisher = str(publisher).strip() if pd.notna(publisher) else ''\n",
    "        editor = str(editor).strip() if pd.notna(editor) else ''\n",
    "        \n",
    "        if publisher and publisher.lower() != 'nan':\n",
    "            data_points.append((year, publisher, editor))\n",
    "    \n",
    "    # Check if we have at least 4 years of data\n",
    "    if len(data_points) < 4:\n",
    "        return ('insufficient_data', None)\n",
    "    \n",
    "    # Check for publisher changes\n",
    "    first_change_year = None\n",
    "    has_publisher_change = False\n",
    "    change_with_same_editor = False\n",
    "    \n",
    "    for i in range(1, len(data_points)):\n",
    "        prev_year, prev_pub, prev_ed = data_points[i-1]\n",
    "        curr_year, curr_pub, curr_ed = data_points[i]\n",
    "        \n",
    "        # Use fuzzy matching with semicolon section handling\n",
    "        if not publishers_match(prev_pub, curr_pub):\n",
    "            has_publisher_change = True\n",
    "            if first_change_year is None:\n",
    "                first_change_year = curr_year\n",
    "                # Check if editor stayed the same during this first change\n",
    "                # Editor must be non-empty in both years to count as \"same editor\"\n",
    "                # Also use fuzzy matching for editors\n",
    "                if prev_ed and curr_ed and strings_match(prev_ed, curr_ed, max_distance=1):\n",
    "                    change_with_same_editor = True\n",
    "    \n",
    "    if not has_publisher_change:\n",
    "        return ('same_publisher', None)\n",
    "    elif change_with_same_editor:\n",
    "        return ('publisher_change_same_editor', first_change_year)\n",
    "    else:\n",
    "        return ('publisher_change', first_change_year)\n",
    "\n",
    "# Apply analysis to each row\n",
    "results = df.apply(analyze_publisher_changes, axis=1)\n",
    "df['category'] = results.apply(lambda x: x[0])\n",
    "df['publisher_change_year'] = results.apply(lambda x: x[1])\n",
    "\n",
    "# Filter out insufficient data\n",
    "valid_df = df[df['category'] != 'insufficient_data'].copy()\n",
    "\n",
    "# Count categories\n",
    "category_counts = valid_df['category'].value_counts()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PUBLISHER CHANGE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal newspapers analyzed: {len(df)}\")\n",
    "print(f\"Newspapers with at least 4 years of data: {len(valid_df)}\")\n",
    "print(f\"Newspapers with insufficient data: {len(df) - len(valid_df)}\")\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"CATEGORY BREAKDOWN:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for cat in ['publisher_change', 'publisher_change_same_editor', 'same_publisher']:\n",
    "    count = category_counts.get(cat, 0)\n",
    "    pct = (count / len(valid_df) * 100) if len(valid_df) > 0 else 0\n",
    "    label = {\n",
    "        'publisher_change': 'Publisher changed (different editor)',\n",
    "        'publisher_change_same_editor': 'Publisher changed (same editor)',\n",
    "        'same_publisher': 'Same publisher throughout'\n",
    "    }[cat]\n",
    "    print(f\"{label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv('data/master.csv', index=False)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Updated master.csv with 'category' and 'publisher_change_year' columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show sample of newspapers with publisher changes\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"SAMPLE: Newspapers with publisher changes\")\n",
    "print(\"-\" * 40)\n",
    "changes_df = valid_df[valid_df['category'].isin(['publisher_change', 'publisher_change_same_editor'])]\n",
    "if len(changes_df) > 0:\n",
    "    sample_cols = ['state', 'town', 'newspaper_name', 'category', 'publisher_change_year']\n",
    "    print(changes_df[sample_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"No publisher changes found.\")\n",
    "\n",
    "# Create lists for each category\n",
    "publisher_change_list = valid_df[valid_df['category'] == 'publisher_change'][['state', 'town', 'newspaper_name', 'publisher_change_year']]\n",
    "publisher_change_same_editor_list = valid_df[valid_df['category'] == 'publisher_change_same_editor'][['state', 'town', 'newspaper_name', 'publisher_change_year']]\n",
    "same_publisher_list = valid_df[valid_df['category'] == 'same_publisher'][['state', 'town', 'newspaper_name']]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DataFrames created:\")\n",
    "print(\"  - publisher_change_list\")\n",
    "print(\"  - publisher_change_same_editor_list\") \n",
    "print(\"  - same_publisher_list\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67861a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 960 newspapers from newspapers_all_years.csv\n",
      "Found 64 entries missing state or town with valid LCCN\n",
      "[22/960] Looking up: savannah morning news (LCCN: sn82015137)\n",
      "  Found town: savannah\n",
      "  Found state: georgia\n",
      "[29/960] Looking up: national republican (LCCN: sn86053573)\n",
      "  Found town: washington\n",
      "  Found state: district of columbia\n",
      "[30/960] Looking up: daily republican (LCCN: sn84038114)\n",
      "  Found town: wilmington\n",
      "[55/960] Looking up: the cincinnati daily star (LCCN: sn85025759)\n",
      "  Found town: cincinnati\n",
      "[85/960] Looking up: the indiana state sentinel (LCCN: sn87056600)\n",
      "  Found town: indianapolis\n",
      "  Found state: indiana\n",
      "[126/960] Looking up: the morning herald (LCCN: sn84038119)\n",
      "  Found town: wilmington\n",
      "  Found state: delaware\n",
      "[154/960] Looking up: the red cloud chief (LCCN: sn84022835)\n",
      "  Found town: red cloud\n",
      "  Found state: nebraska\n",
      "[192/960] Looking up: la crónica (LCCN: sn84025126)\n",
      "  Found town: los angeles\n",
      "  Found state: california\n",
      "[215/960] Looking up: middletown transcript (LCCN: sn84026820)\n",
      "  Found town: middletown\n",
      "  Found state: delaware\n",
      "[367/960] Looking up: el fronterizo (LCCN: sn95070521)\n",
      "  Found town: tucson\n",
      "  Found state: arizona\n",
      "[369/960] Looking up: the placer herald (LCCN: sn82014998)\n",
      "  Found town: rocklin\n",
      "  Found state: california\n",
      "[381/960] Looking up: washington journal (LCCN: sn82014726)\n",
      "  Found town: washington\n",
      "  Found state: district of columbia\n",
      "[400/960] Looking up: arizona weekly enterprise (LCCN: sn94052364)\n",
      "  Found town: tucson\n",
      "  Found state: arizona\n",
      "[408/960] Looking up: the advocate (LCCN: sn85029079)\n",
      "  Found town: meriden\n",
      "  Found state: kansas\n",
      "[433/960] Looking up: the shasta courier (LCCN: sn82015099)\n",
      "  Found town: shasta\n",
      "  Found state: california\n",
      "[436/960] Looking up: the evening world (LCCN: sn83030193)\n",
      "  Found town: new york\n",
      "  Found state: new york\n",
      "[458/960] Looking up: the clifton clarion (LCCN: sn94050557)\n",
      "  Found town: clifton\n",
      "  Found state: arizona\n",
      "[485/960] Looking up: the lincoln county herald (LCCN: sn90061771)\n",
      "  Found town: troy\n",
      "  Found state: missouri\n",
      "[515/960] Looking up: daily tombstone epitaph (LCCN: sn96060682)\n",
      "  Found town: tombstone\n",
      "  Found state: arizona\n",
      "[537/960] Looking up: delta chief (LCCN: sn86066893)\n",
      "  Found town: delta\n",
      "  Found state: colorado\n",
      "[544/960] Looking up: the columbus journal (LCCN: sn95073194)\n",
      "  Found town: columbus\n",
      "  Found state: nebraska\n",
      "[547/960] Looking up: arizona weekly miner (LCCN: sn82014898)\n",
      "  Found town: prescott\n",
      "  Found state: arizona\n",
      "[551/960] Looking up: the springdale news (LCCN: sn83007654)\n",
      "  Found town: springdale\n",
      "  Found state: arkansas\n",
      "[572/960] Looking up: the weekly arizonian (LCCN: sn84024829)\n",
      "  Found town: tucson\n",
      "  Found state: arizona\n",
      "[619/960] Looking up: tombstone epitaph (LCCN: sn95060905)\n",
      "  Found town: tombstone\n",
      "  Found state: arizona\n",
      "[625/960] Looking up: tombstone daily epitaph (LCCN: sn96060681)\n",
      "  Found town: tombstone\n",
      "  Found state: arizona\n",
      "[644/960] Looking up: salt river herald (LCCN: sn87062081)\n",
      "  Found town: phoenix\n",
      "  Found state: arizona\n",
      "[657/960] Looking up: the tombstone (LCCN: sn96060683)\n",
      "  Found town: tombstone\n",
      "  Found state: arizona\n",
      "[664/960] Looking up: west-jersey pioneer (LCCN: sn83032103)\n",
      "  Found town: bridgeton\n",
      "  Found state: new jersey\n",
      "[690/960] Looking up: the nevada picayune (LCCN: sn87091047)\n",
      "  Found town: prescott\n",
      "  Found state: arkansas\n",
      "[698/960] Looking up: the state journal (LCCN: sn87052128)\n",
      "  Found town: jefferson city\n",
      "  Found state: missouri\n",
      "[709/960] Looking up: fair play (LCCN: sn87052181)\n",
      "  Found town: sainte genevieve\n",
      "  Found state: missouri\n",
      "[728/960] Looking up: the enquirer southerner (LCCN: sn92073987)\n",
      "  Error 429 for LCCN sn92073987\n",
      "  No location data found\n",
      "[749/960] Looking up: the southern herald (LCCN: sn87007277)\n",
      "  Error 429 for LCCN sn87007277\n",
      "  No location data found\n",
      "[753/960] Looking up: weekly arizona citizen (LCCN: sn82016240)\n",
      "  Error 429 for LCCN sn82016240\n",
      "  No location data found\n",
      "[762/960] Looking up: the tombstone epitaph (LCCN: sn84021939)\n",
      "  Error 429 for LCCN sn84021939\n",
      "  No location data found\n",
      "[778/960] Looking up: the charlotte democrat (LCCN: sn84020713)\n",
      "  Error 429 for LCCN sn84020713\n",
      "  No location data found\n",
      "[779/960] Looking up: tombstone weekly epitaph (LCCN: sn95060906)\n",
      "  Error 429 for LCCN sn95060906\n",
      "  No location data found\n",
      "[808/960] Looking up: des arc weekly citizen (LCCN: sn84027696)\n",
      "  Error 429 for LCCN sn84027696\n",
      "  No location data found\n",
      "[817/960] Looking up: the weekly roundabout (LCCN: sn86069847)\n",
      "  Error 429 for LCCN sn86069847\n",
      "  No location data found\n",
      "[820/960] Looking up: charlotte home and democrat (LCCN: sn84020714)\n",
      "  Error 429 for LCCN sn84020714\n",
      "  No location data found\n",
      "[831/960] Looking up: the tarboro' southerner (LCCN: sn84026522)\n",
      "  Error 429 for LCCN sn84026522\n",
      "  No location data found\n",
      "[839/960] Looking up: las vegas daily gazette (LCCN: sn90051703)\n",
      "  Error 429 for LCCN sn90051703\n",
      "  No location data found\n",
      "[848/960] Looking up: tolland county press (LCCN: sn84022401)\n",
      "  Error 429 for LCCN sn84022401\n",
      "  No location data found\n",
      "[869/960] Looking up: the troy herald (LCCN: sn86063984)\n",
      "  Error 429 for LCCN sn86063984\n",
      "  No location data found\n",
      "[873/960] Looking up: las vegas morning gazette (LCCN: sn93061631)\n",
      "  Error 429 for LCCN sn93061631\n",
      "  No location data found\n",
      "[877/960] Looking up: richmond democrat (LCCN: sn86063662)\n",
      "  Error 429 for LCCN sn86063662\n",
      "  No location data found\n",
      "[886/960] Looking up: die suedliche post (LCCN: sn91068333)\n",
      "  Error 429 for LCCN sn91068333\n",
      "  No location data found\n",
      "[888/960] Looking up: deming headlight (LCCN: sn83004264)\n",
      "  Error 429 for LCCN sn83004264\n",
      "  No location data found\n",
      "[891/960] Looking up: tri-weekly phoenix (LCCN: sn84027005)\n",
      "  Error 429 for LCCN sn84027005\n",
      "  No location data found\n",
      "[906/960] Looking up: african expositor (LCCN: sn97064613)\n",
      "  Error 429 for LCCN sn97064613\n",
      "  No location data found\n",
      "[907/960] Looking up: water valley progress (LCCN: sn87065501)\n",
      "  Error 429 for LCCN sn87065501\n",
      "  No location data found\n",
      "[909/960] Looking up: the las vegas gazette (LCCN: sn93061633)\n",
      "  Error 429 for LCCN sn93061633\n",
      "  No location data found\n",
      "[921/960] Looking up: delaware straight-out truth teller (LCCN: sn88053093)\n",
      "  Error 429 for LCCN sn88053093\n",
      "  No location data found\n",
      "[925/960] Looking up: the delaware democrat (LCCN: sn88053072)\n",
      "  Error 429 for LCCN sn88053072\n",
      "  No location data found\n",
      "[927/960] Looking up: the new era (LCCN: sn88053061)\n",
      "  Error 429 for LCCN sn88053061\n",
      "  No location data found\n",
      "[930/960] Looking up: the southern eagle (LCCN: sn87065500)\n",
      "  Error 429 for LCCN sn87065500\n",
      "  No location data found\n",
      "[931/960] Looking up: the echo (LCCN: sn83027096)\n",
      "  Error 429 for LCCN sn83027096\n",
      "  No location data found\n",
      "[937/960] Looking up: the weekly examiner (LCCN: sn92051407)\n",
      "  Error 429 for LCCN sn92051407\n",
      "  No location data found\n",
      "[943/960] Looking up: the sunday mirror (LCCN: sn88053051)\n",
      "  Error 429 for LCCN sn88053051\n",
      "  No location data found\n",
      "[947/960] Looking up: the oskaloosa herald (LCCN: sn87058308)\n",
      "  Error 429 for LCCN sn87058308\n",
      "  No location data found\n",
      "[948/960] Looking up: the journal of industry (LCCN: sn92072981)\n",
      "  Error 429 for LCCN sn92072981\n",
      "  No location data found\n",
      "[955/960] Looking up: the tarborough southerner (LCCN: sn85042205)\n",
      "  Error 429 for LCCN sn85042205\n",
      "  No location data found\n",
      "[958/960] Looking up: the county paper (LCCN: sn90061416)\n",
      "  Error 429 for LCCN sn90061416\n",
      "  No location data found\n",
      "\n",
      "Complete!\n",
      "  Updated: 32 entries\n",
      "  Failed lookups: 32 entries\n",
      "  Output saved to: newspapers_all_years_updated.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Newspaper Location Lookup Script\n",
    "Uses the Library of Congress loc.gov API to find state/town information\n",
    "for newspapers missing location data, using LCCN as the lookup key.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = \"newspapers_all_years.csv\"\n",
    "OUTPUT_FILE = \"newspapers_all_years_updated.csv\"\n",
    "RATE_LIMIT_DELAY = 0.5  # seconds between API calls (be nice to LOC servers)\n",
    "\n",
    "def get_newspaper_location(lccn):\n",
    "    \"\"\"\n",
    "    Query the LOC API for newspaper location data using LCCN.\n",
    "    Returns (city, state) tuple or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    if not lccn or lccn.strip() == \"\":\n",
    "        return None, None\n",
    "    \n",
    "    lccn = lccn.strip()\n",
    "    \n",
    "    # Try the loc.gov item endpoint with JSON response\n",
    "    url = f\"https://www.loc.gov/item/{lccn}/?fo=json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract location from the 'item' object\n",
    "            item = data.get('item', {})\n",
    "            \n",
    "            # location_city and location_state are the fields we need\n",
    "            city = item.get('location_city')\n",
    "            state = item.get('location_state')\n",
    "            \n",
    "            # These can be lists or strings, handle both\n",
    "            if isinstance(city, list) and city:\n",
    "                city = city[0]\n",
    "            if isinstance(state, list) and state:\n",
    "                state = state[0]\n",
    "            \n",
    "            # Also check 'location' field which may have combined info\n",
    "            if not city or not state:\n",
    "                location = item.get('location', [])\n",
    "                if location and isinstance(location, list):\n",
    "                    for loc in location:\n",
    "                        if isinstance(loc, str):\n",
    "                            # Format is often \"State--County--City\"\n",
    "                            parts = loc.split('--')\n",
    "                            if len(parts) >= 1 and not state:\n",
    "                                state = parts[0]\n",
    "                            if len(parts) >= 3 and not city:\n",
    "                                city = parts[2]\n",
    "            \n",
    "            return city, state\n",
    "            \n",
    "        elif response.status_code == 404:\n",
    "            print(f\"  LCCN {lccn} not found in LOC database\")\n",
    "            return None, None\n",
    "        else:\n",
    "            print(f\"  Error {response.status_code} for LCCN {lccn}\")\n",
    "            return None, None\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"  Timeout for LCCN {lccn}\")\n",
    "        return None, None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  Request error for LCCN {lccn}: {e}\")\n",
    "        return None, None\n",
    "    except (KeyError, ValueError) as e:\n",
    "        print(f\"  Parse error for LCCN {lccn}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def needs_location_lookup(row):\n",
    "    \"\"\"Check if this row is missing state or town data.\"\"\"\n",
    "    town = row.get('town', '').strip()\n",
    "    state = row.get('state', '').strip()\n",
    "    lccn = row.get('lccn', '').strip()\n",
    "    \n",
    "    # Need lookup if missing town OR state, AND has an LCCN to look up\n",
    "    return (not town or not state) and lccn\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_path = Path(INPUT_FILE)\n",
    "    output_path = Path(OUTPUT_FILE)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        print(f\"Error: Input file '{INPUT_FILE}' not found.\")\n",
    "        print(\"Make sure the file is in the same directory as this script.\")\n",
    "        return\n",
    "    \n",
    "    # Read all rows\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        fieldnames = reader.fieldnames\n",
    "        rows = list(reader)\n",
    "    \n",
    "    print(f\"Loaded {len(rows)} newspapers from {INPUT_FILE}\")\n",
    "    \n",
    "    # Count how many need lookups\n",
    "    needs_lookup = [r for r in rows if needs_location_lookup(r)]\n",
    "    print(f\"Found {len(needs_lookup)} entries missing state or town with valid LCCN\")\n",
    "    \n",
    "    if not needs_lookup:\n",
    "        print(\"No entries need location lookups. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process rows needing lookups\n",
    "    updated_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for i, row in enumerate(rows):\n",
    "        if not needs_location_lookup(row):\n",
    "            continue\n",
    "        \n",
    "        lccn = row['lccn'].strip()\n",
    "        name = row.get('name', 'Unknown')\n",
    "        \n",
    "        print(f\"[{i+1}/{len(rows)}] Looking up: {name} (LCCN: {lccn})\")\n",
    "        \n",
    "        city, state = get_newspaper_location(lccn)\n",
    "        \n",
    "        if city or state:\n",
    "            if not row['town'].strip() and city:\n",
    "                row['town'] = city\n",
    "                print(f\"  Found town: {city}\")\n",
    "            if not row['state'].strip() and state:\n",
    "                row['state'] = state\n",
    "                print(f\"  Found state: {state}\")\n",
    "            updated_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"  No location data found\")\n",
    "        \n",
    "        # Rate limiting - be respectful to LOC servers\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    # Write updated CSV\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    \n",
    "    print(f\"\\nComplete!\")\n",
    "    print(f\"  Updated: {updated_count} entries\")\n",
    "    print(f\"  Failed lookups: {failed_count} entries\")\n",
    "    print(f\"  Output saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c2caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: data/Newspaper Directory Text/Rowell 1869.txt\n",
      "Parsing entries...\n",
      "Found 2298 raw entries\n",
      "Processing: 100/2298 (4.4%) - 0.6s elapsed\n",
      "Processing: 200/2298 (8.7%) - 0.9s elapsed\n",
      "Processing: 300/2298 (13.1%) - 1.2s elapsed\n",
      "Processing: 400/2298 (17.4%) - 1.4s elapsed\n",
      "Processing: 500/2298 (21.8%) - 1.6s elapsed\n",
      "Processing: 600/2298 (26.1%) - 1.7s elapsed\n",
      "Processing: 700/2298 (30.5%) - 1.9s elapsed\n",
      "Processing: 800/2298 (34.8%) - 2.1s elapsed\n",
      "Processing: 900/2298 (39.2%) - 2.2s elapsed\n",
      "Processing: 1000/2298 (43.5%) - 2.3s elapsed\n",
      "Processing: 1100/2298 (47.9%) - 2.5s elapsed\n",
      "Processing: 1200/2298 (52.2%) - 2.7s elapsed\n",
      "Processing: 1300/2298 (56.6%) - 3.2s elapsed\n",
      "Processing: 1400/2298 (60.9%) - 3.3s elapsed\n",
      "Processing: 1500/2298 (65.3%) - 3.5s elapsed\n",
      "Processing: 1600/2298 (69.6%) - 3.7s elapsed\n",
      "Processing: 1700/2298 (74.0%) - 3.8s elapsed\n",
      "Processing: 1800/2298 (78.3%) - 4.0s elapsed\n",
      "Processing: 1900/2298 (82.7%) - 4.1s elapsed\n",
      "Processing: 2000/2298 (87.0%) - 4.2s elapsed\n",
      "Processing: 2100/2298 (91.4%) - 4.4s elapsed\n",
      "Processing: 2200/2298 (95.7%) - 4.5s elapsed\n",
      "Processing: 2298/2298 (100.0%) - 12.9s elapsed\n",
      "\n",
      "==================================================\n",
      "Completed in 16.1 seconds\n",
      "Processed 2298 valid entries\n",
      "Output written to: data/Newspaper Directory Excel/Rowell 1869.csv\n",
      "==================================================\n",
      "Entries with frequency: 1986\n",
      "Entries with political affiliation: 1208\n",
      "Entries with subscription price: 1003\n",
      "Entries with established date: 1280\n",
      "Entries with editor: 1981\n",
      "Entries with publisher: 1990\n",
      "Entries with circulation: 1053\n",
      "Reading file: data/Newspaper Directory Text/Rowell 1871.txt\n",
      "Parsing entries...\n",
      "Found 4795 raw entries\n",
      "Processing: 100/4795 (2.1%) - 0.6s elapsed\n",
      "Processing: 200/4795 (4.2%) - 0.8s elapsed\n",
      "Processing: 300/4795 (6.3%) - 1.0s elapsed\n",
      "Processing: 400/4795 (8.3%) - 1.1s elapsed\n",
      "Processing: 500/4795 (10.4%) - 1.2s elapsed\n",
      "Processing: 600/4795 (12.5%) - 1.3s elapsed\n",
      "Processing: 700/4795 (14.6%) - 1.3s elapsed\n",
      "Processing: 800/4795 (16.7%) - 1.4s elapsed\n",
      "Processing: 900/4795 (18.8%) - 1.5s elapsed\n",
      "Processing: 1000/4795 (20.9%) - 1.6s elapsed\n",
      "Processing: 1100/4795 (22.9%) - 1.7s elapsed\n",
      "Processing: 1200/4795 (25.0%) - 1.8s elapsed\n",
      "Processing: 1300/4795 (27.1%) - 1.9s elapsed\n",
      "Processing: 1400/4795 (29.2%) - 2.0s elapsed\n",
      "Processing: 1500/4795 (31.3%) - 2.0s elapsed\n",
      "Processing: 1600/4795 (33.4%) - 2.1s elapsed\n",
      "Processing: 1700/4795 (35.5%) - 2.2s elapsed\n",
      "Processing: 1800/4795 (37.5%) - 2.3s elapsed\n",
      "Processing: 1900/4795 (39.6%) - 2.5s elapsed\n",
      "Processing: 2000/4795 (41.7%) - 2.6s elapsed\n",
      "Processing: 2100/4795 (43.8%) - 2.7s elapsed\n",
      "Processing: 2200/4795 (45.9%) - 2.8s elapsed\n",
      "Processing: 2300/4795 (48.0%) - 2.9s elapsed\n",
      "Processing: 2400/4795 (50.1%) - 3.0s elapsed\n",
      "Processing: 2500/4795 (52.1%) - 3.1s elapsed\n",
      "Processing: 2600/4795 (54.2%) - 3.2s elapsed\n",
      "Processing: 2700/4795 (56.3%) - 3.3s elapsed\n",
      "Processing: 2800/4795 (58.4%) - 3.5s elapsed\n",
      "Processing: 2900/4795 (60.5%) - 3.9s elapsed\n",
      "Processing: 3000/4795 (62.6%) - 4.0s elapsed\n",
      "Processing: 3100/4795 (64.7%) - 4.1s elapsed\n",
      "Processing: 3200/4795 (66.7%) - 4.2s elapsed\n",
      "Processing: 3300/4795 (68.8%) - 4.2s elapsed\n",
      "Processing: 3400/4795 (70.9%) - 4.3s elapsed\n",
      "Processing: 3500/4795 (73.0%) - 4.4s elapsed\n",
      "Processing: 3600/4795 (75.1%) - 4.5s elapsed\n",
      "Processing: 3700/4795 (77.2%) - 4.6s elapsed\n",
      "Processing: 3800/4795 (79.2%) - 4.7s elapsed\n",
      "Processing: 3900/4795 (81.3%) - 4.8s elapsed\n",
      "Processing: 4000/4795 (83.4%) - 4.9s elapsed\n",
      "Processing: 4100/4795 (85.5%) - 5.1s elapsed\n",
      "Processing: 4200/4795 (87.6%) - 5.2s elapsed\n",
      "Processing: 4300/4795 (89.7%) - 5.4s elapsed\n",
      "Processing: 4400/4795 (91.8%) - 5.4s elapsed\n",
      "Processing: 4500/4795 (93.8%) - 5.6s elapsed\n",
      "Processing: 4600/4795 (95.9%) - 5.7s elapsed\n",
      "Processing: 4700/4795 (98.0%) - 5.8s elapsed\n",
      "Processing: 4795/4795 (100.0%) - 5.9s elapsed\n",
      "\n",
      "==================================================\n",
      "Completed in 5.9 seconds\n",
      "Processed 4795 valid entries\n",
      "Output written to: data/Newspaper Directory Excel/Rowell 1871.csv\n",
      "==================================================\n",
      "Entries with frequency: 4124\n",
      "Entries with political affiliation: 1873\n",
      "Entries with subscription price: 3717\n",
      "Entries with established date: 4343\n",
      "Entries with editor: 4533\n",
      "Entries with publisher: 4609\n",
      "Entries with circulation: 3627\n",
      "Reading file: data/Newspaper Directory Text/Rowell 1872.txt\n",
      "Parsing entries...\n",
      "Found 5069 raw entries\n",
      "Processing: 100/5069 (2.0%) - 0.7s elapsed\n",
      "Processing: 200/5069 (3.9%) - 0.9s elapsed\n",
      "Processing: 300/5069 (5.9%) - 1.1s elapsed\n",
      "Processing: 400/5069 (7.9%) - 1.3s elapsed\n",
      "Processing: 500/5069 (9.9%) - 1.5s elapsed\n",
      "Processing: 600/5069 (11.8%) - 1.6s elapsed\n",
      "Processing: 700/5069 (13.8%) - 1.7s elapsed\n",
      "Processing: 800/5069 (15.8%) - 1.9s elapsed\n",
      "Processing: 900/5069 (17.8%) - 2.0s elapsed\n",
      "Processing: 1000/5069 (19.7%) - 2.2s elapsed\n",
      "Processing: 1100/5069 (21.7%) - 2.3s elapsed\n",
      "Processing: 1200/5069 (23.7%) - 2.6s elapsed\n",
      "Processing: 1300/5069 (25.6%) - 2.8s elapsed\n",
      "Processing: 1400/5069 (27.6%) - 3.0s elapsed\n",
      "Processing: 1500/5069 (29.6%) - 3.2s elapsed\n",
      "Processing: 1600/5069 (31.6%) - 3.4s elapsed\n",
      "Processing: 1700/5069 (33.5%) - 3.6s elapsed\n",
      "Processing: 1800/5069 (35.5%) - 3.7s elapsed\n",
      "Processing: 1900/5069 (37.5%) - 3.9s elapsed\n",
      "Processing: 2000/5069 (39.5%) - 4.1s elapsed\n",
      "Processing: 2100/5069 (41.4%) - 4.3s elapsed\n",
      "Processing: 2200/5069 (43.4%) - 4.4s elapsed\n",
      "Processing: 2300/5069 (45.4%) - 4.6s elapsed\n",
      "Processing: 2400/5069 (47.3%) - 4.8s elapsed\n",
      "Processing: 2500/5069 (49.3%) - 5.0s elapsed\n",
      "Processing: 2600/5069 (51.3%) - 5.2s elapsed\n",
      "Processing: 2700/5069 (53.3%) - 5.4s elapsed\n",
      "Processing: 2800/5069 (55.2%) - 5.6s elapsed\n",
      "Processing: 2900/5069 (57.2%) - 5.8s elapsed\n",
      "Processing: 3000/5069 (59.2%) - 6.2s elapsed\n",
      "Processing: 3100/5069 (61.2%) - 6.9s elapsed\n",
      "Processing: 3200/5069 (63.1%) - 7.1s elapsed\n",
      "Processing: 3300/5069 (65.1%) - 7.3s elapsed\n",
      "Processing: 3400/5069 (67.1%) - 7.4s elapsed\n",
      "Processing: 3500/5069 (69.0%) - 7.5s elapsed\n",
      "Processing: 3600/5069 (71.0%) - 7.7s elapsed\n",
      "Processing: 3700/5069 (73.0%) - 7.9s elapsed\n",
      "Processing: 3800/5069 (75.0%) - 8.1s elapsed\n",
      "Processing: 3900/5069 (76.9%) - 8.3s elapsed\n",
      "Processing: 4000/5069 (78.9%) - 8.5s elapsed\n",
      "Processing: 4100/5069 (80.9%) - 8.8s elapsed\n",
      "Processing: 4200/5069 (82.9%) - 8.9s elapsed\n",
      "Processing: 4300/5069 (84.8%) - 9.1s elapsed\n",
      "Processing: 4400/5069 (86.8%) - 9.3s elapsed\n",
      "Processing: 4500/5069 (88.8%) - 9.5s elapsed\n",
      "Processing: 4600/5069 (90.7%) - 9.6s elapsed\n",
      "Processing: 4700/5069 (92.7%) - 9.8s elapsed\n",
      "Processing: 4800/5069 (94.7%) - 10.0s elapsed\n",
      "Processing: 4900/5069 (96.7%) - 10.1s elapsed\n",
      "Processing: 5000/5069 (98.6%) - 10.3s elapsed\n",
      "Processing: 5069/5069 (100.0%) - 10.4s elapsed\n",
      "\n",
      "==================================================\n",
      "Completed in 10.4 seconds\n",
      "Processed 5069 valid entries\n",
      "Output written to: data/Newspaper Directory Excel/Rowell 1872.csv\n",
      "==================================================\n",
      "Entries with frequency: 4342\n",
      "Entries with political affiliation: 2008\n",
      "Entries with subscription price: 3985\n",
      "Entries with established date: 4558\n",
      "Entries with editor: 4745\n",
      "Entries with publisher: 4845\n",
      "Entries with circulation: 3804\n",
      "Reading file: data/Newspaper Directory Text/Rowell 1873.txt\n",
      "Parsing entries...\n",
      "Found 5253 raw entries\n",
      "Processing: 100/5253 (1.9%) - 0.5s elapsed\n",
      "Processing: 200/5253 (3.8%) - 0.8s elapsed\n",
      "Processing: 300/5253 (5.7%) - 1.1s elapsed\n",
      "Processing: 400/5253 (7.6%) - 1.2s elapsed\n",
      "Processing: 500/5253 (9.5%) - 1.4s elapsed\n",
      "Processing: 600/5253 (11.4%) - 1.5s elapsed\n",
      "Processing: 700/5253 (13.3%) - 1.7s elapsed\n",
      "Processing: 800/5253 (15.2%) - 1.8s elapsed\n",
      "Processing: 900/5253 (17.1%) - 2.0s elapsed\n",
      "Processing: 1000/5253 (19.0%) - 2.1s elapsed\n",
      "Processing: 1100/5253 (20.9%) - 2.2s elapsed\n",
      "Processing: 1200/5253 (22.8%) - 2.4s elapsed\n",
      "Processing: 1300/5253 (24.7%) - 2.6s elapsed\n",
      "Processing: 1400/5253 (26.7%) - 2.7s elapsed\n",
      "Processing: 1500/5253 (28.6%) - 2.8s elapsed\n",
      "Processing: 1600/5253 (30.5%) - 3.0s elapsed\n",
      "Processing: 1700/5253 (32.4%) - 3.1s elapsed\n",
      "Processing: 1800/5253 (34.3%) - 3.3s elapsed\n",
      "Processing: 1900/5253 (36.2%) - 3.5s elapsed\n",
      "Processing: 2000/5253 (38.1%) - 3.6s elapsed\n",
      "Processing: 2100/5253 (40.0%) - 3.8s elapsed\n",
      "Processing: 2200/5253 (41.9%) - 4.0s elapsed\n",
      "Processing: 2300/5253 (43.8%) - 4.2s elapsed\n",
      "Processing: 2400/5253 (45.7%) - 4.3s elapsed\n",
      "Processing: 2500/5253 (47.6%) - 4.5s elapsed\n",
      "Processing: 2600/5253 (49.5%) - 4.7s elapsed\n",
      "Processing: 2700/5253 (51.4%) - 4.9s elapsed\n",
      "Processing: 2800/5253 (53.3%) - 5.1s elapsed\n",
      "Processing: 2900/5253 (55.2%) - 5.3s elapsed\n",
      "Processing: 3000/5253 (57.1%) - 5.5s elapsed\n",
      "Processing: 3100/5253 (59.0%) - 6.7s elapsed\n",
      "Processing: 3200/5253 (60.9%) - 6.8s elapsed\n",
      "Processing: 3300/5253 (62.8%) - 7.0s elapsed\n",
      "Processing: 3400/5253 (64.7%) - 7.3s elapsed\n",
      "Processing: 3500/5253 (66.6%) - 7.4s elapsed\n",
      "Processing: 3600/5253 (68.5%) - 7.6s elapsed\n",
      "Processing: 3700/5253 (70.4%) - 7.8s elapsed\n",
      "Processing: 3800/5253 (72.3%) - 7.9s elapsed\n",
      "Processing: 3900/5253 (74.2%) - 8.1s elapsed\n",
      "Processing: 4000/5253 (76.1%) - 8.3s elapsed\n",
      "Processing: 4100/5253 (78.1%) - 8.5s elapsed\n",
      "Processing: 4200/5253 (80.0%) - 8.7s elapsed\n",
      "Processing: 4300/5253 (81.9%) - 8.8s elapsed\n",
      "Processing: 4400/5253 (83.8%) - 9.0s elapsed\n",
      "Processing: 4500/5253 (85.7%) - 9.1s elapsed\n",
      "Processing: 4600/5253 (87.6%) - 9.2s elapsed\n",
      "Processing: 4700/5253 (89.5%) - 9.4s elapsed\n",
      "Processing: 4800/5253 (91.4%) - 9.6s elapsed\n",
      "Processing: 4900/5253 (93.3%) - 9.8s elapsed\n",
      "Processing: 5000/5253 (95.2%) - 10.1s elapsed\n",
      "Processing: 5100/5253 (97.1%) - 10.2s elapsed\n",
      "Processing: 5200/5253 (99.0%) - 10.3s elapsed\n",
      "Processing: 5253/5253 (100.0%) - 10.4s elapsed\n",
      "\n",
      "==================================================\n",
      "Completed in 10.5 seconds\n",
      "Processed 5253 valid entries\n",
      "Output written to: data/Newspaper Directory Excel/Rowell 1873.csv\n",
      "==================================================\n",
      "Entries with frequency: 4529\n",
      "Entries with political affiliation: 1908\n",
      "Entries with subscription price: 4208\n",
      "Entries with established date: 4665\n",
      "Entries with editor: 4883\n",
      "Entries with publisher: 4969\n",
      "Entries with circulation: 3870\n",
      "Reading file: data/Newspaper Directory Text/Rowell 1876.txt\n",
      "Parsing entries...\n",
      "Found 3013 raw entries\n",
      "Processing: 100/3013 (3.3%) - 0.7s elapsed\n",
      "Processing: 200/3013 (6.6%) - 1.2s elapsed\n",
      "Processing: 300/3013 (10.0%) - 1.5s elapsed\n",
      "Processing: 400/3013 (13.3%) - 1.8s elapsed\n",
      "Processing: 500/3013 (16.6%) - 2.1s elapsed\n",
      "Processing: 600/3013 (19.9%) - 2.3s elapsed\n",
      "Processing: 700/3013 (23.2%) - 2.8s elapsed\n",
      "Processing: 800/3013 (26.6%) - 3.1s elapsed\n",
      "Processing: 900/3013 (29.9%) - 3.4s elapsed\n",
      "Processing: 1000/3013 (33.2%) - 3.7s elapsed\n",
      "Processing: 1100/3013 (36.5%) - 4.1s elapsed\n",
      "Processing: 1200/3013 (39.8%) - 4.4s elapsed\n",
      "Processing: 1300/3013 (43.1%) - 4.6s elapsed\n",
      "Processing: 1400/3013 (46.5%) - 4.9s elapsed\n",
      "Processing: 1500/3013 (49.8%) - 5.2s elapsed\n",
      "Processing: 1600/3013 (53.1%) - 5.5s elapsed\n",
      "Processing: 1700/3013 (56.4%) - 5.8s elapsed\n",
      "Processing: 1800/3013 (59.7%) - 6.1s elapsed\n",
      "Processing: 1900/3013 (63.1%) - 7.1s elapsed\n",
      "Processing: 2000/3013 (66.4%) - 7.4s elapsed\n",
      "Processing: 2100/3013 (69.7%) - 7.6s elapsed\n",
      "Processing: 2200/3013 (73.0%) - 7.9s elapsed\n",
      "Processing: 2300/3013 (76.3%) - 8.2s elapsed\n",
      "Processing: 2400/3013 (79.7%) - 8.6s elapsed\n",
      "Processing: 2500/3013 (83.0%) - 8.9s elapsed\n",
      "Processing: 2600/3013 (86.3%) - 9.1s elapsed\n",
      "Processing: 2700/3013 (89.6%) - 9.5s elapsed\n",
      "Processing: 2800/3013 (92.9%) - 9.9s elapsed\n",
      "Processing: 2900/3013 (96.2%) - 10.3s elapsed\n",
      "Processing: 3000/3013 (99.6%) - 10.6s elapsed\n",
      "Processing: 3013/3013 (100.0%) - 10.6s elapsed\n",
      "\n",
      "==================================================\n",
      "Completed in 10.7 seconds\n",
      "Processed 3013 valid entries\n",
      "Output written to: data/Newspaper Directory Excel/Rowell 1876.csv\n",
      "==================================================\n",
      "Entries with frequency: 2717\n",
      "Entries with political affiliation: 1403\n",
      "Entries with subscription price: 2602\n",
      "Entries with established date: 2726\n",
      "Entries with editor: 2785\n",
      "Entries with publisher: 2805\n",
      "Entries with circulation: 2100\n"
     ]
    }
   ],
   "source": [
    "# possibly improved pre 1877 data extraction\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Known multi-word town names that appear in the directories\n",
    "# This list should be expanded as more are discovered\n",
    "MULTI_WORD_TOWNS = {\n",
    "    # Two-word towns\n",
    "    'DES ARC', 'FORT SMITH', 'UNION SPRINGS', 'BLADEN SPRINGS', \n",
    "    'LITTLE ROCK', 'PINE BLUFF', 'HOT SPRINGS', 'VAN BUREN',\n",
    "    'LAKE VILLAGE', 'DE WITT', 'ARKANSAS CITY', 'FAYETTE C. H.',\n",
    "    'GROVE HILL', 'LA FAYETTE', 'NEW ORLEANS', 'BATON ROUGE',\n",
    "    'PORT GIBSON', 'PASS CHRISTIAN', 'BAY ST. LOUIS', 'MOSS POINT',\n",
    "    'WEST POINT', 'HOLLY SPRINGS', 'WATER VALLEY', 'YAZOO CITY',\n",
    "    'VICKSBURG', 'JACKSON', 'MERIDIAN',\n",
    "    'CARROLLTON',\n",
    "    # Three-word towns\n",
    "    'DEVALL\\'S BLUFF', 'DEVALLS BLUFF',\n",
    "    # Common suffixes that indicate multi-word towns\n",
    "    'COURT HOUSE', 'C. H.',\n",
    "}\n",
    "\n",
    "# Pattern to match town names that end with common suffixes\n",
    "TOWN_SUFFIX_PATTERNS = [\n",
    "    r'C\\.\\s*H\\.?',      # Court House abbreviation\n",
    "    r'SPRINGS?',         # Springs\n",
    "    r'CITY',            # City\n",
    "    r'BLUFF',           # Bluff\n",
    "    r'ROCK',            # Rock\n",
    "    r'HILL',            # Hill\n",
    "    r'POINT',           # Point\n",
    "    r'VILLAGE',         # Village\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean OCR artifacts and normalize text.\"\"\"\n",
    "    replacements = {\n",
    "        'Î': 'A', 'Î•': 'E', 'Îœ': 'M', 'Î': 'N', 'Ð¡': 'C', 'Ð¢': 'T',\n",
    "        'Ã‰': 'E', 'Ñ': 'c', 'Ðµ': 'e', 'Ñ€': 'p', 'Ð': 'N',\n",
    "        '`': \"'\", \"'\": \"'\", '\"': '\"', '\"': '\"',\n",
    "        '\\xad': '', '­': '',\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_town_name(town):\n",
    "    \"\"\"\n",
    "    Normalize town name - fix common OCR errors and standardize format.\n",
    "    \"\"\"\n",
    "    if not town:\n",
    "        return town\n",
    "    \n",
    "    # Fix common OCR errors in town names\n",
    "    ocr_fixes = {\n",
    "        'CAR LLTON': 'CARROLLTON',\n",
    "        'CARLLTON': 'CARROLLTON', \n",
    "        'FRORENCE': 'FLORENCE',\n",
    "        'TUSHALOOSA': 'TUSCALOOSA',\n",
    "        'TUSHEGEE': 'TUSKEGEE',\n",
    "        'SCOTSBORO': 'SCOTTSBORO',\n",
    "        'MONROEVILLÃ‰': 'MONROEVILLE',\n",
    "    }\n",
    "    \n",
    "    town_upper = town.upper().strip()\n",
    "    if town_upper in ocr_fixes:\n",
    "        return ocr_fixes[town_upper]\n",
    "    \n",
    "    return town.strip()\n",
    "\n",
    "\n",
    "def normalize_for_matching(text):\n",
    "    \"\"\"Normalize text for pattern matching - removes extra spaces.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "def normalize_editor_publisher_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text specifically for editor/publisher extraction.\n",
    "    Handles OCR artifacts like hyphenated line breaks and missing spaces.\n",
    "    \"\"\"\n",
    "    normalized = text\n",
    "    \n",
    "    # Remove hyphenated line breaks (e.g., \"edi- tors\" -> \"editors\", \"pub- lisher\" -> \"publisher\")\n",
    "    normalized = re.sub(r'-\\s+', '', normalized)\n",
    "    \n",
    "    # Fix common OCR run-together patterns\n",
    "    normalized = re.sub(r'(editors?)(and)(pub)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(editors?)(and)(prop)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(publishers?)(and)(prop)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(publishers?)(and)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(proprietors?)(and)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(publishers?)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(proprietors?)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalize multiple spaces\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "def extract_circulation(text):\n",
    "    \"\"\"Extract circulation number from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'circulation[:\\s]+(?:about\\s+)?(\\d[\\d,\\.]+)',\n",
    "        r'claims?\\s+(?:about\\s+)?(\\d[\\d,\\.]+)\\s+circulation',\n",
    "        r'circ(?:ulation|\\'?l?n)[:\\s\\.]+(?:about\\s+)?(\\d[\\d,\\.]+)',\n",
    "        r'(\\d[\\d,\\.]+)\\s+circ(?:ulation|\\'?l?n)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '').replace('.', '')\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_political_affiliation(text):\n",
    "    \"\"\"Extract political affiliation from text.\"\"\"\n",
    "    affiliations = ['democratic', 'republican', 'independent', 'neutral',\n",
    "                    'whig', 'conservative', 'liberal', 'radical']\n",
    "    text_lower = text.lower()\n",
    "    for affiliation in affiliations:\n",
    "        if re.search(rf';\\s*{affiliation}\\b', text_lower):\n",
    "            return affiliation.capitalize()\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_subscription_details(text):\n",
    "    \"\"\"Extract detailed subscription info.\"\"\"\n",
    "    daily_match = re.search(r'subscription[-\\s]+daily\\s+\\$(\\d+(?:\\s+\\d{2})?)', text, re.IGNORECASE)\n",
    "    if daily_match:\n",
    "        return f\"${daily_match.group(1).replace(' ', '.')}\"\n",
    "    \n",
    "    weekly_match = re.search(r'(?:subscription[-\\s]+)?weekly\\s+\\$(\\d+(?:\\s+\\d{2})?)', text, re.IGNORECASE)\n",
    "    if weekly_match:\n",
    "        return f\"${weekly_match.group(1).replace(' ', '.')}\"\n",
    "    \n",
    "    std_match = re.search(r'subscription[:\\s]+\\$(\\d+(?:\\s+\\d{2})?)', text, re.IGNORECASE)\n",
    "    if std_match:\n",
    "        return f\"${std_match.group(1).replace(' ', '.')}\"\n",
    "    \n",
    "    cents_match = re.search(r'subscription\\s+(\\d+)\\s+cents', text, re.IGNORECASE)\n",
    "    if cents_match:\n",
    "        return f\"${int(cents_match.group(1))/100:.2f}\"\n",
    "    \n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_frequency(text):\n",
    "    \"\"\"Extract publication frequency from text.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if re.search(r'every\\s+(?:morning|evening|day)', text_lower):\n",
    "        return 'Daily & Weekly' if 'and weekly' in text_lower or 'weekly,' in text_lower else 'Daily'\n",
    "    if re.search(r'tri-?weekly', text_lower):\n",
    "        return 'Tri-weekly & Weekly' if 'and weekly' in text_lower else 'Tri-weekly'\n",
    "    if re.search(r'semi-?weekly', text_lower):\n",
    "        return 'Semi-weekly & Weekly' if 'and weekly' in text_lower else 'Semi-weekly'\n",
    "    if re.search(r'semi-?monthly', text_lower):\n",
    "        return 'Semi-monthly'\n",
    "    if 'quarterly' in text_lower:\n",
    "        return 'Quarterly'\n",
    "    if 'monthly' in text_lower:\n",
    "        return 'Monthly'\n",
    "    \n",
    "    days = ['sundays', 'mondays', 'tuesdays', 'wednesdays', 'thursdays', 'fridays', 'saturdays']\n",
    "    for day in days:\n",
    "        if day in text_lower:\n",
    "            return 'Weekly'\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_established(text):\n",
    "    \"\"\"Extract establishment year from text.\"\"\"\n",
    "    patterns = [\n",
    "        r'establish[e]?d\\s+(\\d{4})',\n",
    "        r'estab[-\\s]*lished\\s+(\\d{4})',\n",
    "        r'es[-\\s]*tablished\\s+(\\d{4})',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            if 1700 <= int(year) <= 1900:\n",
    "                return year\n",
    "    return ''\n",
    "\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"Clean and validate an extracted name.\"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    name = name.strip().strip(',;:.')\n",
    "    \n",
    "    if len(name) < 3:\n",
    "        return None\n",
    "    \n",
    "    false_positives = ['Four', 'Eight', 'The', 'And', 'Weekly', 'Daily', 'Semi', \n",
    "                       'Tri', 'Monthly', 'Sunday', 'Saturday', 'Friday', 'Thursday',\n",
    "                       'Wednesday', 'Tuesday', 'Monday', 'About', 'Claims', 'Size',\n",
    "                       'Subscription', 'Established', 'Circulation', 'Pages',\n",
    "                       'Democratic', 'Republican', 'Independent', 'Neutral',\n",
    "                       'Temperance', 'Association']\n",
    "    if name in false_positives:\n",
    "        return None\n",
    "    \n",
    "    if name.replace(',', '').replace('.', '').isdigit():\n",
    "        return None\n",
    "    \n",
    "    if re.search(r'\\b(editor|publisher|proprietor|and)\\s*$', name, re.IGNORECASE):\n",
    "        return None\n",
    "    \n",
    "    if name[0].islower():\n",
    "        return None\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "def add_name_if_unique(name, name_list):\n",
    "    \"\"\"Add a name to the list if it's not a duplicate.\"\"\"\n",
    "    cleaned = clean_name(name)\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    \n",
    "    cleaned_lower = cleaned.lower()\n",
    "    \n",
    "    for existing in name_list:\n",
    "        if cleaned_lower == existing.lower() or cleaned_lower in existing.lower():\n",
    "            return False\n",
    "    \n",
    "    to_remove = [e for e in name_list if e.lower() in cleaned_lower]\n",
    "    for item in to_remove:\n",
    "        name_list.remove(item)\n",
    "    \n",
    "    name_list.append(cleaned)\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_editor_publisher(text):\n",
    "    \"\"\"Extract editor and publisher names from text.\"\"\"\n",
    "    editors, publishers = [], []\n",
    "    \n",
    "    normalized = normalize_editor_publisher_text(text)\n",
    "    normalized = normalize_for_matching(normalized)\n",
    "    \n",
    "    normalized = re.sub(r'(\\w)(and)(\\w)', r'\\1 \\2 \\3', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(editor[s]?)(and)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(pub)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    normalized = re.sub(r'(and)(prop)', r'\\1 \\2', normalized, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Split by semicolons to process each segment separately\n",
    "    segments = re.split(r';', normalized)\n",
    "    \n",
    "    for segment in segments:\n",
    "        segment = segment.strip()\n",
    "        if not segment:\n",
    "            continue\n",
    "        \n",
    "        name_pattern = r'([A-Z][A-Za-z\\.\\s&,]+?)'\n",
    "        \n",
    "        combined_patterns = [\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s+and\\s+publishers?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s+and\\s+proprietors?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?andpublishers?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?andproprietors?', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        editor_patterns = [\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s*$', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editors?\\s*,', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+editor-in-chief', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        publisher_patterns = [\n",
    "            re.compile(name_pattern + r',?\\s+publishers?\\s*$', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+proprietors?\\s*$', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+publishers?\\s+and\\s+proprietors?', re.IGNORECASE),\n",
    "            re.compile(name_pattern + r',?\\s+publishersandproprietors?', re.IGNORECASE),\n",
    "        ]\n",
    "        \n",
    "        matched_combined = False\n",
    "        for pattern in combined_patterns:\n",
    "            match = pattern.search(segment)\n",
    "            if match:\n",
    "                add_name_if_unique(match.group(1), editors)\n",
    "                add_name_if_unique(match.group(1), publishers)\n",
    "                matched_combined = True\n",
    "                break\n",
    "        \n",
    "        if matched_combined:\n",
    "            continue\n",
    "        \n",
    "        for pattern in editor_patterns:\n",
    "            match = pattern.search(segment)\n",
    "            if match:\n",
    "                add_name_if_unique(match.group(1), editors)\n",
    "                break\n",
    "        \n",
    "        for pattern in publisher_patterns:\n",
    "            match = pattern.search(segment)\n",
    "            if match:\n",
    "                add_name_if_unique(match.group(1), publishers)\n",
    "                break\n",
    "    \n",
    "    return {'editor': '; '.join(editors), 'publisher': '; '.join(publishers)}\n",
    "\n",
    "\n",
    "def is_valid_town_name(town):\n",
    "    \"\"\"Check if the extracted town name is a valid town (not an index/header entry).\"\"\"\n",
    "    invalid_patterns = [\n",
    "        r'^A\\s+LIST', r'DOMINION', r'CANADA', r'BRITISH', r'COLONIES',\n",
    "        r'UNITED\\s+STATES', r'TERRITORIES', r'^INDEX', r'^PAGE\\s*\\d*',\n",
    "        r'NEWSPAPERS?', r'PERIODICALS?', r'ALPHABETICALLY', r'ARRANGED',\n",
    "        r'GIVING\\s+NAME', r'DAYS\\s+OF\\s+ISSUE', r'SUBSCRIPTION\\s+PRICE',\n",
    "        r'EDITOR.?S?\\s+AND\\s+PUBLISHER', r'CIRCULATION', r'ADVERTISEMENTS?',\n",
    "        r'PRINTING\\s+MATERIAL', r'IN\\s+WHICH', r'ARE\\s+PUBLISHED',\n",
    "        r'^NOTE', r'^\\d+$', r'^THE\\s+', r'ALIST\\s+OF',\n",
    "    ]\n",
    "    \n",
    "    town_upper = town.upper().strip()\n",
    "    for pattern in invalid_patterns:\n",
    "        if re.search(pattern, town_upper):\n",
    "            return False\n",
    "    \n",
    "    if len(town) > 50 or len(town.split()) > 4:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def is_valid_entry_text(entry_text):\n",
    "    \"\"\"Check if the entry text looks like a valid newspaper entry.\"\"\"\n",
    "    invalid_patterns = [\n",
    "        r'ALPHABETICALLY\\s+BY\\s+TOWNS', r'DAYS\\s+OF\\s+ISSUE',\n",
    "        r'POLITICS\\s+OR\\s+GENERAL\\s+CHARACTER', r'DATE\\s+OF\\s+ESTABLISHMENT',\n",
    "        r'EDITOR.?S?\\s+AND\\s+PUBLISHER.?S?\\s+NAMES', r'GIV-?\\s*ING\\s+NAME',\n",
    "        r'DOMINION\\s+OF\\s+CANADA', r'BRITISH\\s+COLONIES',\n",
    "        r'UNITED\\s+STATES\\s+AND\\s+TERRITORIES',\n",
    "        r'A\\s*LIST\\s+OF\\s+THE\\s+NEWSPAPERS',\n",
    "    ]\n",
    "    \n",
    "    text_upper = entry_text.upper()\n",
    "    for pattern in invalid_patterns:\n",
    "        if re.search(pattern, text_upper):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_town_and_newspaper(text):\n",
    "    \"\"\"\n",
    "    Extract town name and newspaper name from the beginning of an entry.\n",
    "    Handles multi-word town names properly.\n",
    "    \n",
    "    Returns: (town, newspaper_name, remainder) or (None, None, text) if no match\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return None, None, text\n",
    "    \n",
    "    # First, check for known multi-word towns at the start\n",
    "    text_upper = text.upper()\n",
    "    \n",
    "    for known_town in sorted(MULTI_WORD_TOWNS, key=len, reverse=True):\n",
    "        # Check if text starts with this known town\n",
    "        if text_upper.startswith(known_town):\n",
    "            # Verify it's followed by appropriate delimiter and newspaper name\n",
    "            remainder = text[len(known_town):].lstrip(' ,')\n",
    "            \n",
    "            # Extract newspaper name (up to first semicolon or colon)\n",
    "            paper_match = re.match(r'^([A-Za-z][A-Za-z\\s&\\'\\.\\-]+?)\\s*[;:](.*)$', remainder, re.DOTALL)\n",
    "            if paper_match:\n",
    "                newspaper = paper_match.group(1).strip().rstrip(',')\n",
    "                rest = paper_match.group(2)\n",
    "                return normalize_town_name(known_town), newspaper, rest\n",
    "    \n",
    "    # Check for town + suffix patterns (e.g., \"FAYETTE C. H.\")\n",
    "    for suffix_pattern in TOWN_SUFFIX_PATTERNS:\n",
    "        pattern = rf'^([A-Z][A-Z]+)\\s+({suffix_pattern})[,\\s]+([A-Za-z][A-Za-z\\s&\\'\\.\\-]+?)\\s*[;:](.*)$'\n",
    "        match = re.match(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            town = f\"{match.group(1)} {match.group(2)}\"\n",
    "            newspaper = match.group(3).strip().rstrip(',')\n",
    "            rest = match.group(4)\n",
    "            return normalize_town_name(town), newspaper, rest\n",
    "    \n",
    "    # Standard single-word town pattern\n",
    "    # Town is ALL CAPS, newspaper starts with capital\n",
    "    pattern = r'^([A-Z][A-Z]+)[,\\s]+([A-Za-z][A-Za-z\\s&\\'\\.\\-]+?)\\s*[;:](.*)$'\n",
    "    match = re.match(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        town = match.group(1).strip()\n",
    "        newspaper = match.group(2).strip().rstrip(',')\n",
    "        rest = match.group(3)\n",
    "        return normalize_town_name(town), newspaper, rest\n",
    "    \n",
    "    return None, None, text\n",
    "\n",
    "\n",
    "def find_entry_boundaries(text):\n",
    "    \"\"\"\n",
    "    Find all entry start positions in the text.\n",
    "    An entry starts with: TOWNNAME, Newspaper Name;\n",
    "    \n",
    "    Returns list of (start_pos, town, newspaper_name) tuples\n",
    "    \"\"\"\n",
    "    boundaries = []\n",
    "    \n",
    "    # Build a combined pattern for known multi-word towns + single-word towns\n",
    "    multi_word_pattern = '|'.join(re.escape(t) for t in sorted(MULTI_WORD_TOWNS, key=len, reverse=True))\n",
    "    single_word_pattern = r'[A-Z]{2,}'\n",
    "    \n",
    "    # Combined pattern - note we're looking for these patterns preceded by \n",
    "    # sentence-ending punctuation or start of text\n",
    "    entry_pattern = re.compile(\n",
    "        rf'(?:^|[\\.;])\\s*'  # Start or after sentence end\n",
    "        rf'((?:{multi_word_pattern})|{single_word_pattern})'  # Town name\n",
    "        rf'[,\\s]+'  # Separator\n",
    "        rf'([A-Z][a-zA-Z][a-zA-Z\\s&\\'\\.\\-]*?)'  # Newspaper name  \n",
    "        rf'\\s*;',  # Semicolon\n",
    "        re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    for match in entry_pattern.finditer(text):\n",
    "        town = match.group(1).strip()\n",
    "        newspaper = match.group(2).strip().rstrip(',')\n",
    "        \n",
    "        # Validate this looks like a real entry\n",
    "        if is_valid_town_name(town) and len(newspaper) > 1:\n",
    "            boundaries.append((match.start(), town, newspaper))\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "\n",
    "def parse_newspaper_entries_v2(text):\n",
    "    \"\"\"\n",
    "    Improved parsing that handles:\n",
    "    1. Multiple entries per line\n",
    "    2. Multi-word town names\n",
    "    3. Better entry boundary detection\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Normalize whitespace but preserve some structure\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize multiple spaces\n",
    "    \n",
    "    entries = []\n",
    "    \n",
    "    # Find all entry boundaries\n",
    "    boundaries = find_entry_boundaries(text)\n",
    "    \n",
    "    if not boundaries:\n",
    "        # Fall back to original line-by-line parsing if no boundaries found\n",
    "        return []\n",
    "    \n",
    "    # Extract each entry using the boundaries\n",
    "    for i, (start_pos, town, newspaper) in enumerate(boundaries):\n",
    "        # Find the end of this entry (start of next entry or end of text)\n",
    "        if i + 1 < len(boundaries):\n",
    "            end_pos = boundaries[i + 1][0]\n",
    "        else:\n",
    "            end_pos = len(text)\n",
    "        \n",
    "        # Extract the full entry text\n",
    "        entry_text = text[start_pos:end_pos].strip()\n",
    "        \n",
    "        # Clean up leading punctuation from previous entry\n",
    "        entry_text = re.sub(r'^[\\.;]\\s*', '', entry_text)\n",
    "        \n",
    "        if entry_text and is_valid_entry_text(entry_text):\n",
    "            entries.append((normalize_town_name(town), entry_text))\n",
    "    \n",
    "    return entries\n",
    "\n",
    "\n",
    "def parse_newspaper_entries_fallback(text):\n",
    "    \"\"\"\n",
    "    Fallback line-based parsing for when v2 parser doesn't find entries.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    lines = text.split('\\n')\n",
    "    entries = []\n",
    "    current_entry = []\n",
    "    current_town = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        new_entry_match = re.match(\n",
    "            r'^([A-Z][A-Z\\s,\\.]+?)(?:,\\s*|\\s+)([A-Z][a-zA-Z\\s&\\'\\.\\-]+?)\\s*[;:]',\n",
    "            line\n",
    "        )\n",
    "        \n",
    "        if new_entry_match:\n",
    "            if current_entry and current_town:\n",
    "                entries.append((current_town, ' '.join(current_entry)))\n",
    "            current_town = new_entry_match.group(1).strip().rstrip(',')\n",
    "            current_town = normalize_town_name(current_town)\n",
    "            current_entry = [line]\n",
    "        elif current_entry:\n",
    "            current_entry.append(line)\n",
    "    \n",
    "    if current_entry and current_town:\n",
    "        entries.append((current_town, ' '.join(current_entry)))\n",
    "    \n",
    "    return entries\n",
    "\n",
    "\n",
    "def parse_newspaper_entries(text):\n",
    "    \"\"\"\n",
    "    Parse the text into individual newspaper entries.\n",
    "    This version handles entries that span multiple lines and \n",
    "    multiple entries on the same line.\n",
    "    \"\"\"\n",
    "    # First, try the improved v2 parser\n",
    "    entries = parse_newspaper_entries_v2(text)\n",
    "    \n",
    "    if entries:\n",
    "        return entries\n",
    "    \n",
    "    # Fallback to line-based approach\n",
    "    return parse_newspaper_entries_fallback(text)\n",
    "\n",
    "\n",
    "def parse_entry_details(town, entry_text):\n",
    "    \"\"\"Extract structured data from a single entry.\"\"\"\n",
    "    \n",
    "    # Normalize the town name\n",
    "    town = normalize_town_name(town)\n",
    "    \n",
    "    result = {\n",
    "        'town': town.strip().title(),\n",
    "        'newspaper_name': '',\n",
    "        'frequency': extract_frequency(entry_text),\n",
    "        'political_affiliation': extract_political_affiliation(entry_text),\n",
    "        'subscription_price': extract_subscription_details(entry_text),\n",
    "        'established': extract_established(entry_text),\n",
    "        'circulation': extract_circulation(entry_text),\n",
    "        'raw_text': entry_text[:300] + '...' if len(entry_text) > 300 else entry_text\n",
    "    }\n",
    "    \n",
    "    # Try to extract newspaper name using the improved function\n",
    "    extracted_town, newspaper, _ = extract_town_and_newspaper(entry_text)\n",
    "    \n",
    "    if newspaper:\n",
    "        result['newspaper_name'] = newspaper\n",
    "    else:\n",
    "        # Fallback to original method\n",
    "        # Escape the town name properly for regex\n",
    "        town_escaped = re.escape(town)\n",
    "        name_match = re.match(\n",
    "            rf'^{town_escaped}[,\\s]+([A-Za-z][A-Za-z\\s&\\'\\.\\-,]+?)\\s*[;:]',\n",
    "            entry_text, re.IGNORECASE\n",
    "        )\n",
    "        if name_match:\n",
    "            result['newspaper_name'] = name_match.group(1).strip().rstrip(',;:')\n",
    "    \n",
    "    people = extract_editor_publisher(entry_text)\n",
    "    result['editor'] = people['editor']\n",
    "    result['publisher'] = people['publisher']\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def process_file(input_path, output_path=None):\n",
    "    \"\"\"Process the input file and write results to CSV.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Reading file: {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(\"Parsing entries...\")\n",
    "    raw_entries = parse_newspaper_entries(text)\n",
    "    total_entries = len(raw_entries)\n",
    "    print(f\"Found {total_entries} raw entries\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (town, entry_text) in enumerate(raw_entries):\n",
    "        if (i + 1) % 100 == 0 or i == total_entries - 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            pct = (i + 1) / total_entries * 100\n",
    "            print(f\"Processing: {i + 1}/{total_entries} ({pct:.1f}%) - {elapsed:.1f}s elapsed\")\n",
    "        \n",
    "        if not is_valid_town_name(town):\n",
    "            continue\n",
    "        if not is_valid_entry_text(entry_text):\n",
    "            continue\n",
    "        \n",
    "        details = parse_entry_details(town, entry_text)\n",
    "        if details['newspaper_name'] and len(details['newspaper_name']) > 1:\n",
    "            results.append(details)\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = Path(input_path).stem + '_extracted.csv'\n",
    "    \n",
    "    fieldnames = ['town', 'newspaper_name', 'frequency', 'political_affiliation', \n",
    "                  'subscription_price', 'established', 'editor', 'publisher',\n",
    "                  'circulation', 'raw_text']\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Completed in {elapsed:.1f} seconds\")\n",
    "    print(f\"Processed {len(results)} valid entries\")\n",
    "    print(f\"Output written to: {output_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Entries with frequency: {sum(1 for r in results if r['frequency'])}\")\n",
    "    print(f\"Entries with political affiliation: {sum(1 for r in results if r['political_affiliation'])}\")\n",
    "    print(f\"Entries with subscription price: {sum(1 for r in results if r['subscription_price'])}\")\n",
    "    print(f\"Entries with established date: {sum(1 for r in results if r['established'])}\")\n",
    "    print(f\"Entries with editor: {sum(1 for r in results if r['editor'])}\")\n",
    "    print(f\"Entries with publisher: {sum(1 for r in results if r['publisher'])}\")\n",
    "    print(f\"Entries with circulation: {sum(1 for r in results if r['circulation'])}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"data/Newspaper Directory Text/\")[:5]:\n",
    "    input_file = \"data/Newspaper Directory Text/\" + file\n",
    "    output_file = \"data/Newspaper Directory Excel/\" + file[:-3] + 'csv'\n",
    "    results = process_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1efc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Newspaper Directory Text/Rowell 1869.txt: 4859 entries found\n",
      "  States detected at positions: [(178, 'NEW YORK'), (9997, 'ALABAMA'), (14964, 'ALABAMA'), (14975, 'ALABAMA'), (21921, 'ARKANSAS'), (22122, 'ARKANSAS'), (25267, 'ARKANSAS'), (25684, 'ARKANSAS'), (27603, 'CALIFORNIA'), (29205, 'CALIFORNIA')]...\n",
      "data/Newspaper Directory Text/Rowell 1871.txt: 5878 entries found\n",
      "  States detected at positions: [(1645, 'ALABAMA'), (6202, 'ALABAMA'), (6936, 'ALABAMA'), (10647, 'ALABAMA'), (14436, 'ALABAMA'), (16217, 'ARKANSAS'), (17901, 'ARKANSAS'), (21657, 'ARKANSAS'), (25257, 'CALIFORNIA'), (28971, 'CALIFORNIA')]...\n",
      "data/Newspaper Directory Text/Rowell 1872.txt: 6241 entries found\n",
      "  States detected at positions: [(3031, 'ALABAMA'), (5793, 'ALABAMA'), (8365, 'ALABAMA'), (9568, 'ALABAMA'), (9807, 'ALABAMA'), (15944, 'ALABAMA'), (17629, 'ARKANSAS'), (19444, 'ARKANSAS'), (23221, 'ARKANSAS'), (26938, 'ARKANSAS')]...\n",
      "data/Newspaper Directory Text/Rowell 1873.txt: 6550 entries found\n",
      "  States detected at positions: [(4234, 'ALABAMA'), (6070, 'ALABAMA'), (8785, 'ALABAMA'), (9804, 'ALABAMA'), (16230, 'ALABAMA'), (18164, 'ARKANSAS'), (21031, 'ARKANSAS'), (23281, 'ARKANSAS'), (23656, 'ARKANSAS'), (27325, 'ARKANSAS')]...\n",
      "data/Newspaper Directory Text/Rowell 1876.txt: 7825 entries found\n",
      "  States detected at positions: [(2844, 'ALABAMA'), (5246, 'ALABAMA'), (7929, 'ALABAMA'), (10045, 'ALABAMA'), (15288, 'ALABAMA'), (18118, 'ARKANSAS'), (19086, 'ARKANSAS'), (22502, 'ARKANSAS'), (26260, 'ARKANSAS'), (27636, 'ARKANSAS')]...\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Known US states and territories from that era\n",
    "STATES = {\n",
    "    \"ALABAMA\", \"ARKANSAS\", \"ARIZONA\", \"CALIFORNIA\", \"COLORADO\", \"CONNECTICUT\",\n",
    "    \"DELAWARE\", \"DISTRICT OF COLUMBIA\", \"FLORIDA\", \"GEORGIA\", \"IDAHO\", \"ILLINOIS\",\n",
    "    \"INDIANA\", \"IOWA\", \"KANSAS\", \"KENTUCKY\", \"LOUISIANA\", \"MAINE\", \"MARYLAND\",\n",
    "    \"MASSACHUSETTS\", \"MICHIGAN\", \"MINNESOTA\", \"MISSISSIPPI\", \"MISSOURI\", \"MONTANA\",\n",
    "    \"NEBRASKA\", \"NEVADA\", \"NEW HAMPSHIRE\", \"NEW JERSEY\", \"NEW MEXICO\", \"NEW YORK\",\n",
    "    \"NORTH CAROLINA\", \"OHIO\", \"OREGON\", \"PENNSYLVANIA\", \"RHODE ISLAND\",\n",
    "    \"SOUTH CAROLINA\", \"TENNESSEE\", \"TEXAS\", \"UTAH\", \"VERMONT\", \"VIRGINIA\",\n",
    "    \"WASHINGTON\", \"WEST VIRGINIA\", \"WISCONSIN\", \"WYOMING\",\n",
    "    \"INDIAN TERRITORY\", \"DAKOTA\", \"DOMINION OF CANADA\", \"BRITISH COLONIES\"\n",
    "}\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"Process a newspaper directory file and extract entries to CSV.\"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove page markers\n",
    "    content = re.sub(r'---\\s*Page\\s+\\d+\\s*---', ' ', content)\n",
    "    \n",
    "    # Fix common OCR Greek letter substitutions (uppercase)\n",
    "    content = content.replace('Α', 'A')  # Greek Alpha -> A\n",
    "    content = content.replace('Β', 'B')  # Greek Beta -> B\n",
    "    content = content.replace('Ε', 'E')  # Greek Epsilon -> E\n",
    "    content = content.replace('Η', 'H')  # Greek Eta -> H\n",
    "    content = content.replace('Ι', 'I')  # Greek Iota -> I\n",
    "    content = content.replace('Κ', 'K')  # Greek Kappa -> K\n",
    "    content = content.replace('Μ', 'M')  # Greek Mu -> M\n",
    "    content = content.replace('Ν', 'N')  # Greek Nu -> N\n",
    "    content = content.replace('Ο', 'O')  # Greek Omicron -> O\n",
    "    content = content.replace('Ρ', 'P')  # Greek Rho -> P\n",
    "    content = content.replace('Τ', 'T')  # Greek Tau -> T\n",
    "    content = content.replace('Χ', 'X')  # Greek Chi -> X\n",
    "    content = content.replace('Ζ', 'Z')  # Greek Zeta -> Z\n",
    "    content = content.replace('Θ', 'O')  # Greek Theta -> O (visually similar)\n",
    "    content = content.replace('Φ', 'O')  # Greek Phi -> O (visually similar)\n",
    "    \n",
    "    # Fix common OCR Cyrillic letter substitutions\n",
    "    content = content.replace('С', 'C')  # Cyrillic Es -> C\n",
    "    content = content.replace('О', 'O')  # Cyrillic O -> O\n",
    "    content = content.replace('Р', 'P')  # Cyrillic Er -> P\n",
    "    content = content.replace('Ф', 'O')  # Cyrillic Ef -> O\n",
    "    content = content.replace('А', 'A')  # Cyrillic A -> A\n",
    "    content = content.replace('Е', 'E')  # Cyrillic Ie -> E\n",
    "    content = content.replace('Н', 'H')  # Cyrillic En -> H\n",
    "    content = content.replace('В', 'B')  # Cyrillic Ve -> B\n",
    "    content = content.replace('К', 'K')  # Cyrillic Ka -> K\n",
    "    content = content.replace('М', 'M')  # Cyrillic Em -> M\n",
    "    content = content.replace('Т', 'T')  # Cyrillic Te -> T\n",
    "    \n",
    "    # Fix lowercase Greek/Cyrillic\n",
    "    content = content.replace('ο', 'o')  # Greek lowercase omicron -> o\n",
    "    content = content.replace('а', 'a')  # Cyrillic lowercase a -> a\n",
    "    content = content.replace('е', 'e')  # Cyrillic lowercase ie -> e\n",
    "    content = content.replace('о', 'o')  # Cyrillic lowercase o -> o\n",
    "    content = content.replace('р', 'p')  # Cyrillic lowercase er -> p\n",
    "    content = content.replace('с', 'c')  # Cyrillic lowercase es -> c\n",
    "    \n",
    "    # Fix OCR diacritical errors\n",
    "    content = content.replace('Ü', 'U')\n",
    "    content = content.replace('Ö', 'O')\n",
    "    content = content.replace('Ä', 'A')\n",
    "    content = content.replace('É', 'E')\n",
    "    content = content.replace('È', 'E')\n",
    "    content = content.replace('Ñ', 'N')\n",
    "    content = content.replace('Ç', 'C')\n",
    "    \n",
    "    # Remove OCR artifacts: sequences of O-like characters before town names\n",
    "    # Matches patterns like \"OOO \", \"OOD \", \"COO \", \"CODO \", etc.\n",
    "    content = re.sub(r'\\b[OoCcDd0ΘΦ]{2,}\\s+([A-Z]{2,})', r'\\1', content)\n",
    "    \n",
    "    # Fix missing space between ALL CAPS town and Capitalized newspaper name\n",
    "    # e.g., \"VAN BURENArgus\" -> \"VAN BUREN Argus\"\n",
    "    content = re.sub(r'([A-Z]{4})([A-Z][a-z])', r'\\1 \\2', content)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(content.split())\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Build state position index\n",
    "    state_positions = []\n",
    "    \n",
    "    for state in STATES:\n",
    "        # Pattern allows optional space before period: \"ARKANSAS .\" or \"ARKANSAS.\"\n",
    "        pattern = re.compile(r'\\b' + re.escape(state) + r'\\s*\\.', re.IGNORECASE)\n",
    "        for m in pattern.finditer(text):\n",
    "            state_positions.append((m.start(), state))\n",
    "    \n",
    "    state_positions.sort()\n",
    "    \n",
    "    # Remove duplicate state entries at same/nearby positions\n",
    "    filtered_positions = []\n",
    "    for pos, state in state_positions:\n",
    "        if not filtered_positions or pos - filtered_positions[-1][0] > 10:\n",
    "            filtered_positions.append((pos, state))\n",
    "    state_positions = filtered_positions\n",
    "    \n",
    "    # Main pattern for newspaper entries\n",
    "    pattern = re.compile(\n",
    "        r'\\b'\n",
    "        r'([A-Z][A-Z\\'\\-]+(?:\\s+[A-Z][A-Z\\'\\-]+)*)'  # Group 1: Town (ALL CAPS words)\n",
    "        r'\\s*[,.\\s]\\s*'                              # Separator\n",
    "        r'([A-Z][a-z][^;:†]*?)'                      # Group 2: Newspaper name\n",
    "        r'\\s*[;:†]'                                  # Delimiter\n",
    "    )\n",
    "    \n",
    "    matches = list(pattern.finditer(text))\n",
    "    \n",
    "    # First pass: identify valid entries\n",
    "    valid_matches = []\n",
    "    for match in matches:\n",
    "        pos = match.start()\n",
    "        \n",
    "        # Determine current state based on position\n",
    "        match_state = None\n",
    "        for sp, st in reversed(state_positions):\n",
    "            if sp < pos:\n",
    "                match_state = st\n",
    "                break\n",
    "        \n",
    "        if not match_state:\n",
    "            continue\n",
    "        \n",
    "        town = match.group(1).strip().rstrip(' ,.')\n",
    "        newspaper = match.group(2).strip().rstrip(' ,.')\n",
    "        \n",
    "        # Skip index/header content\n",
    "        if any(kw in newspaper.lower() for kw in ['list of', 'index', 'page']):\n",
    "            continue\n",
    "            \n",
    "        # Skip if newspaper contains what looks like a page header\n",
    "        if re.search(r'\\b\\d+\\s+[A-Z]{4,}\\.', newspaper):\n",
    "            continue\n",
    "        \n",
    "        if len(town) >= 2 and len(newspaper) >= 2:\n",
    "            valid_matches.append((match, match_state, town, newspaper))\n",
    "    \n",
    "    # Second pass: build results\n",
    "    for i, (match, match_state, town, newspaper) in enumerate(valid_matches):\n",
    "        if i + 1 < len(valid_matches):\n",
    "            raw_text = text[match.start():valid_matches[i + 1][0].start()].strip()\n",
    "        else:\n",
    "            raw_text = text[match.start():].strip()\n",
    "        \n",
    "        results.append({\n",
    "            'state': match_state,\n",
    "            'town': town.title(),\n",
    "            'newspaper': newspaper,\n",
    "            'raw_text': raw_text\n",
    "        })\n",
    "    \n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in results:\n",
    "        key = (r['state'], r['town'], r['newspaper'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(r)\n",
    "    \n",
    "    # Remove known false positive entries from document header\n",
    "    false_positives = {\n",
    "        (\"NEW YORK\", \"York\", \"January 1, 1869. ee ~~ CONTENTS\"),\n",
    "        (\"NEW YORK\", \"Xiv\", \"Newspaper Directory Advertiser. XV. A circular to Advertisers, containing the names of more than one thousand newspapers, among which will be found the best advertising mediums in America\"),\n",
    "    }\n",
    "    unique = [r for r in unique if (r['state'], r['town'], r['newspaper']) not in false_positives]\n",
    "    \n",
    "    print(f\"{input_file}: {len(unique)} entries found\")\n",
    "    print(f\"  States detected at positions: {state_positions[:10]}...\")  # Debug\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['state', 'town', 'newspaper', 'raw_text'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique)\n",
    "    \n",
    "    return unique\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"data/Newspaper Directory Text/\")[:5]:\n",
    "    input_file = \"data/Newspaper Directory Text/\" + file\n",
    "    output_file = \"data/Newspaper Directory Excel/\" + file[:-3] + 'csv'\n",
    "    results = process_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28a613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_files(txt_file, csv_file, output_file):\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        txt_content = f.read()[25000:40000]\n",
    "    \n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        csv_content = f.read()[20000:35000]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== INPUT (chars 1000-15000) ===\\n\\n\")\n",
    "        f.write(txt_content)\n",
    "        f.write(\"\\n\\n=== OUTPUT (first 15000 chars) ===\\n\\n\")\n",
    "        f.write(csv_content)\n",
    "\n",
    "compare_files(r'data\\Newspaper Directory Text\\Rowell 1880 - v13.txt', r'data\\Newspaper Directory Excel\\Rowell 1880.csv', r'comparison.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a9eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Rowell 1869.csv: 3072 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 81.4%\n",
      "  political_affiliation: 44.2%\n",
      "  subscription_price: 37.1%\n",
      "  established: 48.8%\n",
      "  editor: 78.7%\n",
      "  publisher: 76.0%\n",
      "  circulation: 38.3%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1871.csv: 5878 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper: 100.0%\n",
      "  frequency: 84.9%\n",
      "  political: 47.6%\n",
      "  editor: 92.7%\n",
      "  publisher: 95.8%\n",
      "  circulation: 83.5%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1872.csv: 6241 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper: 100.0%\n",
      "  frequency: 84.2%\n",
      "  political: 49.0%\n",
      "  editor: 91.2%\n",
      "  publisher: 94.8%\n",
      "  circulation: 83.6%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1873.csv: 6550 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper: 100.0%\n",
      "  frequency: 84.8%\n",
      "  political: 45.2%\n",
      "  editor: 91.3%\n",
      "  publisher: 95.1%\n",
      "  circulation: 88.9%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1876.csv: 7825 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper: 100.0%\n",
      "  frequency: 83.3%\n",
      "  political: 42.7%\n",
      "  editor: 88.6%\n",
      "  publisher: 90.8%\n",
      "  circulation: 72.9%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1877.csv: 7274 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 95.5%\n",
      "  political_affiliation: 66.1%\n",
      "  subscription_price: 80.4%\n",
      "  established: 85.0%\n",
      "  editor: 91.0%\n",
      "  publisher: 91.7%\n",
      "  circulation: 75.2%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1878.csv: 7548 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 96.3%\n",
      "  political_affiliation: 66.0%\n",
      "  subscription_price: 82.4%\n",
      "  established: 86.2%\n",
      "  editor: 91.8%\n",
      "  publisher: 93.9%\n",
      "  circulation: 80.9%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1879.csv: 7785 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 95.8%\n",
      "  political_affiliation: 65.6%\n",
      "  subscription_price: 81.1%\n",
      "  established: 85.1%\n",
      "  editor: 90.4%\n",
      "  publisher: 92.2%\n",
      "  circulation: 60.6%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1880.csv: 8561 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 95.7%\n",
      "  political_affiliation: 63.6%\n",
      "  subscription_price: 79.0%\n",
      "  established: 85.7%\n",
      "  editor: 89.8%\n",
      "  publisher: 92.1%\n",
      "  circulation: 58.1%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1882.csv: 10435 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 96.4%\n",
      "  political_affiliation: 59.4%\n",
      "  subscription_price: 76.5%\n",
      "  established: 84.3%\n",
      "  editor: 89.7%\n",
      "  publisher: 92.2%\n",
      "  circulation: 54.3%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1883.csv: 10171 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 96.5%\n",
      "  political_affiliation: 58.1%\n",
      "  subscription_price: 78.1%\n",
      "  established: 85.5%\n",
      "  editor: 88.9%\n",
      "  publisher: 92.0%\n",
      "  circulation: 56.0%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1884.csv: 11484 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 96.0%\n",
      "  political_affiliation: 54.0%\n",
      "  subscription_price: 76.5%\n",
      "  established: 85.3%\n",
      "  editor: 89.3%\n",
      "  publisher: 92.1%\n",
      "  circulation: 52.1%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1885.csv: 12282 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 95.4%\n",
      "  political_affiliation: 52.6%\n",
      "  subscription_price: 70.7%\n",
      "  established: 80.3%\n",
      "  editor: 87.1%\n",
      "  publisher: 88.4%\n",
      "  circulation: 53.7%\n",
      "  raw_text: 100.0%\n",
      "\n",
      "==================================================\n",
      "Rowell 1890.csv: 15629 rows\n",
      "--------------------------------------------------\n",
      "  state: 100.0%\n",
      "  town: 100.0%\n",
      "  newspaper_name: 100.0%\n",
      "  frequency: 94.6%\n",
      "  political_affiliation: 69.2%\n",
      "  subscription_price: 74.9%\n",
      "  established: 84.4%\n",
      "  editor: 91.7%\n",
      "  publisher: 91.7%\n",
      "  circulation: 53.1%\n",
      "  raw_text: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "dir_path = Path(\"data/Newspaper Directory Excel\")\n",
    "\n",
    "for csv_file in sorted(dir_path.glob(\"*.csv\")):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{csv_file.name}: {len(df)} rows\")\n",
    "    print(\"-\"*50)\n",
    "    for col in df.columns:\n",
    "        pct = df[col].notna().mean() * 100\n",
    "        print(f\"  {col}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71deaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original merger\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "DAYS_OF_WEEK = ['sundays', 'mondays', 'tuesdays', 'wednesdays', 'thursdays', 'fridays', 'saturdays']\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for matching: lowercase, strip whitespace, remove punctuation.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return str(s).lower().strip().replace(\".\", \"\").replace(\",\", \"\").replace(\"'\", \"\").replace(\" \", \"\")\n",
    "\n",
    "def normalize_text_no_days(s):\n",
    "    \"\"\"Normalize text and also remove days of the week.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    text = str(s).lower().strip()\n",
    "    for day in DAYS_OF_WEEK:\n",
    "        text = text.replace(day, \"\")\n",
    "    return text.replace(\".\", \"\").replace(\",\", \"\").replace(\"'\", \"\").replace(\" \", \"\")\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"Calculate similarity ratio between two strings (0 to 1).\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def is_fuzzy_match(town1, name1, town2, name2, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Check if two newspaper records match using fuzzy matching.\n",
    "    More strict: requires high similarity on both fields.\n",
    "    \"\"\"\n",
    "    town_sim = similarity(town1, town2)\n",
    "    name_sim = similarity(name1, name2)\n",
    "    \n",
    "    if town_sim >= threshold and name_sim >= threshold:\n",
    "        return True\n",
    "    if town_sim == 1.0 and name_sim >= 0.85:\n",
    "        return True\n",
    "    if name_sim == 1.0 and town_sim >= 0.85:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def remove_days_from_name(name):\n",
    "    \"\"\"Remove days of the week from a newspaper name, preserving original formatting.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    result = str(name)\n",
    "    for day in DAYS_OF_WEEK:\n",
    "        # Case-insensitive replacement\n",
    "        import re\n",
    "        result = re.sub(re.escape(day), '', result, flags=re.IGNORECASE)\n",
    "    # Clean up extra spaces\n",
    "    result = ' '.join(result.split()).strip()\n",
    "    return result\n",
    "\n",
    "def find_best_match(town, name, position_pct, existing_records, current_established=None, established_lookup=None, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Find the best matching key from existing records.\n",
    "    First tries exact match, then fuzzy match for towns with same first letter,\n",
    "    then tries again with days of week removed from BOTH current and existing names.\n",
    "    Uses lower threshold (80%) if established dates match.\n",
    "    \n",
    "    Returns tuple: (matched_key or None, matched_via_days_removal: bool)\n",
    "    \"\"\"\n",
    "    town_norm = normalize_text(town)\n",
    "    name_norm = normalize_text(name)\n",
    "    name_norm_no_days = normalize_text_no_days(name)\n",
    "    \n",
    "    # First: try exact match\n",
    "    exact_key = (town_norm, name_norm)\n",
    "    if exact_key in existing_records:\n",
    "        return exact_key, False\n",
    "    \n",
    "    # Get first letter of town for filtering\n",
    "    town_first_letter = town_norm[0] if town_norm else \"\"\n",
    "    \n",
    "    # Second: try fuzzy match, only considering towns with same first letter\n",
    "    # Now also compares with days removed from BOTH names\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    matched_via_days = False\n",
    "    \n",
    "    for (ex_town, ex_name), ex_position in existing_records.items():\n",
    "        # Only consider towns starting with same letter\n",
    "        if not ex_town or ex_town[0] != town_first_letter:\n",
    "            continue\n",
    "        \n",
    "        # Check if established dates match for lower threshold\n",
    "        effective_threshold = threshold\n",
    "        if current_established and established_lookup and (ex_town, ex_name) in established_lookup:\n",
    "            ex_established = established_lookup[(ex_town, ex_name)]\n",
    "            if ex_established and str(current_established).strip() == str(ex_established).strip():\n",
    "                effective_threshold = 0.80\n",
    "        \n",
    "        # Try standard fuzzy match first\n",
    "        if is_fuzzy_match(town_norm, name_norm, ex_town, ex_name, effective_threshold):\n",
    "            score = similarity(town_norm, ex_town) + similarity(name_norm, ex_name)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = (ex_town, ex_name)\n",
    "                matched_via_days = False\n",
    "            continue  # Found a match, no need to try days-removed for this record\n",
    "        \n",
    "        # Try with days removed from BOTH current and existing names\n",
    "        ex_name_no_days = normalize_text_no_days(ex_name)\n",
    "        if is_fuzzy_match(town_norm, name_norm_no_days, ex_town, ex_name_no_days, effective_threshold):\n",
    "            score = similarity(town_norm, ex_town) + similarity(name_norm_no_days, ex_name_no_days)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = (ex_town, ex_name)\n",
    "                matched_via_days = True\n",
    "    \n",
    "    return best_match, matched_via_days\n",
    "\n",
    "def load_and_tag_csvs(directory):\n",
    "    \"\"\"Load all CSVs from directory, tag with year, and split into pre/post 1877.\"\"\"\n",
    "    csv1_frames = []\n",
    "    csv2_frames = []\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    for file in Path(directory).glob(\"*.csv\"):\n",
    "        filename = file.stem\n",
    "        year = None\n",
    "        \n",
    "        match = re.search(r'(1[89]\\d{2})', filename)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "        \n",
    "        if year is None:\n",
    "            print(f\"Warning: Could not extract year from {file.name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')\n",
    "        df['_year'] = year\n",
    "        \n",
    "        print(f\"Loaded {file.name} (year {year}): {len(df)} records\")\n",
    "        \n",
    "        if year <= 1876:\n",
    "            csv1_frames.append(df)\n",
    "        else:\n",
    "            csv2_frames.append(df)\n",
    "    \n",
    "    return csv1_frames, csv2_frames\n",
    "\n",
    "def process_dataframe(df, year, has_state=False):\n",
    "    \"\"\"Process a single dataframe: standardize and prepare for merging.\"\"\"\n",
    "    if 'raw_text' in df.columns:\n",
    "        df = df.drop(columns=['raw_text'])\n",
    "    \n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    \n",
    "    data_cols = ['frequency', 'political_affiliation', 'subscription_price', \n",
    "                 'established', 'editor', 'publisher', 'circulation']\n",
    "    \n",
    "    rename_map = {}\n",
    "    for col in data_cols:\n",
    "        if col in df.columns:\n",
    "            rename_map[col] = f\"{year} {col}\"\n",
    "    \n",
    "    df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "def merge_newspapers_core(all_frames):\n",
    "    \"\"\"Core merge logic used by both full and test functions.\"\"\"\n",
    "    merged_records = {}\n",
    "    record_positions = {}\n",
    "    original_names = {}\n",
    "    established_lookup = {}  # Track established dates for each record\n",
    "    \n",
    "    print(f\"Processing {len(all_frames)} files...\")\n",
    "    print(\"Strategy: exact match first, then fuzzy match (same first letter, 90% similarity),\")\n",
    "    print(\"          with days of week removed from BOTH current and existing names,\")\n",
    "    print(\"          80% threshold if established dates match\\n\")\n",
    "    \n",
    "    for df, year, has_state in all_frames:\n",
    "        total_rows = len(df)\n",
    "        print(f\"  Processing year {year} ({total_rows} records)...\")\n",
    "        matches_found = 0\n",
    "        new_records = 0\n",
    "        \n",
    "        # Collect new records for this year, add to main dict after processing\n",
    "        year_new_keys = []\n",
    "        \n",
    "        # Find the established column for this year\n",
    "        established_col = f\"{year} established\"\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            row_num = df.index.get_loc(idx)\n",
    "            position_pct = row_num / max(total_rows - 1, 1)\n",
    "            \n",
    "            town = row.get('town', '')\n",
    "            name = row.get('newspaper_name', '')\n",
    "            state = row.get('state', '') if has_state else ''\n",
    "            current_established = row.get(established_col, None)\n",
    "            \n",
    "            town_norm = normalize_text(town)\n",
    "            name_norm = normalize_text(name)\n",
    "            \n",
    "            if not town_norm or not name_norm:\n",
    "                continue\n",
    "            \n",
    "            # Only match against records from previous years\n",
    "            existing_key, matched_via_days = find_best_match(\n",
    "                town, name, position_pct, record_positions,\n",
    "                current_established=current_established,\n",
    "                established_lookup=established_lookup,\n",
    "                threshold=0.90\n",
    "            )\n",
    "            \n",
    "            if existing_key:\n",
    "                key = existing_key\n",
    "                matches_found += 1\n",
    "                old_pos = record_positions[key]\n",
    "                record_positions[key] = (old_pos + position_pct) / 2\n",
    "                \n",
    "                # If matched via days removal, update the stored name to remove days\n",
    "                if matched_via_days:\n",
    "                    old_town, old_name, old_state = original_names[key]\n",
    "                    cleaned_name = remove_days_from_name(old_name)\n",
    "                    original_names[key] = (old_town, cleaned_name, old_state)\n",
    "            else:\n",
    "                key = (town_norm, name_norm)\n",
    "                merged_records[key] = {}\n",
    "                original_names[key] = (town, name, state)\n",
    "                # Queue this to be added after processing this year\n",
    "                year_new_keys.append((key, position_pct, current_established))\n",
    "                new_records += 1\n",
    "            \n",
    "            if state and not original_names[key][2]:\n",
    "                original_names[key] = (original_names[key][0], original_names[key][1], state)\n",
    "            \n",
    "            year_cols = [c for c in row.index if c.startswith(f\"{year} \")]\n",
    "            for col in year_cols:\n",
    "                merged_records[key][col] = row[col]\n",
    "        \n",
    "        # Now add this year's new records to positions for next year's matching\n",
    "        for key, pos, estab in year_new_keys:\n",
    "            record_positions[key] = pos\n",
    "            if estab:\n",
    "                established_lookup[key] = estab\n",
    "        \n",
    "        print(f\"    -> {matches_found} matched to existing, {new_records} new records\")\n",
    "    \n",
    "    print(f\"\\nTotal unique newspapers found: {len(merged_records)}\")\n",
    "    \n",
    "    rows = []\n",
    "    for key, data in merged_records.items():\n",
    "        town, name, state = original_names[key]\n",
    "        row = {'state': state, 'town': town, 'newspaper_name': name}\n",
    "        row.update(data)\n",
    "        rows.append(row)\n",
    "    \n",
    "    result = pd.DataFrame(rows)\n",
    "    \n",
    "    id_cols = ['state', 'town', 'newspaper_name']\n",
    "    year_cols = [c for c in result.columns if c not in id_cols]\n",
    "    \n",
    "    def sort_key(col):\n",
    "        parts = col.split(' ', 1)\n",
    "        if len(parts) == 2 and parts[0].isdigit():\n",
    "            return (int(parts[0]), parts[1])\n",
    "        return (9999, col)\n",
    "    \n",
    "    year_cols = sorted(year_cols, key=sort_key)\n",
    "    final_cols = id_cols + year_cols\n",
    "    result = result[final_cols]\n",
    "    result = result.sort_values(['state', 'town', 'newspaper_name'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def prepare_frames(directory, max_years=None):\n",
    "    \"\"\"Load and prepare frames, optionally limiting to first N years.\"\"\"\n",
    "    csv1_frames, csv2_frames = load_and_tag_csvs(directory)\n",
    "    \n",
    "    if not csv1_frames and not csv2_frames:\n",
    "        print(\"No CSV files found!\")\n",
    "        return None\n",
    "    \n",
    "    all_frames_raw = []\n",
    "    \n",
    "    for df in csv1_frames:\n",
    "        year = df['_year'].iloc[0]\n",
    "        all_frames_raw.append((df, year, False))\n",
    "    \n",
    "    for df in csv2_frames:\n",
    "        year = df['_year'].iloc[0]\n",
    "        all_frames_raw.append((df, year, True))\n",
    "    \n",
    "    all_frames_raw.sort(key=lambda x: x[1])\n",
    "    \n",
    "    if max_years is not None:\n",
    "        all_frames_raw = all_frames_raw[:max_years]\n",
    "        years_processing = [f[1] for f in all_frames_raw]\n",
    "        print(f\"\\nTEST MODE: Processing only first {max_years} years: {years_processing}\\n\")\n",
    "    \n",
    "    all_frames = []\n",
    "    for df, year, has_state in all_frames_raw:\n",
    "        df = df.drop(columns=['_year'])\n",
    "        df = process_dataframe(df, year, has_state=has_state)\n",
    "        all_frames.append((df, year, has_state))\n",
    "    \n",
    "    return all_frames\n",
    "\n",
    "def merge_newspapers_fuzzy(directory):\n",
    "    \"\"\"Main function to merge all newspaper CSVs with fuzzy matching.\"\"\"\n",
    "    all_frames = prepare_frames(directory)\n",
    "    if all_frames is None:\n",
    "        return None\n",
    "    return merge_newspapers_core(all_frames)\n",
    "\n",
    "def test(directory=r\"data\\Newspaper Directory Excel\"):\n",
    "    \"\"\"Test function that processes only the first 3 years.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING TEST MODE (first 3 years only)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_frames = prepare_frames(directory, max_years=3)\n",
    "    if all_frames is None:\n",
    "        print(\"Failed to load CSV files.\")\n",
    "        return None\n",
    "    \n",
    "    result = merge_newspapers_core(all_frames)\n",
    "    \n",
    "    if result is not None:\n",
    "        output_path = \"master_test.csv\"\n",
    "        result.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccess! Test output saved to: {output_path}\")\n",
    "        print(f\"Total newspapers: {len(result)}\")\n",
    "        print(f\"\\nColumns in output:\")\n",
    "        for col in result.columns:\n",
    "            print(f\"  - {col}\")\n",
    "        print(\"\\nFirst 10 rows preview:\")\n",
    "        print(result.head(10).to_string())\n",
    "    else:\n",
    "        print(\"Failed to create merged CSV.\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    directory = r\"data\\Newspaper Directory Excel\"\n",
    "    \n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--test\":\n",
    "        test(directory)\n",
    "    else:\n",
    "        print(f\"Processing CSVs from: {directory}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        result = merge_newspapers_fuzzy(directory)\n",
    "        \n",
    "        if result is not None:\n",
    "            output_path = \"master.csv\"\n",
    "            result.to_csv(output_path, index=False)\n",
    "            print(f\"\\nSuccess! Output saved to: {output_path}\")\n",
    "            print(f\"Total newspapers: {len(result)}\")\n",
    "            print(f\"\\nColumns in output:\")\n",
    "            for col in result.columns:\n",
    "                print(f\"  - {col}\")\n",
    "        else:\n",
    "            print(\"Failed to create merged CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
