{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7731e54f",
   "metadata": {},
   "source": [
    "This file is a work in progress:\n",
    "TODO: Rewrite it to scrape ALL entries in newspapers_all_years_updated.CSV, not just unmatched newspapers. Then write a script to query the Claude API with a prompt to turn the essays from the Chronicling America API results into structured JSON files (this might be best done in two stages - first pass to extract the editor/publisher timelines, then second pass to turn it into json). Then write a script to merge all of this new data onto master. I'd have to think through how to make sure that all the papers with data from both sources get merged properly, and one issue with this current script is that the json gets merged onto master and then some of these entries don't actually get matched back in, because the name is slightly different. So when they get added into master, I'd need to make sure that they had the exact same info as those in \"newspapers_all_years_updated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dea47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finds unmatched newspaper entries (missing master_id) in data/matches.csv,\n",
    "joins with data/newspapers_all_years_updated.csv on ISSN to get the LCCN,\n",
    "then queries the Library of Congress loc.gov API (current, post-Aug 2025)\n",
    "for \"about this newspaper\" information.\n",
    "\n",
    "NOTE: uses the current loc.gov API exclusively:\n",
    "  - Item endpoint:      https://www.loc.gov/item/{lccn}/?fo=json\n",
    "  - Collection search:  https://www.loc.gov/collections/chronicling-america/\n",
    "                          ?fa=number_lccn:{lccn}&fo=json\n",
    "  - General search:     https://www.loc.gov/search/?q={query}&fo=json\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import requests\n",
    "import math\n",
    "from pathlib import Path\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "MATCHES_CSV = \"data/matches.csv\"\n",
    "NEWSPAPERS_CSV = \"data/newspapers_all_years_updated.csv\"\n",
    "\n",
    "# ── Current loc.gov API endpoints (post-Aug 2025) ─────────────────────────\n",
    "# Item endpoint: returns full bibliographic record for a newspaper title\n",
    "LOC_ITEM_URL = \"https://www.loc.gov/item/{lccn}/?fo=json\"\n",
    "# Collection search: search within Chronicling America by LCCN\n",
    "LOC_COLLECTION_URL = \"https://www.loc.gov/collections/chronicling-america/\"\n",
    "# Newspaper directory: search the full US newspaper directory\n",
    "LOC_DIRECTORY_URL = \"https://www.loc.gov/collections/directory-of-us-newspapers-in-american-libraries/\"\n",
    "# General search fallback\n",
    "LOC_SEARCH_URL = \"https://www.loc.gov/search/\"\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"NewspaperLookup/1.0 (research script)\"}\n",
    "REQUEST_TIMEOUT = 60  # loc.gov can be slow\n",
    "\n",
    "\n",
    "def _build_session() -> requests.Session:\n",
    "    \"\"\"Build a requests session with automatic retries.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=2,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.headers.update(HEADERS)\n",
    "    return session\n",
    "\n",
    "\n",
    "SESSION = _build_session()\n",
    "\n",
    "\n",
    "# HTML essay scraper \n",
    "\n",
    "def _html_to_text(raw_html: str) -> str:\n",
    "    \"\"\"Convert an HTML string (like the essay field) to clean plain text.\"\"\"\n",
    "    import html as html_mod\n",
    "    # Replace </p> and <br> with newlines to preserve paragraph breaks\n",
    "    text = re.sub(r'</p>', '\\n\\n', raw_html)\n",
    "    text = re.sub(r'<br\\s*/?>', '\\n', text)\n",
    "    # Strip remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Decode HTML entities (&#160; &#699; &amp; etc.)\n",
    "    text = html_mod.unescape(text)\n",
    "    # Clean up whitespace within lines\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "    # Normalize paragraph breaks\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# CSV reading\n",
    "\n",
    "def read_csv(filepath: str) -> list[dict]:\n",
    "    \"\"\"Read a CSV and return rows as dicts with stripped headers.\"\"\"\n",
    "    with open(filepath, newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        reader.fieldnames = [h.strip() for h in reader.fieldnames]\n",
    "        return list(reader)\n",
    "\n",
    "\n",
    "def build_issn_to_lccn(newspapers_rows: list[dict]) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Build a lookup from ISSN -> {lccn, name, town, state} using\n",
    "    data/newspapers_all_years_updated.csv.\n",
    "    \"\"\"\n",
    "    lookup = {}\n",
    "    for row in newspapers_rows:\n",
    "        issn = row.get(\"issn\", \"\").strip()\n",
    "        lccn = row.get(\"lccn\", \"\").strip()\n",
    "        if issn and lccn:\n",
    "            lookup[issn] = {\n",
    "                \"lccn\": lccn,\n",
    "                \"name\": row.get(\"name\", \"\").strip(),\n",
    "                \"town\": row.get(\"town\", \"\").strip(),\n",
    "                \"state\": row.get(\"state\", \"\").strip(),\n",
    "            }\n",
    "    return lookup\n",
    "\n",
    "\n",
    "def find_unmatched(rows: list[dict]) -> list[dict]:\n",
    "    \"\"\"Return rows from matches.csv where master_id is empty.\"\"\"\n",
    "    return [row for row in rows if not row.get(\"master_id\", \"\").strip()]\n",
    "\n",
    "\n",
    "# API queries (all using current loc.gov API)\n",
    "\n",
    "def query_loc_item(lccn: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    PRIMARY METHOD: Fetch newspaper metadata from the loc.gov item endpoint.\n",
    "    URL: https://www.loc.gov/item/{lccn}/?fo=json\n",
    "\n",
    "    The essay lives in item.essay as an HTML string.\n",
    "    \"\"\"\n",
    "    url = LOC_ITEM_URL.format(lccn=lccn)\n",
    "    try:\n",
    "        resp = SESSION.get(url, timeout=REQUEST_TIMEOUT)\n",
    "        if resp.status_code == 404:\n",
    "            log.info(f\"    loc.gov/item: 404 for LCCN {lccn}\")\n",
    "            return None\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        item = data.get(\"item\", {})\n",
    "\n",
    "        # The essay is in item.essay as HTML; convert to plain text\n",
    "        essay_html = item.get(\"essay\", \"\")\n",
    "        essay_text = _html_to_text(essay_html) if essay_html else \"\"\n",
    "        essay_contributor = item.get(\"essay_contributor\", [])\n",
    "\n",
    "        return {\n",
    "            \"source\": \"loc.gov/item\",\n",
    "            \"lccn\": lccn,\n",
    "            \"url\": f\"https://www.loc.gov/item/{lccn}/\",\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"date\": item.get(\"date\", \"\"),\n",
    "            \"dates_of_publication\": item.get(\"dates_of_publication\", \"\"),\n",
    "            \"created_published\": item.get(\"created_published\", []),\n",
    "            \"description\": item.get(\"description\", []),\n",
    "            \"subject_headings\": item.get(\"subject_headings\", []),\n",
    "            \"notes\": item.get(\"notes\", []),\n",
    "            \"contributors\": item.get(\"contributor_names\", []),\n",
    "            \"location\": item.get(\"location\", []),\n",
    "            \"call_number\": item.get(\"call_number\", []),\n",
    "            \"medium\": item.get(\"medium\", \"\"),\n",
    "            \"issn\": item.get(\"number_issn\", item.get(\"number\", [])),\n",
    "            \"oclc\": item.get(\"number_oclc\", []),\n",
    "            \"related_items\": item.get(\"related_items\", []),\n",
    "            \"other_title\": item.get(\"other_title\", []),\n",
    "            \"publication_frequency\": item.get(\"publication_frequency\", []),\n",
    "            \"rights\": item.get(\"rights_advisory\", []),\n",
    "            \"essay_html\": essay_html,\n",
    "            \"essay\": essay_text,\n",
    "            \"essay_contributor\": essay_contributor,\n",
    "        }\n",
    "    except requests.exceptions.Timeout:\n",
    "        log.warning(f\"    loc.gov/item timed out for LCCN {lccn} (timeout={REQUEST_TIMEOUT}s)\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        log.warning(f\"    loc.gov/item failed for LCCN {lccn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_loc_collection(lccn: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    FALLBACK #1: Search the Chronicling America collection by LCCN.\n",
    "    URL: https://www.loc.gov/collections/chronicling-america/\n",
    "         ?fa=number_lccn:{lccn}&fo=json\n",
    "\n",
    "    Returns summary info from the collection search results.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"fa\": f\"number_lccn:{lccn}\",\n",
    "        \"fo\": \"json\",\n",
    "        \"c\": 1,\n",
    "    }\n",
    "    try:\n",
    "        resp = SESSION.get(LOC_COLLECTION_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            log.info(f\"    collection search: no results for LCCN {lccn}\")\n",
    "            return None\n",
    "\n",
    "        r = results[0]\n",
    "        return {\n",
    "            \"source\": \"loc.gov/collections/chronicling-america\",\n",
    "            \"lccn\": lccn,\n",
    "            \"url\": r.get(\"url\", r.get(\"id\", \"\")),\n",
    "            \"title\": r.get(\"title\", \"\"),\n",
    "            \"date\": r.get(\"date\", \"\"),\n",
    "            \"description\": r.get(\"description\", []),\n",
    "            \"subjects\": r.get(\"subject\", []),\n",
    "            \"location\": r.get(\"location\", []),\n",
    "            \"contributors\": r.get(\"contributor\", []),\n",
    "            \"total_results\": data.get(\"pagination\", {}).get(\"of\", 0),\n",
    "        }\n",
    "    except requests.exceptions.Timeout:\n",
    "        log.warning(f\"    collection search timed out for LCCN {lccn}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        log.warning(f\"    collection search failed for LCCN {lccn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_loc_directory(lccn: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    FALLBACK #2: Search the US Newspaper Directory collection.\n",
    "    URL: https://www.loc.gov/collections/\n",
    "         directory-of-us-newspapers-in-american-libraries/\n",
    "         ?q={lccn}&fo=json\n",
    "\n",
    "    Some newspapers are in the directory but not digitized in\n",
    "    Chronicling America.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\": lccn,\n",
    "        \"fo\": \"json\",\n",
    "        \"c\": 5,\n",
    "    }\n",
    "    try:\n",
    "        resp = SESSION.get(LOC_DIRECTORY_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            log.info(f\"    directory search: no results for LCCN {lccn}\")\n",
    "            return None\n",
    "\n",
    "        r = results[0]\n",
    "        return {\n",
    "            \"source\": \"loc.gov/collections/directory\",\n",
    "            \"lccn\": lccn,\n",
    "            \"url\": r.get(\"url\", r.get(\"id\", \"\")),\n",
    "            \"title\": r.get(\"title\", \"\"),\n",
    "            \"date\": r.get(\"date\", \"\"),\n",
    "            \"description\": r.get(\"description\", []),\n",
    "            \"subjects\": r.get(\"subject\", []),\n",
    "            \"location\": r.get(\"location\", []),\n",
    "        }\n",
    "    except requests.exceptions.Timeout:\n",
    "        log.warning(f\"    directory search timed out for LCCN {lccn}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        log.warning(f\"    directory search failed for LCCN {lccn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_loc_search_fallback(name: str, issn: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    LAST RESORT: Search loc.gov generally by newspaper name or ISSN.\n",
    "    Used when we have no LCCN at all.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    if issn:\n",
    "        queries.append(issn)\n",
    "    if name:\n",
    "        queries.append(name)\n",
    "\n",
    "    for q in queries:\n",
    "        params = {\n",
    "            \"q\": q,\n",
    "            \"fa\": \"original_format:newspaper\",\n",
    "            \"fo\": \"json\",\n",
    "            \"c\": 5,\n",
    "        }\n",
    "        try:\n",
    "            resp = SESSION.get(LOC_SEARCH_URL, params=params, timeout=REQUEST_TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            results = data.get(\"results\", [])\n",
    "            if results:\n",
    "                r = results[0]\n",
    "                return {\n",
    "                    \"source\": \"loc.gov/search (fallback)\",\n",
    "                    \"title\": r.get(\"title\", \"\"),\n",
    "                    \"date\": r.get(\"date\", \"\"),\n",
    "                    \"description\": r.get(\"description\", []),\n",
    "                    \"subjects\": r.get(\"subject\", []),\n",
    "                    \"location\": r.get(\"location\", []),\n",
    "                    \"url\": r.get(\"url\", r.get(\"id\", \"\")),\n",
    "                    \"contributors\": r.get(\"contributor\", []),\n",
    "                }\n",
    "        except Exception as e:\n",
    "            log.warning(f\"    search fallback failed for '{q}': {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Main orchestration\n",
    "\n",
    "def lookup_newspaper(row: dict, issn_to_lccn: dict, delay: float) -> dict:\n",
    "    \"\"\"\n",
    "    Look up a single unmatched newspaper entry using a cascade:\n",
    "      1. Join on ISSN to get LCCN\n",
    "      2. Try loc.gov/item/{lccn}/?fo=json          (primary — full record)\n",
    "      3. Try loc.gov/collections/chronicling-america (fallback — search)\n",
    "      4. Try loc.gov/collections/directory           (fallback — directory)\n",
    "      5. Try loc.gov/search by name/ISSN            (last resort)\n",
    "    \"\"\"\n",
    "    name = row.get(\"newspapers_all_years_name\", \"\").strip()\n",
    "    issn = row.get(\"issn\", \"\").strip()\n",
    "    master_name = row.get(\"master_name\", \"\").strip()\n",
    "\n",
    "    log.info(f\"Looking up: '{name}' (ISSN: {issn})\")\n",
    "\n",
    "    result = {\n",
    "        \"input\": {\"name\": name, \"issn\": issn, \"master_name\": master_name},\n",
    "        \"lccn\": None,\n",
    "        \"lccn_source\": None,\n",
    "        \"api_result\": None,\n",
    "    }\n",
    "\n",
    "    # Step 1: Try to get LCCN from the newspapers CSV\n",
    "    lccn_info = issn_to_lccn.get(issn)\n",
    "    if lccn_info:\n",
    "        lccn = lccn_info[\"lccn\"]\n",
    "        result[\"lccn\"] = lccn\n",
    "        result[\"lccn_source\"] = \"newspapers_all_years_updated.csv\"\n",
    "        log.info(f\"  LCCN from CSV: {lccn}\")\n",
    "\n",
    "        # Step 2: Primary — loc.gov item endpoint\n",
    "        time.sleep(delay)\n",
    "        api = query_loc_item(lccn)\n",
    "        if api:\n",
    "            result[\"api_result\"] = api\n",
    "            log.info(f\"  ✓ loc.gov/item: {api['title']}\")\n",
    "            return result\n",
    "\n",
    "        # Step 3: Fallback — Chronicling America collection search\n",
    "        time.sleep(delay)\n",
    "        api = query_loc_collection(lccn)\n",
    "        if api:\n",
    "            result[\"api_result\"] = api\n",
    "            log.info(f\"  ✓ collection search: {api['title']}\")\n",
    "            return result\n",
    "\n",
    "        # Step 4: Fallback — US Newspaper Directory\n",
    "        time.sleep(delay)\n",
    "        api = query_loc_directory(lccn)\n",
    "        if api:\n",
    "            result[\"api_result\"] = api\n",
    "            log.info(f\"  ✓ directory search: {api['title']}\")\n",
    "            return result\n",
    "\n",
    "    else:\n",
    "        result[\"lccn_source\"] = \"not_found\"\n",
    "        log.info(f\"  No LCCN found via ISSN join\")\n",
    "\n",
    "    # Step 5: Last resort — general search\n",
    "    log.info(f\"  Falling back to general search...\")\n",
    "    time.sleep(delay)\n",
    "    api = query_loc_search_fallback(name, issn)\n",
    "    if api:\n",
    "        result[\"api_result\"] = api\n",
    "        log.info(f\"  ✓ search fallback: {api['title']}\")\n",
    "    else:\n",
    "        log.info(f\"  ✗ No results from any source\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main(\n",
    "    matches: str = MATCHES_CSV,\n",
    "    newspapers: str = NEWSPAPERS_CSV,\n",
    "    output: str = \"data/unmatched_results.json\",\n",
    "    delay: float = 2.0,\n",
    "    limit: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Can be called directly from a notebook:\n",
    "        main(limit=3)\n",
    "    Or from the command line:\n",
    "        python loc_newspaper_lookup.py --limit 3\n",
    "    \"\"\"\n",
    "    # If running from command line (not Jupyter), parse args\n",
    "    if not any(\"jupyter\" in arg.lower() or \"ipykernel\" in arg.lower() for arg in sys.argv):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=\"Look up unmatched newspaper entries via Library of Congress APIs\"\n",
    "        )\n",
    "        parser.add_argument(\"--matches\", default=matches)\n",
    "        parser.add_argument(\"--newspapers\", default=newspapers)\n",
    "        parser.add_argument(\"--output\", \"-o\", default=output)\n",
    "        parser.add_argument(\"--delay\", \"-d\", type=float, default=delay)\n",
    "        parser.add_argument(\"--limit\", \"-l\", type=int, default=limit)\n",
    "        args = parser.parse_args()\n",
    "        matches, newspapers, output, delay, limit = (\n",
    "            args.matches, args.newspapers, args.output, args.delay, args.limit\n",
    "        )\n",
    "\n",
    "    # Load data\n",
    "    log.info(f\"Reading {matches} ...\")\n",
    "    matches_rows = read_csv(matches)\n",
    "    log.info(f\"  {len(matches_rows)} total rows\")\n",
    "\n",
    "    log.info(f\"Reading {newspapers} ...\")\n",
    "    newspapers_rows = read_csv(newspapers)\n",
    "    log.info(f\"  {len(newspapers_rows)} total rows\")\n",
    "\n",
    "    issn_to_lccn = build_issn_to_lccn(newspapers_rows)\n",
    "    log.info(f\"  Built ISSN→LCCN lookup with {len(issn_to_lccn)} entries\")\n",
    "\n",
    "    # Find unmatched\n",
    "    unmatched = find_unmatched(matches_rows)\n",
    "    log.info(f\"Found {len(unmatched)} unmatched entries (missing master_id)\")\n",
    "\n",
    "    if not unmatched:\n",
    "        log.info(\"Nothing to look up. Exiting.\")\n",
    "        return\n",
    "\n",
    "    if limit:\n",
    "        unmatched = unmatched[: limit]\n",
    "        log.info(f\"Processing first {len(unmatched)} entries (--limit)\")\n",
    "\n",
    "    # Preview LCCN join coverage\n",
    "    have_lccn = sum(1 for r in unmatched if r.get(\"issn\", \"\").strip() in issn_to_lccn)\n",
    "    log.info(f\"Of {len(unmatched)} unmatched, {have_lccn} have LCCNs via ISSN join, \"\n",
    "             f\"{len(unmatched) - have_lccn} will need search fallback\")\n",
    "\n",
    "    # Look up each entry\n",
    "    results = []\n",
    "    for i, row in enumerate(unmatched, 1):\n",
    "        log.info(f\"─── [{i}/{len(unmatched)}] ───\")\n",
    "        result = lookup_newspaper(row, issn_to_lccn, delay)\n",
    "        results.append(result)\n",
    "\n",
    "    # Write output\n",
    "    output_path = Path(output)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Summary\n",
    "    lccn_found = sum(1 for r in results if r[\"lccn\"])\n",
    "    api_found = sum(1 for r in results if r[\"api_result\"])\n",
    "    by_source = {}\n",
    "    for r in results:\n",
    "        if r[\"api_result\"]:\n",
    "            src = r[\"api_result\"][\"source\"]\n",
    "            by_source[src] = by_source.get(src, 0) + 1\n",
    "\n",
    "    log.info(\"═══ Summary ═══\")\n",
    "    log.info(f\"  Total unmatched:          {len(results)}\")\n",
    "    log.info(f\"  LCCN found via ISSN:      {lccn_found}\")\n",
    "    log.info(f\"  API results found:        {api_found}\")\n",
    "    for src, count in sorted(by_source.items()):\n",
    "        log.info(f\"    {src}: {count}\")\n",
    "    log.info(f\"  No results at all:        {len(results) - api_found}\")\n",
    "    log.info(f\"  Results saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def split_results(\n",
    "    input_file: str = \"data/unmatched_results.json\",\n",
    "    output_dir: str = \"data/batches\",\n",
    "    batch_size: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "Reads unmatched_results.json and splits it into JSON files of 25 entries each,\n",
    "keeping only issn, essay, and notes.\n",
    "\"\"\"\n",
    "    with open(input_file, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    num_batches = math.ceil(len(data) / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        slim = []\n",
    "        for entry in batch:\n",
    "            issn = entry.get(\"input\", {}).get(\"issn\", \"\")\n",
    "            api = entry.get(\"api_result\") or {}\n",
    "            essay = api.get(\"essay\", \"\")\n",
    "            notes = api.get(\"notes\", [])\n",
    "\n",
    "            slim.append({\n",
    "                \"issn\": issn,\n",
    "                \"date\": api.get(\"date\", \"\"),\n",
    "                \"dates_of_publication\": api.get(\"dates_of_publication\", \"\"),\n",
    "                \"created_published\": api.get(\"created_published\", []),\n",
    "                \"essay\": essay,\n",
    "                \"notes\": notes,\n",
    "            })\n",
    "\n",
    "        out_path = Path(output_dir) / f\"batch_{i + 1}.json\"\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(slim, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Wrote {len(slim)} entries to {out_path}\")\n",
    "\n",
    "    print(f\"\\nDone: {len(data)} entries across {num_batches} files in {output_dir}/\")\n",
    "\n",
    "    split_results()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    split_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8444208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell adds entries to Master using data from Chronicling America.\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "df = pd.read_csv(\"data/master.csv\", dtype=str, keep_default_na=False)\n",
    "\n",
    "# Add data_source column\n",
    "df[\"data_source\"] = \"\"\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "# Load all JSON files in the folder\n",
    "all_new_rows = []\n",
    "files = sorted(glob.glob(\"data/essay_derived_rows/*.json\"))\n",
    "\n",
    "for filepath in files:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        new_rows = json.load(f)\n",
    "    for row in new_rows:\n",
    "        r = {c: \"\" for c in cols}\n",
    "        r.update({k: v for k, v in row.items() if k in cols})\n",
    "        r[\"data_source\"] = \"essay_derived\"\n",
    "        all_new_rows.append(r)\n",
    "    print(f\"  Loaded {len(new_rows)} rows from {filepath}\")\n",
    "\n",
    "new_df = pd.DataFrame(all_new_rows, columns=cols)\n",
    "df_out = pd.concat([df, new_df], ignore_index=True)\n",
    "df_out.to_csv(\"data/master.csv\", index=False)\n",
    "\n",
    "print(f\"\\nOriginal rows: {len(df)}\")\n",
    "print(f\"Files processed: {len(files)}\")\n",
    "print(f\"New rows appended: {len(all_new_rows)}\")\n",
    "print(f\"Total rows: {len(df_out)}\")\n",
    "print(\"Saved to data/master.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
